{"cells":[{"cell_type":"markdown","metadata":{"id":"ibHFmIWoU3Vx"},"source":["# **SentimentArcs (Part 6): Time Series Feature Analysis**\n","\n","By: Jon Chun\n","12 Jun 2021\n","\n","References:\n","\n","* Coming...\n","\n","TODO:\n","* add global SUBDIR_TIMESERIES_RAW(_CLEAN), SUBDIR_SENTIMENTARCS\n","* rename all processed files to: timeseries_clean/timeseries_clean_novels_ref\n","* drop text_clean, text_raw, vader_rstd from sentiment for size\n","* no autosave(playground sandbox) https://stackoverflow.com/questions/60867546/save-failed-in-google-colab\n","* ---\n","* Demo datafiles\n","* Error detection around Crux points context (out of bounds)\n","* lex_discrete2continous (research binary->gaussian transformation fn)\n","* Text Preprocessing hints/tips/flowchart\n","* Clearly document workflow and partition across notebooks/libraries\n","* Code review and extraction to libraries\n","* Corpus ingestion for any format\n","* XAI (mlm false peak 1717SyuzhetR/1732SentimentR/1797robertalg15 adam watches war argument at dinner) \n","* Centralize and Standardize Model name lists\n","* Normalize model SA Series lengths\n","* Standardize all SA Series with the same method\n","* Seamless report generation/file saving\n","* Get raw text from SentimentR\n","* Filter out non-printable characters\n","* Roll-over Crux-Points (SentNo+Sent/Parag) (plotly)\n","* Label/Roll-over Chapter/Sect No at Boundries\n","* Generate Report PDF/csv\n","* Option to select raw or discrete2continous transformation (Bing)\n","* Annotation functionality + Share/Collaboration of findings/reseearch\n","* clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n","* plotly prefered library to save dynamic images: kaleido\n","* Correlation heatmaps: Justify choice of Spearman, Pearson, or other algo\n","\n","Facts:\n","* SyuzhetR vs SentimentTime Clean/Preprocess\n","* V.Woolf - To The Lighthouse\n","* SyuzhetR Clean: 3511 (SyuzhetR Preprocessed) Sentences (SentimentTime Preprocessed) 3403\n","* SentimentTime Clean: (Raw) 3402  (Clean) 3402\n","\n","\n","Preprocessing of Corpus Textfile\n","* Put headers in ALL CAPS\n","* Put \\n\\n between each CHAPTER/BOOK or SECTION header or Paragraphs\n","* Keep your format/spacing consistent\n","* Try to use utf-8 (not cp1252 (e.g. \\n <- \\n\\r)\n","* No leading blank lines, one trailing blank line at end of textfile\n","* Check for illegal, non-printable or other problematic code (e.g. curly single/double quotes)"]},{"cell_type":"markdown","metadata":{"id":"jmk-6u1fYP9b"},"source":["# **Reference Code**\n","\n","Surveys:\n","* https://github.com/prrao87/fine-grained-sentiment (20210409) Fine-grained SA (7 Models)\n","\n","\n","Other:\n","* https://github.com/annabiancajones/GA_capstone_project/blob/master/part3_mine_refine.ipynb\n","* https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 CV"]},{"cell_type":"markdown","metadata":{"id":"43oGeYK19Pyq"},"source":["# **[RESTART RUNTIME] May be Required for these Libaries**"]},{"cell_type":"code","source":["# If you see [Interactive namespace is empty] in response to the [%who] command below\n","#   your working with a fresh Linux Virtual Machine,\n","#   any previous work is lost,\n","#   and you need to SEQUENTIALLY execute EVERY cell this Notebook from the beginning \n","\n","%whos"],"metadata":{"id":"OBRU1f0vDVmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Takes far too long for inference, \n","#   currently not used\n","\n","# !pip install moepy"],"metadata":{"id":"MREjoHDm8juf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install dtaidistance"],"metadata":{"id":"1N-dEQbfjAQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sktime"],"metadata":{"id":"-tqkikuj286D"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mT-77aUWNOT_"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required (only needed for Plotly)\n","\n","# Designed Security Hole in older version of PyYAML, must upgrade to use plotly\n","\n","!pip install pyyaml==5.4.1"]},{"cell_type":"code","source":["# To Reduce Time Series Dimensionality\n","\n","!pip install lttb"],"metadata":{"id":"1EIEJMfQkODG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tslearn"],"metadata":{"id":"uggjF0ptYV50"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [STEP 1] Manual Configuration"],"metadata":{"id":"_Y0sLmhmSisA"}},{"cell_type":"markdown","metadata":{"id":"8vykhYcp47yD"},"source":["## [INPUT] Connect Google gDrive to this Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGSD0nWR47yD"},"outputs":[],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive')\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4tMr1i-47yE"},"outputs":[],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/cdh/sentiment_arcs/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","#@markdown (e.g. /gdrive/MyDrive/research/sentiment_arcs/)\n","\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n\\n')\n","\n","!ls\n","\n","print(f'\\n\\nVERIFY that this is the correct SentimentArcs Subdirectory')\n","\"\"\"\n","\n","# Step #2: Move to Parent directory of Sentiment_Arcs\n","# =======\n","parentdir_sentiment_arcs = '/'.join(Path_to_SentimentArcs.split('/')[:-2])\n","print(f'subdir_parent: {parentdir_sentiment_arcs}\\n')\n","%cd $parentdir_sentiment_arcs\n","\n","# TODO: \n","# Step #3: If project sentiment_arcs subdir does not exist, \n","#          clone it from github\n","# =======\n","import os\n","\n","if not os.path.isdir('sentiment_arcs'):\n","  # NOTE: This will not work until SentimentArcs becomes an open sourced PUBLIC repo\n","  # !git clone https://github.com/jon-chun/sentiment_arcs.git\n","\n","  # Test on open access github repo\n","  !git clone https://github.com/jon-chun/nabokov_palefire.git\n","\n","\n","# Step #4: Change into sentiment_arcs subdir\n","# =======\n","%cd ./sentiment_arcs\n","# Test on open acess github repo\n","# %cd ./nabokov_palefire\n","\n","# Step #5: Confirm contents of sentiment_arcs subdir\n","# =======\n","!ls \n","\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"8r3-sp6k47yE"},"source":["## [INPUT] Define Directory and Input Corpus"]},{"cell_type":"code","source":["\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/cdh/sentiment_arcs/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","#@markdown (e.g. /gdrive/MyDrive/research/sentiment_arcs/)\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n","\n","#@markdown Please check that the required textfiles and datafiles exist in the correct subdirectories before continuing.\n","\n","\n"],"metadata":{"id":"JDRUCiWHx9xJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWdjF3og47yF"},"outputs":[],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/cdh/sentiment_arcs/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","#@markdown (e.g. /gdrive/MyDrive/research/sentiment_arcs/)\n","\n","\n","\n","#@markdown **Sentiment Arcs Directory Structure** \\\n","#@markdown \\\n","#@markdown **1. Input Directories:** \\\n","#@markdown (a) Raw textfiles in subdir: ./text_raw/(text_type)/  \\\n","#@markdown (b) Cleaned textfiles in subdir: ./text_clean/(text_type)/ \\\n","#@markdown \\\n","#@markdown **2. Output Directories** \\\n","#@markdown (1) Raw Sentiment time series datafiles and plots in subdir: ./sentiment_raw/(text_type) \\\n","#@markdown (2) Cleaned Sentiment time series datafiles and plots in subdir: ./sentiment_clean/(text_type) \\\n","#@markdown \\\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n","\n","#@markdown Please check that the required textfiles and datafiles exist in the correct subdirectories before continuing.\n"]},{"cell_type":"code","source":["# Global Variable\n","\"\"\"\n","SUBDIR_SENTIMENTARCS = '/gdrive/MyDrive/cdh/sentiment_arcs'\n","SUBDIR_TIMESERIES_RAW = f'/timeseries_raw/timeseries_raw_{Corpus_Genre}_{Corpus_Type}'\n","SUBDIR_TIMESERIES_CLEAN = f'/timeseries_raw/timeseries_clean_{Corpus_Genre}_{Corpus_Type}'\n","SUBDIR_TIMESERIES_RAW\n","\"\"\";"],"metadata":{"id":"iCwObHQL31EY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[TEST] Download Books**\n","\n","* Books http://glozman.com/textpages.html\n","\n","* Web Script: \n"],"metadata":{"id":"w8IjOF7EHNzJ"}},{"cell_type":"code","source":["!pip install -U grab"],"metadata":{"id":"kVkJ_jWEHNmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","\n","from grab import Grab\n","\n","logging.basicConfig(level=logging.DEBUG)\n","\n","g = Grab()\n","\n","url_root = 'http://glozman.com/textpages.html'\n","url_base = 'http://glozman.com/'\n","g.go(url_root)\n","\n","# g.go('https://github.com/login')\n","# g.doc.set_input('login', '****')\n","# g.doc.set_input('password', '****')\n","# g.doc.submit()\n","\n","# g.doc.save('/tmp/x.html')\n","\n","# g.doc('//ul[@id=\"user-links\"]//button[contains(@class, \"signout\")]').assert_exists()\n","\n","# home_url = g.doc('//a[contains(@class, \"header-nav-link name\")]/@href').text()\n","# repo_url = home_url + '?tab=repositories'\n","\n","# g.go(repo_url)\n","\n","url_books_ls = []\n","for elem in g.doc.select('//li/a'):\n","  # print('%s: %s' % (elem.text(), g.make_url_absolute(elem.attr('href'))))\n","  aurl_book = elem.attr('href')\n","  print(f\"elem.attr(): {aurl_book}\")\n","  afullurl_book = f'{url_base}/{aurl_book}'\n","  print(f'afullurl_book: {afullurl_book}')\n","  url_books_ls.append(afullurl_book)"],"metadata":{"id":"EliteNMiHNkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url_books_ls[0]"],"metadata":{"id":"eLefrgqxHNg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import urllib.request\n","import re"],"metadata":{"id":"Yc0jipIfKMQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"wU1xXaMjPczv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir books\n","%cd books\n","!pwd"],"metadata":{"id":"2nTSIyBQPl_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for aurl in url_books_ls:\n","  aurl_clean = ('%20').join(aurl.split(' '))\n","  print(f'Trying to get arul: {aurl_clean}')\n","  try:\n","    with urllib.request.urlopen(aurl_clean) as f:\n","      file_text = f.read().decode(errors=\"ignore\") # 'windows-1252')) # 'utf-8'))\n","    file_out = aurl.split('/')[-1]\n","    file_out = aurl.split('.')[-2]\n","    file_out = '_'.join(file_out.split(' '))\n","    file_out = re.sub(r\"[^a-zA-Z0-9_ ]\", \"\", file_out)\n","    file_out = re.sub(r\"[_]+\", \"_\", file_out)\n","    file_out = file_out.lower()\n","    file_out = file_out + '.txt'\n","    file_out = re.sub(r\"comtextpages\",\"\", file_out)\n","    print(f'Writing to: {file_out}')\n","    with open(file_out, 'w') as fp:\n","      fp.write(file_text)\n","  except urllib.error.URLError as e:\n","    print(e.reason)"],"metadata":{"id":"zbpMpVujKF4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -altr *.txt"],"metadata":{"id":"i9Uq18nKKFsU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -altr *.txt | wc -l"],"metadata":{"id":"8Bngzo0SP5xN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!head -n 20 clancy_tom_patriot_games.txt\n","\n","print('\\n\\n')\n","\n","!tail -n 20 clancy_tom_patriot_games.txt"],"metadata":{"id":"1Nzc_pQVOlt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!head -n 20 clancy_tom_red_storm_rising.txt\n","\n","print(\"\\n\\n\")\n","\n","!tail -n 20 clancy_tom_red_storm_rising.txt"],"metadata":{"id":"oalFRMKSPMPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 2] Automatic Configuration/Setup**"],"metadata":{"id":"P00BhwLVyL8X"}},{"cell_type":"code","source":["# Define all Sub/Dir global CONSTANTS\n","\n","import os\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_subdirs.py'\n","\n","get_subdirs(Corpus_Genre, Corpus_Type, 'lex2ml')"],"metadata":{"id":"0_-UEIcmAogi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_R11BEJ_47yA"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KRkk4Cg47yB"},"outputs":[],"source":["# Configure Jupyter\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"QJO7kLz-47yF"},"source":["## Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syHXsIGs47yG"},"outputs":[],"source":["# Define all Corpus Texts & Ensemble Models as global CONSTANTS\n","\n","import yaml\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/read_yaml.py'\n","\n","read_yaml(Corpus_Genre, Corpus_Type)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = corpus_titles_dt.keys()\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')"]},{"cell_type":"markdown","metadata":{"id":"o5GqEXyRkPjj"},"source":["## Install Python Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEyFz33IMugd"},"outputs":[],"source":["# Intentionally left blank"]},{"cell_type":"markdown","metadata":{"id":"If55aLQYsk_K"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVNT7dGQsmw3"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","pd.set_option('max_colwidth', 100) # -1)\n","\n","from pandas.core.arrays.numeric import T\n","\n","from glob import glob\n","import copy\n","# import yaml # Already done above\n","import json # Already done above\n","from itertools import groupby\n"]},{"cell_type":"code","source":["from pandas.core.arrays.numeric import T"],"metadata":{"id":"_aWhBFNYZ6AT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Time Series Smoothing and Scaling\n","\n","from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n","from statsmodels import robust as sm_robust\n","\n","# Too Slow\n","# from moepy import lowess, eda\n","# lowess_moepy = lowess.Lowess()\n","\n","from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n","from sklearn.preprocessing import StandardScaler # To sandardize time series\n","from sklearn.preprocessing import RobustScaler   # To deal with outliers\n","\n","scaler_minmax = MinMaxScaler()\n","scaler_zscore = StandardScaler()\n","scaler_robust = RobustScaler()"],"metadata":{"id":"YAOuOUeGtUtp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Time Series Dimensionality Reduction and Clustering\n","\n","import lttb\n","from lttb.validators import *\n","\n","from dtaidistance import clustering"],"metadata":{"id":"DMaxPl3d3Ud3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Distance/Similiarity Metrics for Time Series\n","\n","from tslearn.metrics import dtw, soft_dtw, soft_dtw_alignment, ctw, lcss, gak\n","\n","# calculating euclidean distance between vectors\n","from scipy.spatial.distance import euclidean\n","\n","# calculating manhattan distance between vectors\n","from scipy.spatial.distance import cityblock"],"metadata":{"id":"LL8I2ouVY8C9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eieKCE997ayn"},"outputs":[],"source":["# Plotly Visualizations\n","# Note: Security Hole in default, must upgrade above\n","#       !pip install pyyaml==5.4.1\n","\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import plotly"]},{"cell_type":"markdown","source":["## Define Global Parameters\n","\n","\n"],"metadata":{"id":"umZfB0YCqajW"}},{"cell_type":"code","source":["# Define Globals\n","\n","# Main data structure: Dictionary (key=text_name) of DataFrames (cols: text_raw, text_clean)\n","corpus_texts_dt = {}\n","\n","# corpus_sa_dt = {}\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_globals.py'\n","\n","SLANG_DT.keys()\n"],"metadata":{"id":"e8_qLJBbtoOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EA1yTaY_9Qod"},"source":["## Setup Matplotlib Style\n","\n","* https://matplotlib.org/stable/tutorials/introductory/customizing.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1s24O-S9JJX"},"outputs":[],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------\\n')\n","plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"7dPPrZwyIIze"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hM3oRY-UOmzX"},"outputs":[],"source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')"]},{"cell_type":"markdown","metadata":{"id":"CenqyAnJ7NLA"},"source":["## Utility Functions"]},{"cell_type":"markdown","source":["### Generate Convenient Data Lists"],"metadata":{"id":"JXG_G6um4ijG"}},{"cell_type":"code","source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"],"metadata":{"id":"TO08GFoGlP3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get_model_families()\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_model_families.py'\n","\n","ensemble_models_dt = get_model_famalies(models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","ensemble_models_dt['lexicon']"],"metadata":{"id":"hWjkdbgEnum_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### File Functions"],"metadata":{"id":"WlfujTpEKhCp"}},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"],"metadata":{"id":"yOX-fpiuApL4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode text for JSON.dump()\n","\n","class NumpyEncoder(json.JSONEncoder):\n","    \"\"\" Special json encoder for numpy types \"\"\"\n","    # https://stackoverflow.com/questions/57269741/typeerror-object-of-type-ndarray-is-not-json-serializable\n","    def default(self, obj):\n","        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n","                            np.int16, np.int32, np.int64, np.uint8,\n","                            np.uint16, np.uint32, np.uint64)):\n","            return int(obj)\n","        elif isinstance(obj, (np.float_, np.float16, np.float32,\n","                              np.float64)):\n","            return float(obj)\n","        elif isinstance(obj, (np.ndarray,)):\n","            return obj.tolist()\n","        return json.JSONEncoder.default(self, obj)"],"metadata":{"id":"KDmAIFEMTHoQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2GvYiOSIt4j"},"source":["# **[STEP 3] Read, Clean & EDA Visualizations**"]},{"cell_type":"markdown","source":["## Read All Raw Sentiment Data"],"metadata":{"id":"udJwooqId0o4"}},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","print(f'Reading from SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n')\n","sentiment_raw_datafile_ls = os.listdir(SUBDIR_SENTIMENT_RAW)\n","\n","corpus_texts_dt = {}\n","first_bo = T\n","\n","!ls -altr $SUBDIR_SENTIMENT_RAW\n","print('\\n')\n","\n","for i,afile in enumerate(sentiment_raw_datafile_ls):\n","  temp_dt = {}\n","  print(f'File #{i}: {afile}')\n","  afile_key = afile.split('.')[0].split('_')[-1]\n","  print(f'         {afile_key}')\n","  temp_dt = read_dict_dfs(in_file=afile, in_dir=SUBDIR_SENTIMENT_RAW)\n","  corpus_titles_ls = list(temp_dt.keys())\n","  # print(corpus_titles_ls)\n","  for j, atitle in enumerate(corpus_titles_ls):\n","    print(f'    Text #{j}: {atitle}')\n","    if (atitle in corpus_texts_dt.keys()):\n","      print(f'Append:')\n","      corpus_texts_dt[atitle] = pd.concat([corpus_texts_dt[atitle],temp_dt[atitle]], axis=1)\n","      # corpus_texts_dt[atitle] = temp_dt[atitle]\n","      # first_bo = False\n","    else:\n","      print(f'  New:')\n","      corpus_texts_dt[atitle] = temp_dt[atitle].copy()"],"metadata":{"id":"s0rIe6Fw3Qdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt.keys()"],"metadata":{"id":"2fwPYOFfk1mK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[corpus_titles_ls[1]].info()"],"metadata":{"id":"KgZjgqU5pCK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ensemble_ls = list(set(corpus_texts_dt[corpus_titles_ls[1]].columns) - set(['text_clean','text_raw']))\n","ensemble_ls"],"metadata":{"id":"Y56g3cQHpMUI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Delete Duplicate Columns"],"metadata":{"id":"lsZQL9BMd5h4"}},{"cell_type":"code","source":["# Check for any duplicated columns/models\n","\n","# corpus_texts_dt[atext].columns.duplicated()\n","print(corpus_texts_dt[corpus_titles_ls[0]].columns.value_counts())\n","\n","print('\\n')\n","next(iter(zip(corpus_texts_dt[corpus_titles_ls[0]].columns.duplicated(), corpus_texts_dt[corpus_titles_ls[0]].columns)))"],"metadata":{"id":"GfnOYr2seGSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for Duplicate Models/Col names\n","\n","column_list = pd.Series()\n","\n","for atext_no in range(len(corpus_texts_dt.keys())):\n","  atext = corpus_texts_ls[atext_no]\n","  df = corpus_texts_dt[atext]\n","  print(f'\\n\\n\\nProcessing #{atext_no}: {atext}')\n","  # col_dups_ct = corpus_texts_dt[atext].columns.duplicated().sum()\n","  # col_dups_ls = corpus_texts_dt[atext].columns[corpus_texts_dt[atext].columns.duplicated()]\n","\n","  col_dups_ct = df.columns.duplicated().sum()\n","\n","  if col_dups_ct > 0:\n","    col_dups_ls = df.columns[df.columns.duplicated()]\n","    print(f'\\nBEFORE: {col_dups_ct} duplicated columns')\n","    print(f'  {\", \".join(col_dups_ls)}')  \n","\n","    # get names of duplicated columns\n","    column_list = df.columns.value_counts()\n","    col_del_ls = column_list[column_list>1]\n","    col_del_ls\n","\n","    # Add '_dup' suffix to columns repeated more than once (keep one original without _dup suffix)\n","    my_suffix = '_dup'\n","    df.columns = [name if (duplicated == False & ~(name.endswith('_dup'))) else name + my_suffix for duplicated, name in zip(df.columns.duplicated(), df.columns)]\n","\n","    # Drop all columns with '_dup' suffix\n","    col_drop_ls = [x for x in df.columns if x.endswith('_dup')]\n","    df.drop(columns=col_drop_ls, inplace=True)\n","\n","    # check to see if table has duplicate column names which prohibits upload to BigQuery - TRUE Desired\n","    len(df.columns) == len(set(df.columns))\n","\n","  else:\n","    print(f'No Duplicated Columns')"],"metadata":{"id":"EMKDUFNReGSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# After deleting duplicates, check remaining cols/models of last text processed\n","\n","corpus_texts_dt[corpus_titles_ls[0]].info()"],"metadata":{"id":"xSBkC33ReGSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Interpolate and NaN/None Values"],"metadata":{"id":"YgQcCuvNetie"}},{"cell_type":"code","source":["# TODO:"],"metadata":{"id":"-yILfCfUetW3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visually Verify Data"],"metadata":{"id":"E5HBYW3Sd8ua"}},{"cell_type":"code","source":["for i, atext in enumerate(corpus_texts_ls):\n","    \n","  atitle = f'{corpus_titles_dt[atext][0]}\\nSentiment Analysis (SMA 10%)\\nNo Normalization/Standardization'\n","  corpus_texts_dt[atext][ensemble_ls].rolling(200, center=True).mean().plot(title=atitle)\n","  plt.grid(True, alpha=0.7) \n","  plt.show();              "],"metadata":{"id":"Y9EdCkyfpEa6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for clean DataFrame\n","\n","# Check for NaN values\n","# TODO:\n","\"\"\"\n","print(f'Any Null values: [{corpus_texts_dt[corpus_titles_ls[0]].isnull().values.any()}]')\n","\n","print('\\n')\n","\n","corpus_texts_dt[corpus_titles_ls[0]].columns.duplicated()\n","\n","print('\\n')\n","\n","print(corpus_texts_dt[corpus_titles_ls[0]].columns.value_counts())\n","\n","print('\\n')\n","\n","next(iter(zip(corpus_texts_dt[atext].columns.duplicated(), corpus_texts_dt[atext].columns)))\n","\"\"\";"],"metadata":{"id":"8iSmhkCqKjDZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Make Robust to Outliers"],"metadata":{"id":"6Bdy1oyFqjvh"}},{"cell_type":"code","source":["import statsmodels.robust.scale as sm_robust"],"metadata":{"id":"c-KGxMCRmjUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clip Outliers based on IQR: RobustScaler())\n","\n","def clip_outliers(floats_ser):\n","  '''\n","  Given a pd.Series of float values\n","  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n","  '''\n","  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n","\n","  # Old mean/std, less robust\n","  # ser_std = floats_ser.std()\n","  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n","\n","  floats_np = np.array(floats_ser)\n","  ser_median = floats_ser.median()\n","  ser_mad = sm_robust.mad(floats_np)\n","  # print(f'ser_median = {ser_median}')\n","  # print(f'ser_mad = {ser_mad}')\n","\n","  if ser_mad == 0:\n","    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n","    #   so pass back the original time series\n","    floats_clip_ls = list(floats_ser)\n","\n","  else:\n","    ser_oldmax = floats_ser.max()\n","    ser_oldmin = floats_ser.min()\n","    # print(f'ser_max = {ser_oldmax}')\n","    # print(f'ser_min = {ser_oldmin}')\n","\n","    ser_upperlim = ser_median + 2.5*ser_mad\n","    ser_lowerlim = ser_median - 2.5*ser_mad\n","    # print(f'ser_upperlim = {ser_upperlim}')\n","    # print(f'ser_lowerlim = {ser_lowerlim}')\n","\n","    # Clip outliers to max or min values\n","    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n","    # print(f'max floast_ls {floats_ls.max()}')\n","\n","    # def map2range(value, low, high, new_low, new_high):\n","    #   '''map a value from one range to another'''\n","    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n","\n","    # Map all float values to range [-1.0 to 1.0]\n","    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n","\n","    # listmax_fl = float(max(floats_ls))\n","    # floats_ls = [i/listmax_fl for i in floats_ls]\n","    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n","\n","  return floats_clip_ls  # floats_clip_sig_ls\n","\n","# Test\n","# Will not work on first run as corpus_sents_df is not defined yet\n","\n","# data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n","# test_ls = clip_outliers(pd.Series(data))\n","\n","print('Comparison Test: (a) Manual IRQ Clipping vs (b) RobustScaler()')\n","# Plot #1: Clipped Outliers with IQR\n","test_ls = clip_outliers(corpus_texts_dt[corpus_texts_ls[0]]['vader'])\n","# test_ls = clip_outliers(corpus_texts_dt[corpus_texts_ls[0]]['afinn'].iloc[0])\n","# print(f'new min is {min(test_ls)}')\n","# print(f'new max is {max(test_ls)}')\n","pd.DataFrame(test_ls).rolling(300, center=True, min_periods=0).mean().plot(label='clipped', alpha=0.7);\n","corpus_texts_dt[corpus_texts_ls[0]]['vader'].rolling(300, center=True, min_periods=0).mean().plot(label='original', alpha=0.7)\n","\n","# transformer = scaler_robust.fit(corpus_texts_dt[corpus_texts_ls[0]]['vader'].values.reshape(-1, 1))\n","\n","# Plot #2: Scale Outliers with RobustScaler()\n","test_df = corpus_texts_dt[corpus_texts_ls[0]]['vader'].copy(deep=True) #   pd.DataFrame()\n","# test_df = pd.DataFrame({'vader': scaler_robust.fit_transform(np.array(corpus_texts_dt[corpus_texts_ls[0]]['vader']).reshape(-1, 1))})\n","test_df['vader_std'] = pd.Series(scaler_robust.fit_transform(np.array(corpus_texts_dt[corpus_texts_ls[0]]['vader']).reshape(-1, 1)).flatten())\n","test_df['vader_std'].rolling(300, center=True, min_periods=0).mean().plot(label='RobustScaler', alpha=0.7)\n","\n","plt.title('Dealing with Outliers in Sentiment Time Series\\n(a) Manually Clip with IQR or,\\n (b) Scale with RobustScaler()')\n","plt.grid(True, alpha=0.7)\n","plt.legend()\n","plt.show();"],"metadata":{"id":"bRcJeM2gqjj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[corpus_titles_ls[0]].info()"],"metadata":{"id":"jA9pvR5zZ_Fo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Deal with Outliers: (a) Manually clip with IQR, or (b) Automatically Scale RobustScaler()\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f'Text #{i}: {atext}')\n","  \n","  win_10per = int(0.10 * corpus_texts_dt[corpus_texts_ls[0]].shape[0])\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  models_rstd_ls = []\n","  for j, amodel in enumerate(ensemble_ls):\n","    amodel_rstd = f'{amodel}_rstd'\n","    # print(f'  Model #{j}: {amodel} (Model_Std: {amodel_rstd})')\n","    # clip_outliers(corpus_texts_dt[corpus_texts_ls[0]]['vader'])\n","\n","    # Option (a): Manually Clip with 2.5*IQR\n","    # corpus_texts_dt[atext][amodel_rstd] = pd.Series(clip_outliers(corpus_texts_dt[atext][amodel])) # .reshape(-1,1)).flatten())\n","\n","    # Option (b): Automatically Scale wit scikit-learns ScalerRobust()\n","    corpus_texts_dt[atext][amodel_rstd] = pd.Series(scaler_robust.fit_transform(np.array(corpus_texts_dt[atext][amodel]).reshape(-1,1)).flatten())\n","\n","    # Plot\n","    _ = ax.plot(corpus_texts_dt[atext][amodel_rstd].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rstd, alpha=0.3)\n","\n","    models_rstd_ls.append(amodel_rstd)\n","\n","  # Plot Median of Ensemble\n","  _ = ax.plot(corpus_texts_dt[atext][models_rstd_ls].median(axis=1).rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Shrink current axis by 20%\n","  # box = ax.get_position()\n","  # ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n","\n","  # Put a legend to the right of the current axis\n","  _ = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  plt.grid(True, alpha=0.7)\n","  atitle = plt.title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window=10%)\\nClipped with IQR + zScore Standardized')\n","  plt.show();\n"],"metadata":{"id":"Rhf9BBH1lty4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Standardization with zScore"],"metadata":{"id":"9Qj4LlVWjRt6"}},{"cell_type":"code","source":["%%time\n","\n","# NOTE:\n","\n","# zScore Standardization (mean=0, std=1)\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f'Text #{i}: {atext}')\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  models_std_ls = []\n","  for j, amodel in enumerate(ensemble_ls):\n","    amodel_rstd = f'{amodel}_rstd'\n","    amodel_rzstd = f'{amodel}_rzstd'\n","    # print(f'  Model #{j}: {amodel} (Model_Std: {amodel_rzstd})')\n","    # clip_outliers(corpus_texts_dt[corpus_texts_ls[0]]['vader'])\n","\n","    # Get SMA 10% window length\n","    win_10per = int(0.10 * corpus_texts_dt[atext][amodel_rstd].shape[0])\n","\n","\n","    # UNCOMMENT only ONE of these TWO Options\n","    # ---------------------------------------\n","    # Option (a): Manually Clip with IQR\n","    corpus_texts_dt[atext][amodel_rzstd] = scaler_zscore.fit_transform(np.array(corpus_texts_dt[atext][amodel_rstd]).reshape(-1,1))\n","\n","    # Option (b): Automatically Scale wit scikit-learns ScalerRobust()\n","    # corpus_texts_dt[atext][amodel_rzstd] = pd.Series(scaler_robust.fit_transform(np.array(corpus_texts_dt[atext][amodel]).reshape(-1,1)).flatten())\n","\n","    # Plot amodel_rzstd\n","    _ = ax.plot(corpus_texts_dt[atext][amodel_rzstd].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rzstd, alpha=0.3)\n","\n","    models_std_ls.append(amodel_rzstd)\n","\n","  # Plot Median of Ensemble\n","  _ = ax.plot(corpus_texts_dt[atext][models_std_ls].median(axis=1).rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Put a legend to the right of the current axis\n","  ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  plt.grid(True, alpha=0.7)\n","  plt.title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window=10%)\\nClipped with IQR + zScore Standardized')\n","  plt.show();\n"],"metadata":{"id":"dGxMQCFHiq5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the Robust '_rstd' Columns in for all Texts\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f'Text #{i}: {atext}')\n","\n","  models_rstd_ls = [x for x in corpus_texts_dt[atext].columns if '_rstd' in x]\n","  # models_rstd_ls\n","\n","  corpus_texts_dt[atext].drop(columns=models_rstd_ls, inplace=True)\n","  corpus_texts_dt[atext].info()\n","\n","  # Verify no '_rstd' columns exist\n","  [x for x in corpus_texts_dt[atext].columns if '_rstd' in x]\n","  print('\\n')"],"metadata":{"id":"B_W-yqEbuhTK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [TEMP] Numpy Experiments"],"metadata":{"id":"KCo-Q6D7MM0M"}},{"cell_type":"code","source":["# Test numpy: line vector vs column vector\n","\n","x1_test = np.linspace(0,1,100)\n","x1_test.shape\n","print(x1_test[:5])\n","x2_test = np.linspace(0,1,100).reshape(-1,1)\n","x2_test.shape\n","x2_test[:5]"],"metadata":{"id":"xhU0O4iNAzuC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test numpy\n","\n","A = np.array([1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256])\n","B = A.reshape(-1,1)\n","B\n","C = np.array(list(range(len(A))))\n","C\n","D = np.dstack((C,A))\n","D\n","\n","E = np.array(list(zip(C,A)))\n","E.shape\n","F= scaler_minmax.fit_transform(E)\n","F\n","plt.plot(F[:,0], F[:,1])"],"metadata":{"id":"g9WW6KtIsoXU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SMA + LOWESS Smoothing"],"metadata":{"id":"z1fagURcqoe9"}},{"cell_type":"code","source":["#@markdown **Select Smoothing Parameters:**\n","\n","#@markdown Simple Moving Average/Rolling Mean (default 10%):\n","\n","Window_Percent = \"10\" #@param [\"5\", \"10\", \"15\", \"20\"]\n","\n","#@markdown Second: LOWESS (default 20):\n","\n","Inv_Fraction = \"20\" #@param [\"5\", \"10\",\"15\",\"20\",\"30\", \"40\"]\n","\n","#@markdown **NOTE:** frac = 1.0/Inv_Fraction (e.g. if frac = 1./30 = 0.03)"],"metadata":{"id":"0_5yg38XxRye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# timeit_res = %timeit -n1 -r1 -o sum(range(1000000))\n","\n","\n","# NOTE:      1m30s  19:10 on 20220308 Colab Pro/CPU w/moepy (1 Novel  x 2 Models)\n","#            5m28s  19:12 on 20220308 Colab Pro/CPU w/moepy (2 Novels x 2 Models)\n","#            5m29s  19:39 on 20220308 Colab Pro/CPU w/moepy (2 Novels x 2 Models) ~ 1m20s per Model per Novel\n","#        ~1h24m29s  19:39 on 20220308 Colab Pro/CPU w/moepy (2 Novels x 32 Models) \n","#            4m23s  06:34 on 20220309 Colab Pro/CPU w/statsmodels (2 Novels x 32 Models) ~ 4.5s per Model per Novel\n","#            4m48s  11:53 on 20220309 Colab Pro/CPU w/statsmodels (2 novels x 32 Models)\n","\n","# zScore Standardization (mean=0, std=1)\n","\n","# Smoothing Parameters\n","win_per = int(Window_Percent)/100\n","afrac_inv = int(Inv_Fraction) # [10, 20, 30]\n","\n","\n","# for i, atext in enumerate(corpus_texts_dt.keys()):\n","for i, atext in enumerate(corpus_titles_ls):\n","  print(f'Text #{i}: {atext}')\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  models_smalowess_rzstd_ls = []\n","  models_sma_rzstd_ls = []\n","  for j, amodel in enumerate(ensemble_ls):\n","    # amodel_rstd = f'{amodel}_rstd'\n","    # Assume ensemble_ls only contains model roots, if not\n","    #   could add check to prevent duplicating models with suffixes\n","    amodel_rzstd = f'{amodel}_rzstd'\n","    amodel_sma_rzstd = f'{amodel}_sma_rzstd'\n","    amodel_smalowess_rzstd = f'{amodel}_smalowess_rzstd'\n","    print(f'  Model #{j}: {amodel} (Model_Std: {amodel_rzstd})')\n","\n","    # Save SMA of Robust+zScore Standized Models\n","    sent_ct = int(win_per * corpus_texts_dt[atext][amodel].shape[0])\n","    corpus_texts_dt[atext][amodel_sma_rzstd] = corpus_texts_dt[atext][amodel_rzstd].rolling(sent_ct, center=True, min_periods=0).mean()\n","\n","    # Get x/y values as numpy arrays\n","    x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","    # y_sma_rzstd = corpus_texts_dt[atext][amodel_rzstd].rolling(win_10per, center=True, min_periods=0).mean().to_numpy()\n","    y_sma_rzstd = corpus_texts_dt[atext][amodel_sma_rzstd].to_numpy()\n","\n","    # Get LOWESS smoothing of SMA smoothed Sentiment\n","    # UNCOMMENT only ONE of these TWO Options\n","    # ---------------------------------------\n","\n","    # Option (a): statsmodels LOWESS\n","    sm_x, y_smalowess_rzstd_pred = sm_lowess(y_sma_rzstd, x,  frac=1./afrac_inv, it=5).T\n","\n","    # Option (b): moepy LOWESS\n","    # print('Computing MOEPy LOWESS for RStd Time Series')\n","    # lowess_moepy.fit(x, y_sma_rzstd, frac=1./afrac_inv)\n","    # y_sma_rzstd_pred = lowess_moepy.predict(x)\n","    # moepy fit _sma_rstd\n","    # print('Computing MOEPy LOWESS for SMA_RStd Time Series')\n","    ## lowess_moepy.fit(x, y_sma_rzstd, frac=1./afrac_inv)\n","    ## y_smalowess_rzstd_pred = lowess_moepy.predict(x)\n","\n","    # Save SMA smoothed Robust+zScore Standardized Time Series for Model=amodel\n","    # (if > 5 models comment out) corpus_texts_dt[atext][amodel_sma_rzstd] = pd.Series(y_sma_rzstd)\n","\n","    # Save SMA+LOWESS smoothed Robust+zScore Standardized Time Series for Model=amodel\n","    corpus_texts_dt[atext][amodel_smalowess_rzstd] = pd.Series(y_smalowess_rzstd_pred)\n","\n","    # Plot amodel_smalowess_rzstd\n","    # atitle = f'{corpus_titles_dt[atext][0]}\\nSentimentArcs Model: {amodel}\\nLOWESS Smoothed (frac=1./{afrac_inv})\\nClipped (2.5*IQR) and Standardized (zScore)'\n","    # _ = ax.plot(y_sma_rzstd, label=amodel_sma_rzstd, alpha=0.3) # .rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rzstd, alpha=0.3)\n","    _ = ax.plot(corpus_texts_dt[atext][amodel_sma_rzstd], label=amodel_sma_rzstd, alpha=0.3) # .rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rzstd, alpha=0.3)\n","    _ = ax.plot(corpus_texts_dt[atext][amodel_smalowess_rzstd], label=amodel_smalowess_rzstd, alpha=0.3) # .rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rzstd, alpha=0.3)\n","\n","    print(f'  Appending {amodel_smalowess_rzstd} to models_smalowess_rzstd_ls')\n","    models_smalowess_rzstd_ls.append(amodel_smalowess_rzstd)\n","    models_sma_rzstd_ls.append(amodel_sma_rzstd)\n","\n","  # Plot Ensemble Median(SMA+LOWESS)\n","  print('Plotting Ensemble Median(SMA+LOWESS)')\n","  models_smalowess_rzstd_ls_median = corpus_texts_dt[atext][models_smalowess_rzstd_ls].median(axis=1)\n","  _ = ax.plot(models_smalowess_rzstd_ls_median, label='Ensemble Median(SMA+LOWESS)', color='r', linewidth=3) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Plot Ensemble LOWESS(Median(SMA+LOWESS))\n","  print('Plotting Ensemble LOWESS(Median(SMA+LOWESS))')\n","\n","  # Get x/y values as numpy arrays\n","  x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","  y_lowess_smalowess_median = models_smalowess_rzstd_ls_median.to_numpy()\n","\n","  # Get LOWESS smoothing of SMA smoothed Sentiment\n","  # UNCOMMENT only ONE of these TWO Options\n","  # ---------------------------------------\n","\n","  # Option (a): statsmodels LOWESS\n","  sm_x, y_lowess_smalowess_median_pred = sm_lowess(y_lowess_smalowess_median, x,  frac=1./afrac_inv, it=5).T\n","\n","  # Option (b): moepy LOWESS\n","  # print('Computing MOEPy LOWESS for RStd Time Series')\n","  # lowess_moepy.fit(x, y_lowess_smalowess_median, frac=1./afrac_inv)\n","  # y_lowess_smalowess_median = lowess_moepy.predict(x)\n","  # moepy fit _sma_rstd\n","  # print('Computing MOEPy LOWESS for SMA_RStd Time Series')\n","  ## lowess_moepy.fit(x, y_lowess_smalowess_median, frac=1./afrac_inv)\n","  ## y_lowess_smalowess_median_pred = lowess_moepy.predict(x)\n","\n","  # Save SMA smoothed Robust+zScore Standardized Time Series for Model=amodel\n","  # (if > 5 models comment out) corpus_texts_dt[atext][amodel_sma_rzstd] = pd.Series(y_sma_median)\n","\n","  # Save SMA+LOWESS smoothed Robust+zScore Standardized Time Series for Model=amodel\n","  # corpus_texts_dt[atext][f'{amodel}_sma_median_pred'] = pd.Series(y_lowess_smalowess_median_pred)\n","\n","  lowesssma_ls_median = y_lowess_smalowess_median_pred # lowess(sma_ls_median)\n","  _ = ax.plot(lowesssma_ls_median, label='Ensemble LOWESS(Median(SMA+LOWESS))', color='b', linewidth=3) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Put a legend to the right of the current axis\n","  ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  plt.grid(True, alpha=0.7)\n","  plt.title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window={Window_Percent}) + LOWESS (frac=1./{Inv_Fraction}))\\nClipped with IQR + zScore Standardized')\n","  plt.show();"],"metadata":{"id":"Th51r59KxRyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos list\n"],"metadata":{"id":"sfQ_wCyDXQ2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_sma_ls = [x for x in corpus_texts_dt[atext].columns if x.endswith('_sma_rzstd')]\n","models_sma_ls"],"metadata":{"id":"fFZd0rox1XCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify LOWESS(SMA+LOWESS) is a good fit (Adjust outter/last LOWESS frac as necessary)\n","\n","# atext = corpus_texts_ls[0]\n","model_ct = 2\n","\n","win_per = int(int(Window_Percent) * corpus_texts_dt[atext].shape[0])\n","\n","models_smalowess_ls = [x for x in corpus_texts_dt[atext].columns if x.endswith('_smalowess_rzstd')]\n","models_sma_ls = [x for x in corpus_texts_dt[atext].columns if x.endswith('_sma_rzstd')]\n","# sma_ls = [x.replace('_smalowess_', '_') for x in models_smalowess_ls]\n","\n","# afrac_inv = 10 # (defined above)\n","\n","\n","# for i, atext in enumerate(corpus_texts_dt.keys()):\n","for i, atext in enumerate(corpus_titles_ls):\n","  print(f'Text #{i}: {atext}')\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  # for i, amodel_smalowess in enumerate(smalowess_ls[:model_ct]):\n","  for i, amodel_smalowess in enumerate(models_smalowess_ls[:model_ct]):\n","    # for i, amodel_smalowess in enumerate(models_smalowess_ls):\n","\n","    # _ = ax.plot(models_smalowess_rzstd_ls_median, label='Ensemble Median', color='r', linewidth=3) # .rolling(win_per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","    smalowess_label = f'sma+lowess: {models_sma_ls[i]}'\n","    _ = ax.plot(corpus_texts_dt[atext][models_smalowess_ls[i]], label=smalowess_label, alpha=0.3, linewidth=3)\n","    sma_label = f'sma: {models_sma_ls[i]}'\n","    # _ = ax.plot(corpus_texts_dt[atext][sma_models_sma_lsls[i]].rolling(win_per, center=True, min_periods=0).mean(), label=sma_label, alpha=0.3, linewidth=3)\n","    _ = ax.plot(corpus_texts_dt[atext][models_sma_ls[i]], label=sma_label, alpha=0.3, linewidth=3)\n","\n","  # Put a legend to the right of the current axis\n","  _ = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  _ = ax.grid(True, alpha=0.7)\n","  _ = ax.set_title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {model_ct} Models\\nSmoothed: SMA only (vs) SMA (window=10%) + LOWESS (frac=1./{afrac_inv}\\nClipped with IQR + zScore Standardized')\n","  plt.show(); \n","\n","print('Verify that the smooth LOWESS curves relatively accurately follow the jagged SMA curves below')\n","print('  If not, go back and adjust the parameters for LOWESS/SMA so they more closely match (e.g. increase LOWESS 20->30)')"],"metadata":{"id":"l4jyNryXlC_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create lists of Model Types\n","\n","# Get Types of Models\n","print('Model Types (e.g. for VADER):')\n","[x for x in corpus_texts_dt[corpus_texts_ls[0]].columns if 'vader' in x]\n","print('\\n\\n')\n","\n","models_smalowess_ls = [x for x in corpus_texts_dt[atext].columns if x.endswith('_smalowess_rzstd')]\n","print(f'\\n\\nModel Type: models_smalowess_ls=[*_smalowess_rzstd]\\n  {models_smalowess_ls}')\n","print(f'   Model Count: {len(models_smalowess_ls)}\\n\\n')\n","\n","models_rzstd_ls = [x for x in corpus_texts_dt[atext].columns if (x. endswith('_rzstd') & ~x.endswith('_smalowess_rzstd') &~x.endswith('_sma_rzstd'))]\n","print(f'\\n\\nModel Type=[*_rzstd]\\n  {models_rzstd_ls}')\n","print(f'   Model Count: {len(models_rzstd_ls)}\\n\\n')\n"],"metadata":{"id":"5caObmrgTTKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Ensemble SMA+LOWESS Arcs for all Texts\n","\n","for i, atext in enumerate(corpus_titles_ls):\n","  print(f'Plotting {corpus_titles_dt[atext][0]}:')\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  for j, amodel in enumerate(models_smalowess_ls):\n","    _ = ax.plot(corpus_texts_dt[atext][amodel], label=amodel, alpha=0.3)\n","    # _ = ax.plot(corpus_texts_dt[atext][models_smalowess_ls].plot(title=f'{corpus_titles_dt[atext][0]}')\n","\n","  _ = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  _ = ax.grid(True, alpha=0.7)\n","  _ = ax.set_title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window={Window_Percent}) + LOWESS (frac=1./{Inv_Fraction}))\\nClipped with IQR + zScore Standardized')\n","  fig.show();"],"metadata":{"id":"fmUEAaKjCjUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Global Variable to hold different Ensembles Centrality Series for each Text\n","\n","corpus_centrality_dt = {}"],"metadata":{"id":"D-EHiIksWOKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_smalowess_ls"],"metadata":{"id":"oIIRnKErExKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos list"],"metadata":{"id":"RPlTbU7xorRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# NOTE: \n","\n","# Compute, Plot, and Save Different measures of Centrality\n","\n","# TODO: Recompute for to make this code cell more robust, indep from prev cell execution?\n","# models_smalowess_ls\n","\n","\n","# Individual Model Smoothed Arcs\n","# 1. SMA(rzstd) - temporal accuracy\n","# 2. LOWESS(SMA(rzstd)) - peak detection\n","\n","# Collective Ensemble Smoothed Medians\n","# a. Median(LOWESS(SMA(rzstd)))\n","# b. LOWESS(Median(LOWESS(SMA(rzstd))))\n","# c. LOWESS(Median(SMA(rzstd)))\n","# d. LOWESS(Median(rzstd))\n","\n","# Plot SMA+LOWESS Smoothed Sentiment Arcs\n","\n","# Smoothing Parameters\n","win_per = int(Window_Percent)/100\n","afrac_inv = int(Inv_Fraction) # [10, 20, 30]\n","\n","\n","# for i, atext in enumerate(corpus_texts_dt.keys()):\n","for i, atext in enumerate(corpus_titles_ls):\n","  print(f'Text #{i}: {atext}')\n","\n","  # Create an key:(empty)value Dict entry to avoid key error upon insert\n","  corpus_centrality_dt[atext] = pd.Series()\n","\n","  # Save SMA of Robust+zScore Standized Models\n","  sent_ct = int(win_per * corpus_texts_dt[atext][amodel].shape[0])\n","  # corpus_texts_dt[atext][amodel_sma_rzstd] = corpus_texts_dt[atext][amodel_rzstd].rolling(sent_ct, center=True, min_periods=0).mean()\n","  # win_per = int(int(Window_Percent)/100 * corpus_texts_dt[atext].shape[0])\n","  # std_scaler_fl = 4.5 # 5.0\n","  # from socket import AF_AX25\n","\n","  # smalowess_ls = [x for x in corpus_texts_dt[atext].columns if x.endswith('_smalowess_rzstd')]\n","  # smalowess_ls\n","\n","\n","  fig, ax  = plt.subplots(nrows=2, ncols=1, sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n","\n","\n","  # Top Plot: Ensemble Sentiment Time Series\n","\n","  # _ = ax.plot(models_smalowess_rzstd_ls_median, label='Ensemble Median', color='r', linewidth=3) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","  for amodel in models_smalowess_ls:\n","    _ = ax[0].plot(corpus_texts_dt[atext][amodel], label=amodel, alpha=0.3, linewidth=1)\n","\n","  # TOP PLOTS\n","  # -------------------------------------------------- \n","\n","  # Take Median immediately after Robust IQR+zScore Standardization (rzstd), before Smoothing\n","  # Common Series(3 x rzstd_median_ser): Median(rzstd)\n","  # a. SMA(Median(rzstd)) \n","  # b. LOWESS(Median(rzstd))\n","  # c. LOWESS(SMA(Median(rzstd)))\n","\n","  # Take Median after Smoothing\n","  # SMA - better for localization, so do first\n","  # LOWESS -better for peak detection, so do last\n","  # apply median() first to min outliers, thereafter use mean() to avoid further compression of features (excempt for Rolling Mean=SMA use mean() even if first transformation step)\n","  # omit _rzstd_ where possible, meaning clear\n","  # Common Series(3 x sma_rzstd_ser): SMA(rzstd)\n","  # d. Median(LOWESS(SMA(rzstd)))\n","  # e. LOWESS(Median(LOWESS(SMA(rzstd))))\n","  # f. LOWESS(Median(SMA(rzstd)))\n","\n","  # Resuable subcomponents\n","  rzstd_median_ser = corpus_texts_dt[atext][models_rzstd_ls].median(axis=1)\n","  sma_rzstd_ser = corpus_texts_dt[atext][models_rzstd_ls].rolling(sent_ct, center=True, min_periods=0).mean()\n","  x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","\n","  # Plot: (a) SMA(Median(rzstd))\n","  # TODO: Debug problem with corpus_titles_ls[0] (too high, seems unnormed)\n","  plota_sma_median = corpus_texts_dt[atext][models_rzstd_ls].median(axis=1).rolling(sent_ct, center=True, min_periods=0).mean()\n","  corpus_centrality_dt[atext]['sma_median_rzstd'] = plota_sma_median\n","  _ = ax[0].plot(plota_sma_median, label='SMA(Median(rzstd))', color='r', linewidth=3, alpha=0.7) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Plot: (b) LOWESS(Median(rzstd))\n","  # Get x/y values as numpy arrays\n","  # x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","  y = rzstd_median_ser.to_numpy()\n","  # LOWESS fit\n","  sm_x, y_sm = sm_lowess(y, x,  frac=1./afrac_inv, it=5).T\n","  plotb_lowess_median = y_sm\n","  corpus_centrality_dt[atext]['lowess_median_rzstd'] = plotb_lowess_median\n","  # plot\n","  _ = ax[0].plot(plotb_lowess_median, label='LOWESS(Median(rzstd))', color='c', linewidth=3, alpha=0.7) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Plot: (c) LOWESS(SMA(Median(rzstd)))\n","  # Get x/y values as numpy arrays\n","  # x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","  y = rzstd_median_ser.rolling(sent_ct, center=True, min_periods=0).mean().to_numpy()\n","  # LOWESS fit\n","  sm_x, y_sm = sm_lowess(y, x,  frac=1./afrac_inv, it=5).T\n","  plotc_lowess_sma_median = y_sm\n","  corpus_centrality_dt[atext]['lowess_sma_median_rzstd'] = plotc_lowess_sma_median\n","  # plot\n","  _ = ax[0].plot(plotc_lowess_sma_median, label='LOWESS(SMA(Median(rzstd)))', color='b', linewidth=3, alpha=0.7) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Plot: (d) Median(LOWESS(SMA(rzstd)))\n","  plotd_median_lowess_sma = corpus_texts_dt[atext][models_smalowess_ls].median(axis=1)\n","  corpus_centrality_dt[atext]['median_lowess_sma_rzstd'] = plotd_median_lowess_sma\n","  _ = ax[0].plot(plotd_median_lowess_sma, label='Median(LOWESS(SMA(rzstd)))', color='r', linewidth=3, alpha=0.7) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Plot: (e) LOWESS(Median(LOWESS(SMA(rzstd)))) smalowess_ls\n","  # Get x/y values as numpy arrays\n","  # x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","  y = plotd_median_lowess_sma.to_numpy()\n","  # LOWESS fit\n","  sm_x, y_sm = sm_lowess(y, x,  frac=1./afrac_inv, it=5).T\n","  plote_lowess_sma_median = y_sm\n","  corpus_centrality_dt[atext]['lowess_median_lowess_sma_rzstd'] = plote_lowess_sma_median\n","  # plot\n","  _ = ax[0].plot(plote_lowess_sma_median, label='LOWESS(Median(LOWESS(SMA(rzstd))))', color='g', linewidth=3, alpha=0.7) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Plot: (f) LOWESS(Median(SMA(rzstd)))\n","  # Get x/y values as numpy arrays\n","  # x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","  y = sma_rzstd_ser.median(axis=1).to_numpy()\n","  # LOWESS fit\n","  sm_x, y_sm = sm_lowess(y, x,  frac=1./afrac_inv, it=5).T\n","  plotf_lowess_sma_median = y_sm\n","  corpus_centrality_dt[atext]['lowess_median_sma_rzstd'] = plotf_lowess_sma_median\n","  # plot\n","  _ = ax[0].plot(plotf_lowess_sma_median, label='LOWESS(Median(SMA(rzstd)))', color='g', linewidth=3, alpha=0.7) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","\n","  # Put a legend to the right of the current axis\n","  _ = ax[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","  _ = ax[0].grid(True, alpha=0.3)\n","  _ = ax[0].set_title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window=10%) + LOWESS (frac=1./{afrac_inv}\\nClipped with IQR + zScore Standardized')\n","\n","\n","\n","  # BOTTOM PLOTS: Ensemble Std and MinMax Range Time Series\n","  # --------------------------------------------------\n","\n","  # Already computed several cells above as models_smalowess_ls\n","  # smalowess_ls = [x for x in corpus_texts_dt[atext].columns if x.endswith('_smalowess_rzstd')]\n","  # smalowess_ls\n","\n","  # Get list\n","  # sma_ls = [x.replace('_smalowess_', '_') for x in smalowess_ls]\n","\n","  # TODO: MinMaxScaler() for std_ser and minmax_ser to put on same scale\n","  # Reshape numpy with reshape(-1, 1), flatten(), or ravel()\n","  # test_df = pd.DataFrame({'vader': scaler_robust.fit_transform(np.array(corpus_texts_dt[atext]['vader']).reshape(-1, 1))})\n","  # test_df['vader_std'] = pd.Series(scaler_robust.fit_transform(np.array(corpus_texts_dt[atext]['vader']).reshape(-1, 1)).flatten())\n","  # test_df['vader_std'].rolling(300, center=True, min_periods=0).mean().plot(label='RobustScaler', alpha=0.7)\n","  # test_df = pd.DataFrame({'vader': scaler_robust.fit_transform(np.array(corpus_texts_dt[atext]['vader']).reshape(-1, 1))})\n","  std_minmax_ser = pd.Series(scaler_minmax.fit_transform(np.array(corpus_texts_dt[atext][models_smalowess_ls].std(axis=1)).reshape(-1, 1)).flatten())\n","  range_minmax_ser = pd.Series(scaler_minmax.fit_transform(np.array(corpus_texts_dt[atext][models_smalowess_ls].max(axis=1).values - corpus_texts_dt[atext][models_smalowess_ls].min(axis=1).values).reshape(-1, 1)).flatten())\n","\n","  # _ = ax.plot(models_smalowess_rzstd_ls_median, label='Ensemble Median', color='r', linewidth=3) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","  _ = ax[1].plot(std_minmax_ser, label=f'Std Dev', color='r', alpha=0.7, linewidth=3)\n","  _ = ax[1].plot(range_minmax_ser, label='MinMax Range', alpha=0.7)\n","  # _ = ax.plot(corpus_texts_dt[atext][sma_ls[:model_ct]].rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', alpha=0.3, linewidth=3)\n","\n","  # Put a legend to the right of the current axis\n","  _ = ax[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  # ax[1].set_xticks(major_ticks)\n","  # ax[1].set_xticks(minor_ticks, minor=True)\n","  # ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90, ha='center')\n","\n","  _ = ax[1].grid(True, alpha=0.7)\n","  # plt.title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {model_ct} Models\\nSmoothed: SMA only (vs) SMA (window=10%) + LOWESS (frac=1./{afrac_inv}\\nClipped with IQR + zScore Standardized')\n","  _ = ax[1].set_title('Standard Deviations vs MinMax Range (Both Stadardized)')\n","  plt.show();  \n","\n"],"metadata":{"id":"KWSu3fzKnTJ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Grid Search"],"metadata":{"id":"MDbwWiEkgoyQ"}},{"cell_type":"code","source":["%whos list"],"metadata":{"id":"ekqWXWx_iVUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ensemble_ls"],"metadata":{"id":"WVfjTuvciXfh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if 'vader' in x]"],"metadata":{"id":"Kgy9FYaJhwI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title \n","import ipywidgets as widgets\n","from ipywidgets import HBox, Label\n","from ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider\n","import time\n","import pandas as pd\n","\n","#Create DF\n","df = df = pd.DataFrame(columns = ['Dropdown_column', 'Float_column'])\n","df\n","\n","# Layout\n","form_item_layout = Layout(\n","    display='flex',\n","    flex_flow='row',\n","    justify_content='space-between',\n",")\n","\n","\n","button_item_layout = Layout(\n","    display='flex',\n","    flex_flow='row',\n","    justify_content='center',\n","    padding = '5%'\n",")\n","\n","\n","# Independent dropdown item\n","\n","drop_down_input = 'Dropdown_input_1'\n","\n","drop_down = widgets.Dropdown(options=('Dropdown_input_1', 'Dropdown_input_2'))\n","\n","def dropdown_handler(change):\n","    global drop_down_input\n","    print('\\r','Dropdown: ' + str(change.new),end='')\n","    drop_down_input = change.new\n","\n","drop_down.observe(dropdown_handler, names='value')\n","\n","\n","# Dependent drop down\n","\n","# Dependent drop down elements\n","\n","dependent_drop_down_elements = {}\n","dependent_drop_down_elements['Dropdown_input_1'] = ensemble_ls # ['A', 'B']\n","dependent_drop_down_elements['Dropdown_input_2'] = ['C', 'D', 'E'] \n","\n","# Define dependent drop down\n","\n","dependent_drop_down = widgets.Dropdown(options=(dependent_drop_down_elements['Dropdown_input_1']))\n","\n","def dropdown_handler(change):\n","    global drop_down_input\n","    print('\\r','Dropdown: ' + str(change.new),end='')\n","    drop_down_input = change.new  \n","drop_down.observe(dropdown_handler, names='value')\n","\n","\n","\n","# Button\n","\n","button = widgets.Button(description='Add row to dataframe')\n","out = widgets.Output()\n","def on_button_clicked(b):\n","    global df\n","    button.description = 'Row added'\n","    time.sleep(1)\n","    with out:\n","      new_row = {'Dropdown_column': drop_down_input, 'Float_column': float_input}\n","      df = df.append(new_row, ignore_index=True)\n","      button.description = 'Add row to dataframe'\n","      out.clear_output()  \n","      display(df)\n","button.on_click(on_button_clicked)\n","\n","# Form items\n","\n","form_items = [         \n","    Box([Label(value='Independent dropdown'),\n","         drop_down], layout=form_item_layout),\n","    Box([Label(value='Dependent dropdown'),\n","         dependent_drop_down], layout=form_item_layout)\n","         ]\n","\n","form = Box(form_items, layout=Layout(\n","    display='flex',\n","    flex_flow='column',\n","    border='solid 1px',\n","    align_items='stretch',\n","    width='30%',\n","    padding = '1%'\n","))\n","display(form)\n","display(out)"],"metadata":{"id":"tj3gV0ZGj2Ls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos list"],"metadata":{"id":"iT10WqV6lYB1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_smalowess_rzstd_ls.sort()\n","models_smalowess_rzstd_str = ','.join([f'\"{x}\"' for x in models_smalowess_rzstd_ls])\n","models_smalowess_rzstd_str\n","\n","# models_smalowess_rzstd_str = ','.join(models_smalowess_rzstd_ls)\n","# models_smalowess_rzstd_str"],"metadata":{"id":"HZ684-xSlatG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://www.semicolonworld.com/question/83905/how-to-create-a-dynamic-dependent-dropdown-menu-using-ipywidgets"],"metadata":{"id":"JHulrRotkhux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title \n","import ipywidgets as widgets\n","from ipywidgets import HBox, Label\n","from ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider\n","import time\n","import pandas as pd\n","\n","#Create DF\n","# df = df = pd.DataFrame(columns = ['Dropdown_column', 'Float_column'])\n","# df\n","\n","# Layout\n","form_item_layout = Layout(\n","    display='flex',\n","    flex_flow='row',\n","    justify_content='space-between',\n",")\n","\n","\n","# Independent dropdown item\n","\n","drop_down_input = 'Dropdown_input_1'\n","\n","drop_down = widgets.Dropdown(options=(models_smalowess_rzstd_ls)) # 'Dropdown_input_1', 'Dropdown_input_2'))\n","\n","def dropdown_handler(change):\n","    global drop_down_input\n","    print('\\r','Dropdown: ' + str(change.new),end='')\n","    drop_down_input = change.new\n","\n","drop_down.observe(dropdown_handler, names='value')\n","\n","# Form items\n","\n","form_items = [         \n","    Box([Label(value='Model Preprocessed'),\n","         drop_down], layout=form_item_layout),\n","         ]\n","\n","form = Box(form_items, layout=Layout(\n","    display='flex',\n","    flex_flow='column',\n","    border='solid 1px',\n","    align_items='stretch',\n","    width='30%',\n","    padding = '1%'\n","))\n","display(form)\n","display(out)"],"metadata":{"id":"F5c_bnUOkcKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"EAs9bt_wmIFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown **Instructions:**\n","\n","#@markdown <li> Select a Smoothing Technique\n","#@markdown <li> Choose the Parameters for the Smoothing Technique Selected\n","#@markdown <li> Execute this code cell\n","\n","#@markdown <hr>\n","\n","#@markdown **Smoothing Technique**\n","Smoothing_Algo = \"LOWESS\" #@param [\"SMA\", \"LOWESS\"]\n","\n","Sample_Model = \"roberta15lg_smalowess_rzstd\" #@param models [\"afinn_smalowess_rzstd\",\"flair_smalowess_rzstd\",\"hinglish_smalowess_rzstd\",\"huggingface_smalowess_rzstd\",\"imdb2way_smalowess_rzstd\",\"nlptown_smalowess_rzstd\",\"pattern_smalowess_rzstd\",\"pysentimentr_huliu_smalowess_rzstd\",\"pysentimentr_jockersrinker_smalowess_rzstd\",\"pysentimentr_lmcd_smalowess_rzstd\",\"pysentimentr_nrc_smalowess_rzstd\",\"pysentimentr_senticnet_smalowess_rzstd\",\"pysentimentr_sentiword_smalowess_rzstd\",\"roberta15lg_smalowess_rzstd\",\"robertaxml8lang_smalowess_rzstd\",\"sentimentr_huliu_smalowess_rzstd\",\"sentimentr_jockers_smalowess_rzstd\",\"sentimentr_jockersrinker_smalowess_rzstd\",\"sentimentr_loughran_mcdonald_smalowess_rzstd\",\"sentimentr_nrc_smalowess_rzstd\",\"sentimentr_senticnet_smalowess_rzstd\",\"sentimentr_sentiword_smalowess_rzstd\",\"sentimentr_socal_google_smalowess_rzstd\",\"stanza_smalowess_rzstd\",\"syuzhetr_afinn_smalowess_rzstd\",\"syuzhetr_bing_smalowess_rzstd\",\"syuzhetr_nrc_smalowess_rzstd\",\"syuzhetr_syuzhet_smalowess_rzstd\",\"t5imdb50k_smalowess_rzstd\",\"textblob_smalowess_rzstd\",\"vader_smalowess_rzstd\",\"yelp_smalowess_rzstd\"]\n","\n","#@markdown <hr>\n","\n","#@markdown **Window Percent for SMA Smoothing (default 10%)**\n","Window_Start = 6 #@param {type:\"slider\", min:2, max:10, step:1}\n","Window_End = 14 #@param {type:\"slider\", min:10, max:20, step:1}\n","Window_Step = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n","\n","win_start_int = int(Window_Start)\n","win_end_int = int(Window_End)\n","win_step_int = int(Window_Step)\n","\n","#@markdown <hr>\n","\n","#@markdown **Inverse Fraction for LOWESS Smoothing (default 20)**\n","Inv_Frac_Start = 10 #@param {type:\"slider\", min:2, max:20, step:1}\n","Inv_Frac_End = 30 #@param {type:\"slider\", min:20, max:30, step:1}\n","Inv_Frac_Step = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n","\n","invfrac_start_int = int(Inv_Frac_Start)\n","invfrac_end_int = int(Inv_Frac_End) # + 1\n","invfrac_step_int = int(Inv_Frac_Step)\n","\n"],"metadata":{"id":"duzb5qOugoyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Grid Search over the Smoothing Hyperparameter Space\n","\n","if Smoothing_Algo == 'SMA':\n","\n","  for i, win_size in enumerate(range(win_start_int, win_end_int, win_step_int)):\n","    print(f'Loop #{i}: {win_size}')\n","\n","    # Verify Ensemble SMA+LOWESS Arcs for all Texts\n","\n","    for i, atext in enumerate(corpus_titles_ls):\n","      print(f'Plotting {corpus_titles_dt[atext][0]}:')\n","\n","      fig = plt.figure()\n","      ax = plt.subplot(111)\n","\n","\n","      _ = ax.plot(corpus_texts_dt[atext][Sample_Model], label=amodel, alpha=0.3)\n","      # _ = ax.plot(corpus_texts_dt[atext][models_smalowess_ls].plot(title=f'{corpus_titles_dt[atext][0]}')\n","\n","      _ = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","      _ = ax.grid(True, alpha=0.7)\n","      _ = ax.set_title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window={Window_Percent}) + LOWESS (frac=1./{Inv_Fraction}))\\nClipped with IQR + zScore Standardized')\n","      fig.show();\n","\n","elif Smoothing_Algo == 'LOWESS':\n","\n","  for i, frac_size in enumerate(range(invfrac_start_int, invfrac_end_int, invfrac_step_int)):\n","    print(f'Loop #{i}: {frac_size}')\n","\n","    # Verify Ensemble SMA+LOWESS Arcs for all Texts\n","\n","    for i, atext in enumerate(corpus_titles_ls):\n","      print(f'Plotting {corpus_titles_dt[atext][0]}:')\n","\n","      fig = plt.figure()\n","      ax = plt.subplot(111)\n","\n","      for j, amodel in enumerate(models_smalowess_ls):\n","        _ = ax.plot(corpus_texts_dt[atext][amodel], label=amodel, alpha=0.3)\n","        # _ = ax.plot(corpus_texts_dt[atext][models_smalowess_ls].plot(title=f'{corpus_titles_dt[atext][0]}')\n","\n","      _ = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","      _ = ax.grid(True, alpha=0.7)\n","      _ = ax.set_title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window={Window_Percent}) + LOWESS (frac=1./{Inv_Fraction}))\\nClipped with IQR + zScore Standardized')\n","      fig.show();\n","\n","else:\n","  print(f'ERROR: Illegal value for Smoothing_Algo: {Smoothing_Algo}')"],"metadata":{"id":"mL88d5ZCgoyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"H_Dc8-jhgoyQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [TEMP] Rename folders under SentimentArcs to be more intuitive"],"metadata":{"id":"NGiNpjFwL5fa"}},{"cell_type":"code","source":["os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')"],"metadata":{"id":"SpYEaqlHJrrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %cd ../sentiment_clean\n","!ls -altr"],"metadata":{"id":"6ZVatkoyJ9Ve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !mv social_clean_social_new sentiment_clean_social_new\n","# !mv finance_clean_social_new sentiment_clean_finance_new"],"metadata":{"id":"O9VUIg6QregZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !mkdir social_clean_social_new"],"metadata":{"id":"HhCF4jVBtL28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","!mkdir timeseries_clean_novels_new\n","!mkdir timeseries_clean_novels_ref\n","!mkdir timeseries_clean_social_new\n","!mkdir timeseries_clean_social_ref\n","!mkdir timeseries_clean_finance_new\n","!mkdir timeseries_clean_finance_ref\n","\n","!ls -altr\n","\"\"\";"],"metadata":{"id":"MKCJwtFvHXTP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V9QSeIStrc11"},"source":["## Save Checkpoint (Dict of DataFiles)"]},{"cell_type":"code","source":["# Review all variations of each model being saved\n","\n","[x for x in corpus_texts_dt[corpus_titles_ls[0]] if 'vader' in x]"],"metadata":{"id":"exUZVoFiGCl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Delete text_clean, text_raw from all files downstream of text_clean to save space\n","\n","[x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if 'text' in x]"],"metadata":{"id":"3RNDTKGyTKwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Global Variable\n","\"\"\"\n","SUBDIR_SENTIMENTARCS = '/gdrive/MyDrive/cdh/sentiment_arcs'\n","SUBDIR_TIMESERIES_RAW = f'/timeseries_raw/timeseries_raw_{Corpus_Genre}_{Corpus_Type}'\n","SUBDIR_TIMESERIES_CLEAN = f'/timeseries_raw/timeseries_clean_{Corpus_Genre}_{Corpus_Type}'\n","SUBDIR_TIMESERIES_RAW\n","\"\"\";"],"metadata":{"id":"H83fv0GmMw1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note Structure of corpus_distance_dt[atext] = \n","# (model, distance) for all models\n","# (distance, silimarity_metric e.g. mahattan)\n","# (baseline, time_series of Ensemble Centrality Series, long)\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(SUBDIR_SENTIMENTARCS)\n","\n","print(f'\\nSaving to Subdirectory:\\n  {SUBDIR_TIMESERIES_RAW}\\n\\n')\n","print('--------------------------------------------------\\n\\n')\n","\n","# Save Each Text Distance Dictionary as a Separate File\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f'Saving Timeseries #{i}: [{atext}]')\n","  # Generate Unique Filename\n","  filename_save = f'timeseries_raw_{Corpus_Genre}_{Corpus_Type}_{atext}.csv'\n","  print(f'             to file: [{filename_save}]')\n","  print(f'           in subdir: [{SUBDIR_TIMESERIES_RAW}]\\n\\n')\n","  corpus_texts_dt[atext].head()\n","  corpus_texts_dt[atext].to_csv(f'.{SUBDIR_TIMESERIES_RAW}/{filename_save}')\n","\n","  # pretty = json.dumps(corpus_texts_dt[atext], indent=4)\n","  # print(pretty)\n"],"metadata":{"id":"_cVGcsRorc15"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 4] Evaluate Ensemble on Texts**\n","\n","* (code) ts_dtw_clustering_dtaidistance_emo_20211201(1).ipynb"],"metadata":{"id":"H8yJbfBp9jeL"}},{"cell_type":"markdown","source":["## Select Ensemble Baseline and Model Preprocessing"],"metadata":{"id":"i1v4CM7M9E2n"}},{"cell_type":"code","source":["#@markdown **Choose Type of Model Proprocessing (default - SMA+LOWESS):**\n","\n","Normalized_Series = 'SMA+LOWESS' #@param ['SMA', 'SMA+LOWESS']\n","\n","#@markdown **Choose Type of Ensemble Centrality (default - lowess_sma_median_rzstd):**\n","\n","Centrality_Series = 'lowess_median_lowess_sma_rzstd' #@param ['sma_median_rzstd', 'lowess_median_rzstd', 'lowess_sma_median_rzstd', 'median_lowess_sma_rzstd', 'lowess_median_lowess_sma_rzstd', 'lowess_median_sma_rzstd']\n","\n","\n","for atext in corpus_titles_ls:\n","  # atitle = f'Sentiment Analysis\\n{corpus_titles_dt[atext][0]}\\nCentrality Series: {Centrality_Series}'\n","  \n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  if Normalized_Series == 'SMA':\n","    models_preproc_ls = models_sma_rzstd_ls\n","  elif Normalized_Series == 'SMA+LOWESS':\n","    models_preproc_ls = models_smalowess_rzstd_ls\n","  else:\n","    print(f'ERROR: Invalid value for Normalized_Series: {Normalized_Series}')\n","\n","  # Plot ALL Ensemble Models for a Text \n","  for amodel in models_preproc_ls:\n","    _ = ax.plot(corpus_texts_dt[atext][amodel], label=amodel, alpha=0.3)\n","\n","  # Plot ONE Ensemble Baseline for a Text\n","  _ = ax.plot(corpus_centrality_dt[atext][Centrality_Series], label=Centrality_Series, color='red', linewidth=3, alpha=0.7)\n","\n","  # Put a legend to the right of the current axis\n","  _ = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  plt.grid(True, alpha=0.7)\n","  _ = ax.set_title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window={Window_Percent}) + LOWESS (frac=1./{Inv_Fraction}))\\nClipped with IQR + zScore Standardized')\n","  plt.show();"],"metadata":{"id":"KutLns5GDA22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Ensemble Models and Variations for each Model\n","\n","temp_ls = corpus_texts_dt[corpus_texts_ls[0]].columns\n","\n","[x for x in temp_ls if '_rzstd' in x]\n","\n","print(f'\\n\\nThere are {len(temp_ls)} Columns/Models\\n\\n')\n","\n","[x for x in corpus_texts_dt[corpus_texts_ls[0]].columns if 'vader' in x]"],"metadata":{"id":"G_NgM7KsK6Oc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Simple Correlation Matricies"],"metadata":{"id":"MHTbao0dSTLr"}},{"cell_type":"code","source":["# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n","# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n","\n","Correlation_Metric = \"spearman\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n","# corr_methods_ls = ['pearson', 'spearman', 'kendall']\n","\n","col_rzstd_ls = []\n","for amodel in ensemble_ls:\n","  acol_rzstd = f'{amodel}_rzstd'\n","  col_rzstd_ls.append(acol_rzstd)\n","print(f'\\n\\nDEFAULT Models:\\n    col_rzstd_ls: {col_rzstd_ls}')\n","\n","# OPTIONAL EDIT: Manually select problematic model to remove from analysis \n","#                (e.g. Pattern can misbehave at times)\n","model_root_bad = 'zzz'\n","# model_root_bad = ''\n","col_rzstd_ls = [x for x in col_rzstd_ls if model_root_bad not in x]\n","print(f'\\n\\nMODIFED Models:\\n    col_rzstd_ls: {col_rzstd_ls}\\n\\n')\n","\n","for atext in corpus_texts_ls:\n","\n","  corr_df = corpus_texts_dt[atext][col_rzstd_ls].dropna(axis=0, how='any').corr(method=Correlation_Metric)\n","\n","  # Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n","\n","  heat_fig = sns.clustermap(corr_df, \n","                      # corpus_sents_df[col_rzstd_ls].dropna(axis=0, how='any').corr(method=corr_method),\n","                      row_cluster=True,\n","                      col_cluster=True,\n","                      annot=True,\n","                      # annot_kws={\"size\": 15},\n","                      figsize=(20, 20))\n","\n","  # plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n","  # plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n","\n","  heat_title = f'{corpus_titles_dt[atext][0]}\\n{Correlation_Metric.capitalize()} Correlation of Sentiment Time Series\\nIQR Clip + zScore Standardization'\n","  _ = heat_fig.fig.suptitle(heat_title, y=1.05, fontsize=20)\n","  \n","  plt.show();\n"],"metadata":{"id":"uNg8ceOD9JlR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Rank Distance from Ensemble Median"],"metadata":{"id":"zVVXV5ln9RFj"}},{"cell_type":"code","source":["# Globals variable\n","\n","corpus_distance_dt = {}"],"metadata":{"id":"PbVQJLD3x5Wu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Compute Ensemble Median and each Time Series\n","# https://stats.stackexchange.com/questions/185912/alternate-distance-metrics-for-two-time-series\n","\n","# https://tslearn.readthedocs.io/en/stable/gen_modules/tslearn.metrics.html \n","\n","# https://www.semanticscholar.org/paper/A-Formally-Robust-Time-Series-Distance-Metric-Toller-Geiger/a6b598bfa5d679003b20a292839a93e7c0cd3705 (2020 0c)\n","# https://www.semanticscholar.org/paper/A-review-on-distance-based-time-series-Abanda-Mori/1a22c4fcff260a8623a1fd677f2e7a7916343dae (2018 75c)\n","\n","# Magnitude and Temporal Alignment: Euclidian (better for removed outliers/smoothed arcs)\n","\n","# Overall Shape w/o excessive distortions: DWT with Global Constraints (Sakoe-Chiba band and the Itakura Parallelogram)\n","\n","# LCSS - Longest Common Subsequence - has been originally developed to analyse string similarity but can also be used for numerical time series\n"],"metadata":{"id":"jN1TloWE9Jcl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# NOTE:      >1s @18:05 on 20220310 Colab Pro/CPU:euclidean   (2 Novels/32 Models)\n","#            >1s @18:06 on 20220310 Colab Pro/CPU:manhattan   (2 Novels/32 Models)\n","#          1m28s @15:50 on 20220310 Colab Pro/CPU:dtw    (2 Novels/32 Models)\n","#          3m17s @17:47 ON 20220310 Colab Pro/CPU:ctw    (2 Novels/32 Models)\n","#          1m20s @17:47 ON 20220310 Colab Pro/CPU:lcss   (2 Novels/32 Models) returns matrix\n","#         11m47s @17:53 ON 20220310 Colab Pro/CPU:soft_dtw  (2 Novels/32 Models) all same\n","#        ~10m20s @17:47 ON 20220310 Colab Pro/CPU:soft_dtw_alignment  (2 Novels/32 Models) - returns matrix\n","\n","# Rank Each Model by Distance from Ensemble Baseline (Choosen Centrality Metric)\n","\n","corpus_distance_dt = {}\n","\n","#@markdown **Select a Metric to measure Distance from the Baseline:** \n","# Distance_Metric = \"lcss\" #@param [\"euclidean\", \"manhattan\", \"dtw\", \"soft_dtw\", \"soft_dtw_alignment\", \"ctw\", \"lcss\", \"gak\"]\n","Distance_Metric = \"euclidean\" #@param [\"euclidean\", \"manhattan\", \"dtw\", \"ctw\"]\n","\n","#@markdown **NOTE:** Distances are stored in a Dict of Dicts with 2 metadata cols (distance, baseline)\n","\n","print('REVIEW Selections:')\n","print('------------------\\n')\n","print(f'Ensemble Baseline Centrality Metric: {Centrality_Series}')\n","print(f'    Model Preprocessing & Smoothing: {Normalized_Series}')\n","print(f'                 Correlation Metric: {Correlation_Metric.capitalize()}')\n","print(f'                    Distance Metric: {Distance_Metric}\\n\\n')\n","\n","# Get and Compare each Model Series with the Ensemble Baseline Centrality Series\n","for i, atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text: #{i} {atext}')\n","  print('----------------------------------------------')\n","\n","  # Get Ensemble Baseline Centrality Series\n","  # ensemble_baseline = corpus_centrality_dt[atext][Centrality_Series]\n","\n","  # Get the Preprocessed & Smoothed Model Series \n","  if Normalized_Series == 'SMA':\n","    models_preproc_ls = models_sma_rzstd_ls\n","  elif Normalized_Series == 'SMA+LOWESS':\n","    models_preproc_ls = models_smalowess_rzstd_ls\n","  else:\n","    print(f'ERROR: Invalid value for Normalized_Series: {Normalized_Series}')\n","\n","  # Compare each Proprocessed&Smoothed Model with the Chosen Ensemble Baseline Centrality Series\n","  models_distance_dt = {}\n","  for j, amodel in enumerate(models_preproc_ls):\n","    print(f'          Model: #{j} {amodel}')\n","    print(f'      Baseline: {Centrality_Series}\\n      vs Model: {amodel}')\n","\n","    if Distance_Metric == 'euclidean':\n","      adist = euclidean(np.array(corpus_texts_dt[atext][amodel]), np.array(corpus_centrality_dt[atext][Centrality_Series]))\n","    elif Distance_Metric == 'manhattan':\n","      adist = cityblock(np.array(corpus_texts_dt[atext][amodel]), np.array(corpus_centrality_dt[atext][Centrality_Series]))\n","    elif Distance_Metric == 'dtw':\n","      adist = dtw(np.array(corpus_texts_dt[atext][amodel]), np.array(corpus_centrality_dt[atext][Centrality_Series]))\n","    elif Distance_Metric == 'soft_dtw':\n","      adist = soft_dtw(np.array(corpus_texts_dt[atext][amodel]), np.array(corpus_centrality_dt[atext][Centrality_Series]))\n","    elif Distance_Metric == 'soft_dtw_alignment':\n","      adist = soft_dtw_alignment(np.array(corpus_texts_dt[atext][amodel]), np.array(corpus_centrality_dt[atext][Centrality_Series]))\n","    elif Distance_Metric == 'ctw':\n","      adist = ctw(np.array(corpus_texts_dt[atext][amodel]), np.array(corpus_centrality_dt[atext][Centrality_Series]))\n","    elif Distance_Metric == 'lcss':\n","      adist = lcss(np.array(corpus_texts_dt[atext][amodel]), np.array(corpus_centrality_dt[atext][Centrality_Series]))    \n","    else:\n","      print(f'ERROR: Illegal value for Distance_Metric: {Distance_Metric}')\n","  \n","    print(f'      Distance: {adist}\\n\\n')\n","    models_distance_dt[amodel] = float(adist)\n","  \n","  # Sort models_distance_dt in ascending order, models closest to ensemble central baseline first\n","  # ONLY Python 3.7+: {k: v for k, v in sorted(x.items(), key=lambda item: item[1])} \n","  models_distance_sorted_dt = {k:v for k,v in sorted(models_distance_dt.items(), key=lambda item: float(item[1]))}\n","  models_distance_sorted_dt['distance_metric'] = Distance_Metric\n","  models_distance_sorted_dt['baseline'] = corpus_centrality_dt[atext][Centrality_Series]\n","  corpus_distance_dt[atext] = copy.deepcopy(models_distance_sorted_dt)\n"],"metadata":{"id":"QA6AzeMmoNLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Sample Text Model Count and Model Names\n","\n","atext\n","print('\\n')\n","corpus_distance_dt[atext].keys()\n","print('\\n')\n","print(f'[{len(list(corpus_distance_dt[atext].keys()))}] Models')"],"metadata":{"id":"jkT3z8gagSbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove metadata cols (distance,baseline) and rank the remaining Models by chosen distance metric\n","\n","corpus_distance_dt.keys()\n","print('\\n')\n","corpus_distance_dt[corpus_titles_ls[0]].items()\n","print('\\n')\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  # print(f'\\n\\nProcessing Text: #{i} {atext}')\n","  # print('----------------------------------------------')\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  D = copy.deepcopy(corpus_distance_dt[atext])\n","  # Pop off the two metadata key:values that are not Model Time Series\n","  distance = D.pop('distance_metric', None)\n","  baseline = D.pop('baseline', None)\n","\n","  _ = ax.bar(range(len(D)), list(D.values()), align='center')\n","  _ = ax.set_xticks(np.arange(len(D)))\n","  # _ = ax.set_xticks(range(len(D)), list(D.values()))\n","  # _ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\n","  labels_clean = [x.split('_sma')[0] for x in D.keys()]\n","  # _ = ax.set_xticklabels(D.keys(), fontsize=12 , rotation=90, ha='right')\n","  _ = ax.set_xticklabels(labels_clean, fontsize=12 , rotation=90, ha='right')\n","  # _ = ax.set_xticklabels(rotation=90, ha='right')\n","  # _ = ax.bar_label(ax.containers[0]) # matplotlib >=3.4.0\n","  # Label Bars\n","  for i, v in enumerate(D.values()):\n","    _ = ax.text(i, v + .25, str(f'{v:.3f}'), color='white', fontweight='bold', rotation=90, va='top')\n","\n","  atitle = f\"{corpus_titles_dt[atext][0]}\\nModel Distance from SentimentArc Ensemble Center Baseline of {len(ensemble_ls)} Models\\nPreprocessing & Smoothing: ({Normalized_Series})\\nEnsemble Central Baseline: ({Centrality_Series}) using Metric: ({Distance_Metric})\"\n","\n","  _ = ax.grid(True, alpha=0.5)\n","  _ = ax.set_title(atitle)\n","  plt.show();"],"metadata":{"id":"OasjcSR7OF3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_ls"],"metadata":{"id":"wXjGN_l-v1x2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_distance_dt.keys()"],"metadata":{"id":"3HSCXvphv6j2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_distance_dt['scollins_thehungergames1'].keys()"],"metadata":{"id":"RY0gNm5ywHuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify plaintext ranking of distance of each model from ensemble central \n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text: #{i} {atext}')\n","  print('----------------------------------------------')\n","\n","  amodel_distance_dt = {}\n","\n","  D = corpus_distance_dt[atext]\n","  print(f'Text: {atext} (Sorted by distance to Ensemble Central Baseline):\\n')\n","  for amodel,adistance in D.items():\n","    if amodel in ['baseline', 'distance_metric']:\n","      continue\n","    else:\n","      print(f' {amodel :.<50}{adistance:.3f}')\n","\n","    amodel_distance_dt[amodel] = adistance\n","  \n","  # Sort models_distance_dt in ascending order, models closest to ensemble central baseline first\n","  # ONLY Python 3.7+: {k: v for k, v in sorted(x.items(), key=lambda item: item[1])} \n","\n","  # models_distance_sorted_dt = {k:v for k,v in sorted(models_distance_dt.items(), key=lambda item: item[1])}\n","  # corpus_distance_dt[atext] = models_distance_sorted_dt\n","\n","  # models_distance_sorted_dt = {k:v for k,v in sorted(models_distance_dt.items(), key=lambda item: item[1])}\n","  corpus_distance_dt[atext] = amodel_distance_dt"],"metadata":{"id":"bdCMQfWMyx-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_55gz93LAB3"},"source":["## Save Checkpoint (Dict of Dicts)"]},{"cell_type":"code","source":["# Write Model Distances Dict of Dicts to .json file\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(SUBDIR_SENTIMENTARCS)\n","\n","for i, atext in enumerate(corpus_distance_dt.keys()):\n","  print(f'Saving Timeseries #{i}: [{atext}]')\n","  # Generate Unique Filename\n","  fname_distance_json = f'timeseries_distance_{Distance_Metric.lower()}_raw_{Corpus_Genre}_{Corpus_Type}_{atext}.json'\n","  fname_distance_json_path = f'.{SUBDIR_TIMESERIES_RAW}/{fname_distance_json}'\n","  print(f'             to file: [{fname_distance_json}]')\n","  print(f'           in subdir: [{SUBDIR_TIMESERIES_RAW}]\\n\\n')\n","\n","  atext_distance_dt = corpus_distance_dt[atext]\n","  dumped_dt2json = json.dumps(atext_distance_dt, cls=NumpyEncoder)\n","  with open(fname_distance_json_path, 'w') as fp:\n","      fp.write(dumped_dt2json + '\\n') "],"metadata":{"id":"IbzTk1Dsz3Wa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Saved json dump by reading back in\n","\n","test_dt = {}\n","\n","for i, atext in enumerate(corpus_distance_dt.keys()):\n","  print(f'     Text #{i}: {atext}')\n","  atext_distance_dt = corpus_distance_dt[atext]\n","  # print(f'        Type: {type(list(atext_distance_dt.keys()))}')\n","  # atext_distance_dt.keys()\n","\n","  fname_distance_json = f'timeseries_distance_{Distance_Metric.lower()}_raw_{Corpus_Genre}_{Corpus_Type}_{atext}.json'\n","  fname_distance_json_path = f'.{SUBDIR_TIMESERIES_RAW}/{fname_distance_json}'\n","\n","  print(f'Reading File: {fname_distance_json}\\n')\n","  with open(fname_distance_json_path, 'r') as fp:\n","    test_dt = json.load(fp)\n","\n","  # print(json.dumps(test_dt, indent=4))\n","  # pprint.pprint(test_dt, width=1)\n","  print(f'Just read {atext}:')\n","  test_dt "],"metadata":{"id":"IykEBjOkz3Wa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [SKIP]"],"metadata":{"id":"-RzweFCnLAB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lowess_grid_dt = {}\n","crux_ct_ls = []\n","# temp_df['sent_no'] = pd.Series([x for x in corpus_sents_df['sent_no']])\n","temp_df['avg_stdscaler'] = corpus_sents_df[models_subset_ls].mean()\n","\n","fig = plt.figure()\n","ax = plt.axes()\n","\n","\n","for afrac in range(frac_start_int, frac_end_int, frac_step_int):\n","  print(f'Processing afrac = {afrac}')\n","  # Compute error between subset of models\n","  afrac_fl = afrac/100\n","  temp_df = get_lowess(corpus_sents_df, models_ls=models_subset_ls, text_unit='sentence', afrac=afrac_fl, do_plot=False);\n","  temp_df['minmax_diff'] = temp_df.max(axis=1) - temp_df.min(axis=1)\n","  diff_sum = temp_df['minmax_diff'].sum()\n","  print(f\"  Sum(minmax_diff): {diff_sum}\");\n","  lowess_grid_dt[afrac] = diff_sum\n","  # Compute Crux Points\n","  temp_df['sent_no'] = pd.Series(list(range(temp_df.shape[0])))\n","  crux_ls = get_crux_points(temp_df,\n","                            'median',\n","                            text_type='sentence', \n","                            win_per=5, \n","                            sec_y_labels=False, \n","                            sec_y_height=0, \n","                            subtitle_str=' ', \n","                            do_plot=False,\n","                            save2file=False)\n","  ax.plot(temp_df['sent_no'], temp_df['median'], label=f'frac={afrac}')\n","  # plt.plot(data=temp_df, x='sent_no', y='median', label=f'frac={afrac}')\n","  crux_ct_ls.append(len(crux_ls))\n","  print(f'  {len(crux_ls)} Crux Points')\n","\n","plt.title(f\"{CORPUS_FULL} \\n LOWESS Smoothing Grid Search (frac={Frac_Start} to {Frac_End}\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"yHRu4carLAB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# np.array(corpus_texts_dt[atext][amodel_std]).reshape(-1,1).shape\n","\n","np.array(corpus_texts_dt[atext][amodel_std]).reshape(-1,).shape\n","\n","np.array(corpus_texts_dt[atext][amodel_std]).flatten().shape"],"metadata":{"id":"o1oI4RWPLAB5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Agglomerative Hierarichal Clustering"],"metadata":{"id":"gcXWuUhj9KSD"}},{"cell_type":"code","source":["# Global Variable\n","\n","corpus_lttb_dt = {}"],"metadata":{"id":"bh3BhZ8yUTES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Ensemble Models and Variations for each Model\n","\n","temp_ls = corpus_texts_dt[corpus_texts_ls[0]].columns\n","\n","[x for x in temp_ls if '_rzstd' in x]\n","\n","print(f'\\n\\nThere are {len(temp_ls)} Columns/Models\\n\\n')\n","\n","[x for x in corpus_texts_dt[corpus_texts_ls[0]].columns if 'vader' in x]"],"metadata":{"id":"CY-dey1N9JgK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get list of model values that are (a) Clipped with IRQ to treat outliers and (b) zScore Standardized\n","\n","models_rzstd_ls = [x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if (x.endswith('_rzstd') & ~x.endswith('_smalowess_rzstd') & ~x.endswith('_sma_rzstd'))]\n","print('\\n'.join(models_rzstd_ls))\n","\n","print('\\n\\nModel with values that are (a) Clipped with IRQ to treat outliers and (b) zScore Standardized')"],"metadata":{"id":"fHlQNqmCa0go"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LTTB\n","\n","* Dimensionality Reduction\n","* Extract Features (Peaks)\n","* Overall Arc & Vertical Feature Similarity w/DTW"],"metadata":{"id":"n6mXtwBFSLNF"}},{"cell_type":"code","source":["import lttb # https://git.sr.ht/~javiljoen/lttb-numpy (202110 )\n","\n","# import lttbc # https://github.com/dgoeries/lttbc (20201020 7stars)\n","from lttb.validators import *\n","\n","# Global Variable\n","LTTB_CT = 100\n","\n","corpus_lttb_dt = {}"],"metadata":{"id":"MPmjiLxBSR9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare SMA vs SMA+LOWESS Smoothed Sentiments\n","\n","_ = corpus_texts_dt[corpus_titles_ls[0]]['vader_smalowess_rzstd'].plot(label='SMA+LOWESS', alpha=0.3)\n","_ = corpus_texts_dt[corpus_titles_ls[0]]['vader_rzstd'].rolling(300, center=True, min_periods=0).mean().plot(label='SMA', alpha=0.3)"],"metadata":{"id":"gK0-mk5jT8gK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LTTB_CT = 100\n","\n","# Get np.arrays of test Sentiment TS\n","test_ts_vals_np = corpus_texts_dt[corpus_titles_ls[0]]['vader_rzstd'].rolling(300, center=True, min_periods=0).mean().values # .reshape(-1,1)\n","# type(test_ts_vals_np)\n","test_ts_vals_np.shape\n","test_ts_indx_np = np.arange(test_ts_vals_np.shape[0])\n","test_ts_indx_np.shape\n","\n","test_ts_np = np.column_stack((test_ts_indx_np, test_ts_vals_np))\n","test_ts_np.shape\n","\n","# Downsample with stricter validators\n","test_ts_sm_np = lttb.downsample(test_ts_np, n_out=LTTB_CT, validators=[has_two_columns, x_is_regular])\n","type(test_ts_sm_np)\n","test_ts_sm_indx_np = np.arange(test_ts_sm_np.shape[0])\n","test_ts_sm_indx_np.shape\n","\n","# Plot\n","plt.plot(test_ts_indx_np, test_ts_vals_np.transpose())\n","# plt.plot(test_ts_indx_np, test_ts_vals_np.transpose())\n","plt.show()\n","\n","plt.plot(test_ts_sm_indx_np, test_ts_sm_np.transpose()[1])\n","# plt.plot(test_ts_indx_np, test_ts_vals_np[1].transpose()[1])\n","plt.show();"],"metadata":{"id":"TjzAiREoVIFR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scale_mult*100"],"metadata":{"id":"eCwdXDiq-vem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[atext][amodel_rzstd].shape"],"metadata":{"id":"SKdYfgLE-zSa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["amodel_rzstd"],"metadata":{"id":"wEMhvyL0DDWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test and see how original Model (*_rzstd) compares to 100 point downsampled LTTB\n","\n","atext = corpus_titles_ls[0]\n","amodel_rzstd = 'vader_rzstd'\n","\n","temp_lttb_df = pd.DataFrame()\n","\n","# Get Sentiment Values Col\n","temp_ts_vals_np = corpus_texts_dt[atext][amodel_rzstd].rolling(win_10per, center=True, min_periods=0).mean().values # .reshape(-1,1)\n","# type(temp_ts_vals_np)\n","# temp_ts_np.shape\n","# Generate Index Col\n","temp_ts_indx_np = np.arange(temp_ts_vals_np.shape[0])\n","# temp_ts_indx_np.shape\n","# Col Stack\n","temp_ts_np = np.column_stack((temp_ts_indx_np, temp_ts_vals_np))\n","# temp_ts_np.shape\n","\n","# Downsample with stricter validators\n","# NOTE: 'x_is_regular' makes storage/comparison easier by sacrificing accuracy for smaller LTTB_CT\n","temp_ts_vals_sm_np = lttb.downsample(temp_ts_np, n_out=LTTB_CT, validators=[has_two_columns, x_is_regular])\n","# type(temp_ts_vals_sm_np)\n","temp_ts_sm_indx_np = np.arange(temp_ts_vals_sm_np.shape[0])\n","# temp_ts_sm_indx_np.shape\n","\n","# Impute Sentence Number corresponding to Datapoints\n","orig_len = temp_ts_vals_np.shape[0]\n","temp_sm_len = temp_ts_vals_sm_np.shape[0]\n","scale_mult = orig_len/temp_sm_len\n","# print(f'scale_mult: {scale_mult}')\n","\n","# TODO: Fix the horizontal shift early and esp late in the LTTB time series\n","# sentno_np = np.round(scale_mult * temp_ts_sm_indx_np)\n","sentno_np = (scale_mult + 1) * temp_ts_sm_indx_np\n","\n","# Save\n","# temp_lttb_dt = {\"sentno\" : list(sentno_np), \"sentiment\" : list(temp_ts_vals_sm_np)}\n","# temp_lttb_df[amodel_rzstd] = pd.DataFrame(temp_lttb_dt)\n","temp_lttb_df[amodel_rzstd] = pd.Series(list(temp_ts_vals_sm_np))\n","\n","# Plot\n","plt_title = f'{corpus_titles_dt[atext][0]} Standardized Sentiment\\nModel: {amodel_rzstd} Smoothed (SMA 10%)\\n{temp_ts_indx_np.shape[0]} Datapoints'\n","_ = plt.plot(temp_ts_indx_np, temp_ts_vals_np.transpose())\n","_ = plt.title(plt_title)\n","# plt.plot(temp_ts_indx_np, temp_ts_vals_np.transpose())\n","# plt.show()\n","\n","plt_title = f'{corpus_titles_dt[atext][0]} Standardized Sentiment\\nModel: {amodel_rzstd} Smoothed (SMA 10%)\\nReduced to {LTTB_CT} Datapoints via LTTB'\n","# plt.plot(temp_ts_sm_indx_np, temp_ts_sm_np.transpose()[1])\n","_ = plt.plot(sentno_np, temp_ts_vals_sm_np.transpose()[1])\n","_ = plt.title(plt_title)\n","# plt.plot(temp_ts_indx_np, temp_ts_vals_np[1].transpose()[1])\n","plt.show();"],"metadata":{"id":"__JuukUqUbjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentno_np[-5:]"],"metadata":{"id":"gE7RSdyI_WMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_ts_sm_indx_np"],"metadata":{"id":"E4ClG0Mkge7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# NOTE: 43s\n","\n","# Compare SMA with LTTB Approximation\n","\n","corpus_lttb_dt = {}\n","\n","LTTB_CT = 100 # [50, 75, 100, 200, 500, 1000]\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f'Text #{i}: {atext}')\n","\n","  # Get 10% Window Size for SMA\n","  win_10per = int(0.10 * corpus_texts_dt[atext].shape[0])\n","  \n","  # Get list of model values that are (a) Clipped with IRQ to treat outliers and (b) zScore Standardized\n","  models_rzstd_ls = [x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if (x.endswith('_rzstd') & ~x.endswith('_smalowess_rzstd') & ~x.endswith('_sma_rzstd'))]\n","\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  # models_std_ls = []\n","  temp_lttb_df = pd.DataFrame()\n","  for j, amodel_rzstd in enumerate(models_rzstd_ls):\n","\n","    # Get Sentiment Values Col\n","    temp_ts_vals_np = corpus_texts_dt[atext][amodel_rzstd].rolling(win_10per, center=True, min_periods=0).mean().values # .reshape(-1,1)\n","    \n","    # type(temp_ts_vals_np)\n","    # temp_ts_np.shape\n","    # Generate Index Col\n","    temp_ts_indx_np = np.arange(temp_ts_vals_np.shape[0])\n","    # temp_ts_indx_np.shape\n","    # Col Stack\n","    temp_ts_np = np.column_stack((temp_ts_indx_np, temp_ts_vals_np))\n","    # temp_ts_np.shape\n","\n","    # Downsample with stricter validators\n","    # NOTE: 'x_is_regular' makes storage/comparison easier by sacrificing accuracy for smaller LTTB_CT\n","    temp_ts_vals_sm_np = lttb.downsample(temp_ts_np, n_out=LTTB_CT, validators=[has_two_columns, x_is_regular])\n","    # type(temp_ts_vals_sm_np)\n","    temp_ts_sm_indx_np = np.arange(temp_ts_vals_sm_np.shape[0])\n","    # temp_ts_sm_indx_np.shape\n","\n","    # Impute Sentence Number corresponding to Datapoints\n","    orig_len = temp_ts_vals_np.shape[0]\n","    temp_sm_len = temp_ts_vals_sm_np.shape[0]\n","    scale_mult = orig_len/temp_sm_len\n","    # print(f'scale_mult: {scale_mult}')\n","\n","    # TODO: Fix the horizontal shift early and esp late in the LTTB time series\n","    sentno_np = (scale_mult+1) * temp_ts_sm_indx_np\n","\n","    # Save\n","    # temp_lttb_dt = {\"sentno\" : list(sentno_np), \"sentiment\" : list(temp_ts_vals_sm_np)}\n","    # temp_lttb_df[amodel_rzstd] = pd.DataFrame(temp_lttb_dt)\n","    temp_lttb_df[amodel_rzstd] = list(temp_ts_vals_sm_np)\n","\n","    # Plot\n","    plt_title = f'{corpus_titles_dt[atext][0]} Standardized Sentiment\\nModel: {amodel_rzstd} Smoothed (SMA 10%)\\n{temp_ts_indx_np.shape[0]} Datapoints'\n","    _ = plt.plot(temp_ts_indx_np, temp_ts_vals_np.transpose())\n","    _ = plt.title(plt_title)\n","    # plt.plot(temp_ts_indx_np, temp_ts_vals_np.transpose())\n","    # plt.show()\n","\n","    plt_title = f'{corpus_titles_dt[atext][0]} Standardized Sentiment\\nModel: {amodel_rzstd} Smoothed (SMA 10%)\\nReduced to {LTTB_CT} Datapoints via LTTB'\n","    # plt.plot(temp_ts_sm_indx_np, temp_ts_sm_np.transpose()[1])\n","    _ = plt.plot(sentno_np, temp_ts_vals_sm_np.transpose()[1])\n","    _ = plt.title(plt_title)\n","    # plt.plot(temp_ts_indx_np, temp_ts_vals_np[1].transpose()[1])\n","    plt.show();\n","\n","  # Add median Col/Model\n","  # median_x_val = ... Ensemble has multiple irregular time series - difficult to find median\n","  # median_y_val = corpus_texts_dt[atext][models_rzstd_ls].median(axis=1)\n","  # temp_lttb_df['median_rzstd'] = zip(median_x_val, median_y_val)\n","  corpus_lttb_dt[atext] = temp_lttb_df"],"metadata":{"id":"B43II-_YrU_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp_lttb_df.columns"],"metadata":{"id":"BgufqLT_EUts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_lttb_dt.keys()"],"metadata":{"id":"MlvSQWZVw1DP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_lttb_dt[corpus_titles_ls[0]].info()"],"metadata":{"id":"W_gBiXCXw2Zg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_lttb_dt[corpus_titles_ls[0]]['vader_rzstd']"],"metadata":{"id":"ZYUWi0AwyRcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_lttb_dt[corpus_titles_ls[0]]['roberta15lg_rzstd']"],"metadata":{"id":"k4HzA5KTG4Jx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# corpus_lttb_dt[corpus_titles_ls[0]]['median_rzstd']"],"metadata":{"id":"BE35HLz2Go9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(corpus_lttb_dt[corpus_titles_ls[0]]['vader_rzstd'].to_list()[0])"],"metadata":{"id":"10wMKS6Qz0QA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify different Models have different/non-uniform x-values \n","atext = corpus_titles_ls[0]\n","amodel_rzstd1 = 'vader_rzstd'\n","amodel_rzstd2 = 'roberta15lg_rzstd'\n","# amodel_rzstd3 = 'median'\n","\n","x_vals_ls = [alist[0] for alist in corpus_lttb_dt[atext][amodel_rzstd1].to_list()]\n","y_vals_ls = [alist[1] for alist in corpus_lttb_dt[atext][amodel_rzstd1].to_list()]\n","plt.plot(x_vals_ls, y_vals_ls, label='vader_rzstd')\n","\n","x_vals_ls = [alist[0] for alist in corpus_lttb_dt[atext][amodel_rzstd2].to_list()]\n","y_vals_ls = [alist[1] for alist in corpus_lttb_dt[atext][amodel_rzstd2].to_list()]\n","plt.plot(x_vals_ls, y_vals_ls, label='roberta15lg_rzstd')\n","\n","# x_vals_ls = [alist[0] for alist in corpus_lttb_dt[atext][amodel_rzstd3].to_list()]\n","# y_vals_ls = [alist[1] for alist in corpus_lttb_dt[atext][amodel_rzstd3].to_list()]\n","# plt.plot(x_vals_ls, y_vals_ls, label='median')\n","\n","plt_title = f'{atext} Standardized Sentiment\\nModel: {amodel_rzstd2} Smoothed (SMA 10%)\\nReduced to {LTTB_CT} Datapoints via LTTB'\n","_ = plt.title(plt_title)\n","\n","plt.grid(True, alpha=0.7)\n","plt.legend()\n","plt.show();"],"metadata":{"id":"Qj9pKpU30BfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\"\"\"\n","# NOTE: 41s\n","\n","# Compare SMA with LTTB Approximation\n","\n","LTTB_CT = 100\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f'Text #{i}: {atext}')\n","\n","  win_10per = int(0.10 * corpus_texts_dt[corpus_titles_ls[0]].shape[0])\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  models_std_ls = []\n","  for j, amodel_rzstd in enumerate(models_rzstd_ls):\n","\n","    # Get Sentiment Values Col\n","    temp_ts_vals_np = corpus_texts_dt[atext][amodel_rzstd].rolling(win_10per, center=True, min_periods=0).mean().values # .reshape(-1,1)\n","    # type(temp_ts_vals_np)\n","    # temp_ts_np.shape\n","    # Generate Index Col\n","    temp_ts_indx_np = np.arange(temp_ts_vals_np.shape[0])\n","    # temp_ts_indx_np.shape\n","    # Col Stack\n","    temp_ts_np = np.column_stack((temp_ts_indx_np, temp_ts_vals_np))\n","    # temp_ts_np.shape\n","\n","    # Downsample with stricter validators\n","    temp_ts_vals_sm_np = lttb.downsample(temp_ts_np, n_out=LTTB_CT) #, validators=[has_two_columns, x_is_regular])\n","    # type(temp_ts_vals_sm_np)\n","    temp_ts_sm_indx_np = np.arange(temp_ts_vals_sm_np.shape[0])\n","    # temp_ts_sm_indx_np.shape\n","\n","    # Impute Sentence Number corresponding to Datapoints\n","    orig_len = temp_ts_vals_np.shape[0]\n","    temp_sm_len = temp_ts_vals_sm_np.shape[0]\n","    scale_mult = orig_len/temp_sm_len\n","    # print(f'scale_mult: {scale_mult}')\n","\n","    sentno_np = np.round(scale_mult * temp_ts_sm_indx_np)\n","\n","    # Save\n","    corpus_lttb_dt[amodel_rzstd] = pd.DataFrame()\n","\n","    # Plot\n","    plt_title = f'{corpus_titles_dt[atext][0]} Standardized Sentiment\\nModel: {amodel_rzstd} Smoothed (SMA 10%)\\n{temp_ts_indx_np.shape[0]} Datapoints'\n","    _ = plt.plot(temp_ts_indx_np, temp_ts_vals_np.transpose())\n","    _ = plt.title(plt_title)\n","    # plt.plot(temp_ts_indx_np, temp_ts_vals_np.transpose())\n","    # plt.show()\n","\n","    plt_title = f'{corpus_titles_dt[atext][0]} Standardized Sentiment\\nModel: {amodel_rzstd} Smoothed (SMA 10%)\\nReduced to {LTTB_CT} Datapoints via LTTB'\n","    # plt.plot(temp_ts_sm_indx_np, temp_ts_sm_np.transpose()[1])\n","    _ = plt.plot(sentno_np, temp_ts_vals_sm_np.transpose()[1])\n","    _ = plt.title(plt_title)\n","    # plt.plot(temp_ts_indx_np, temp_ts_vals_np[1].transpose()[1])\n","    plt.show();\n","\n","  corpus_lttb_dt[atext] = temp_lttb_df\n","\"\"\";"],"metadata":{"id":"uR3foWsLsF6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DTW\n","\n","Code:\n","\n","* https://towardsdatascience.com/how-to-apply-hierarchical-clustering-to-time-series-a5fe2a7d8447 (DTW) *\n","* https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf \n","* https://www.sktime.org/en/latest/examples/mrseql.html Classification Mr-SEQL\n","\n","Guidance:\n","\n","* https://stats.stackexchange.com/questions/63546/comparing-hierarchical-clustering-dendrograms-obtained-by-different-distances \n","\n","Ranks:\n","* https://paperswithcode.com/task/time-series-clustering\n","\n","Papers:\n","* https://reader.elsevier.com/reader/sd/pii/S2666827020300013?token=2286F6993FF63B6B3096B72F09503A950095DD5F1C1BB11146BDC0EAAA8E4D942BAD3A216FDBE65978BAE3B5C9F2363F&originRegion=us-east-1&originCreation=20210817184824"],"metadata":{"id":"8ZsH1S0ZSMQX"}},{"cell_type":"code","source":["from dtaidistance import dtw\n","\n","series = [\n","    np.array([0, 0, 1, 2, 1, 0, 1, 0, 0], dtype=np.double),\n","    np.array([0.0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0]),\n","    np.array([0.0, 0, 1, 2, 1, 0, 0, 0])]\n","    \n","ds = dtw.distance_matrix_fast(series)\n","ds"],"metadata":{"id":"D-Ns0D0XSOPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_lttb_dt[corpus_titles_ls[0]]['vader_rzstd']"],"metadata":{"id":"2xf-SJQxITYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_vals_ls = [data_ls[0] for data_ls in corpus_lttb_dt[corpus_titles_ls[0]]['vader_rzstd'].to_list()]\n","y_vals_ls = [data_ls[1] for data_ls in corpus_lttb_dt[corpus_titles_ls[0]]['vader_rzstd'].to_list()]\n","\n","plt.plot(x_vals_ls, y_vals_ls)"],"metadata":{"id":"B_aIyOVCBVz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.array(x_vals_ls))"],"metadata":{"id":"Bi70YRUXJbx9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for dup Cols/Models\n","\n","l = [1,2,3,2,1,5,6,5,5,5]\n","l = corpus_lttb_dt[corpus_titles_ls[0]].columns\n","\n","l_iter = groupby(sorted(l))\n","\n","for a_iter in l_iter:\n","  aname, acount = a_iter\n","  print(f'{aname}: {len(list(acount))}')\n","\n","# py3.8 res = [(x, count) for x, g in groupby(sorted(l)) if (lambda x : count = len(list(g)); count > 1]"],"metadata":{"id":"ZynRf-SPOApu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["series_ls = []\n","for acol in corpus_lttb_dt[corpus_titles_ls[0]].columns:\n","  temp_ls_ls = corpus_lttb_dt[corpus_titles_ls[0]][acol]\n","  temp_ls = [x[0] for x in temp_ls_ls]\n","  series_ls.append(np.array(corpus_lttb_dt[corpus_titles_ls[0]][acol][0]))\n","\n","len(series_ls)\n","print('\\n')\n","len(series_ls[0])"],"metadata":{"id":"-1YsZ6xr_ZUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(corpus_lttb_dt[corpus_titles_ls[0]].columns)"],"metadata":{"id":"Dflc-q6pP3Ro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_lttb_dt.keys()"],"metadata":{"id":"ah7EpL9cH5fC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create Distance Matricies \n","\n","# Global Variable\n","corpus_dtai_dt = {}\n","models_ls = []\n","\n","for i, atext in enumerate(corpus_lttb_dt.keys()):\n","  print(f'\\n\\nText #{i}: {atext}')\n","\n","  series_ls = []\n","\n","  models_rzstd_ls = corpus_lttb_dt[atext].columns\n","  for amodel_rzstd in models_rzstd_ls:\n","    models_ls.append(amodel_rzstd)\n","    # length = len(series_list[i])\n","    # series_list[i] = series_list[i].values.reshape((length, 1))\n","\n","    x_vals_ls = [data_ls[0] for data_ls in corpus_lttb_dt[corpus_titles_ls[0]][amodel_rzstd].to_list()]\n","    y_vals_ls = [data_ls[1] for data_ls in corpus_lttb_dt[corpus_titles_ls[0]][amodel_rzstd].to_list()]\n","    # print(f'len(y_vals_ls): {len(y_vals_ls)}')\n","    series_ls.append(np.array(y_vals_ls, dtype=np.double))\n","        \n","  dist_np = dtw.distance_matrix_fast(series_ls)\n","\n","  corpus_dtai_dt[atext] = dist_np\n","\n","\"\"\"\n","  # Initialize distance matrix\n","  n_series = len(series_ls)\n","  distance_matrix = np.zeros(shape=(n_series, n_series))\n","\n","  # Build distance matrix\n","  for i in range(n_series):\n","    for j in range(n_series):\n","      x = series_ls[i]\n","      y = series_ls[j]\n","      if i != j:\n","        dist = dtw_distance(x, y)\n","        distance_matrix[i, j] = dist\n","\"\"\";"],"metadata":{"id":"XBOslSzN2s8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_dtai_dt.keys()"],"metadata":{"id":"OqhK9y-gPv0k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_dtai_dt['cmieville_thecityandthecity'][0].shape"],"metadata":{"id":"3C2BfcVUPxn3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Series of \n","\n","len(series_ls)\n","print('\\n')\n","series_ls[0].shape\n","type(series_ls[0])"],"metadata":{"id":"XU0pmo5jPtSq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Agglomerative Hierarichal Clustering"],"metadata":{"id":"UleG9SMMSPCv"}},{"cell_type":"code","source":["atext = corpus_titles_ls[0]\n","\n","series = corpus_dtai_dt[atext]\n","\n","# Custom Hierarchical clustering\n","model1 = clustering.Hierarchical(dtw.distance_matrix_fast, {})\n","cluster_idx = model1.fit(series)\n","\n","# Augment Hierarchical object to keep track of the full tree\n","model2 = clustering.HierarchicalTree(model1)\n","cluster_idx = model2.fit(series)\n","\n","# SciPy linkage clustering\n","model3 = clustering.LinkageTree(dtw.distance_matrix_fast, {})\n","cluster_idx = model3.fit(series)"],"metadata":{"id":"CbiSLWOt2s3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Customize and Label Hierarchical Plot (Option #3: SciPy Linkage)\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [3, 1]}, figsize=(30, 20))\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-8,\n","           ts_left_margin=2, ts_sample_length=1)"],"metadata":{"id":"iS7oUNSx_qIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ts_label_margin\n","# ts_left_margin\n","# ts_sample_length\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1,2]}, figsize=(30, 20))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-10,\n","           ts_left_margin=10, ts_sample_length=1)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"9wrY0gf6dSh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [SKIP]"],"metadata":{"id":"i4066UgVpuDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ts_label_margin\n","# ts_left_margin\n","# ts_sample_length\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1,3]}, figsize=(30, 10))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-6,\n","           ts_left_margin=2, ts_sample_length=1)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"NtoFfv9Ydpsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ts_label_margin\n","# ts_left_margin\n","# ts_sample_length\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1,3]}, figsize=(30, 10))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-6,\n","           ts_left_margin=2, ts_sample_length=1)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"shtyAJf4bz75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ts_label_margin\n","# ts_left_margin\n","# ts_sample_length\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 2]}, figsize=(30, 20))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-2,\n","           ts_left_margin=0, ts_sample_length=4)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"dnM7U5bDaqci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# \n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 2]}, figsize=(30, 20))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-2,\n","           ts_left_margin=0, ts_sample_length=1)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"o-MrSgDjaX-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 2]}, figsize=(30, 20))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=0,\n","           ts_left_margin=0, ts_sample_length=1)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"6GGujghuaBvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 2]}, figsize=(30, 20))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=0,\n","           ts_left_margin=4, ts_sample_length=1)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"V5-J11P7TZpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 2]}, figsize=(30, 20))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-8.5,\n","           ts_left_margin=0, ts_sample_length=1)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"cI4kNXLcQkHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqH6FmhbwqPv"},"outputs":[],"source":["ts_labels = ['SentimentR',\n","             'SyuzhetR',\n","             'TextBlob',\n","             'Flair',\n","             'Stanza',\n","             'Logistic Regression',\n","             'LSTM',\n","             'CNN',\n","             'RoBERTa 15 Large',\n","             'T5']\n","\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 2]}, figsize=(30, 20))\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-8.5,\n","           ts_left_margin=4, ts_sample_length=1)\n","\n","Image(filename='hierarchy.png') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2xnjrsCwqMt"},"outputs":[],"source":["Image(filename='hierarchy.png') "]},{"cell_type":"code","source":["fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 2]}, figsize=(30, 20))\n","\n","# show_ts_label = lambda idx: \"ts-\" + str(idx)\n","# show_ts_label = lambda idx: ts_labels[idx]\n","show_ts_label = lambda idx: models_ls[idx]\n","model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n","           show_tr_label=True, ts_label_margin=-8.5,\n","           ts_left_margin=4, ts_sample_length=3)\n","\n","Image(filename='hierarchy.png') "],"metadata":{"id":"fZaDe2kFSv4v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rVVhthSk_qAk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SentimentArcs Metrics\n","\n","* (code) sentimentarcs_part7_join_norm.ipynb"],"metadata":{"id":"LcSATz9O9gEl"}},{"cell_type":"markdown","source":["### Model-Corpus Compatibility (MCC)\n","\n","MCC(corpus) = 1 / ( | amodel(corpus)- median(corpus) | / len(corpus) )\n","\n","For each Corpus, compute a Coherence Metric for all Models by:\n","* Computing the Euclidian Distance of each zScore/SMA Model from the zScore/SMA Median\n","* Sum all Euclidian Distances \n","* Identify and record furtherest outliers per Corpus/Model\n","* Sum all Euclidian Distances after removing 2-3 of ~35 outliers (5-10% discard)\n","* Normalize 2 Sums of Euclidian Distances over the entire set of Corpora\n","* Rank order the Corpora in terms of Coherence\n","* Rank Order Models in terms of Outlier frequency"],"metadata":{"id":"1fZ25puVyhVo"}},{"cell_type":"code","source":["# TODO\n","\n","# Drop all 'vader_rstd' cols/models\n","# Add 'vader_sma_rzstd' cols/models\n","# ? add 'median_' col/model?\n","# Rank Models by MCC Metric per Text\n","# Rank Familes by MFC Metric per Text\n","# Norm all Metrics\n","# Create 3x3x3 Grid of Metrics and guidelines to proceed for each situation"],"metadata":{"id":"UeE8w8798NFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if 'vader' in x]"],"metadata":{"id":"n3jPCa4_74JY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if '_rzstd' in x]"],"metadata":{"id":"1vH7C7k62qqu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[corpus_titles_ls[0]].drop(columns=['median_rzstd'], inplace=True)"],"metadata":{"id":"CIELfa6X2F3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, atext in enumerate(corpus_titles_ls):\n","  print(f'\\n\\nProcessing #{i}: {atext}')\n","  win_10per = int(0.10 * corpus_texts_dt[atext].shape[0])\n","  models_rzstd_ls = [x for x in corpus_texts_dt[atext].columns if (x.endswith('_rzstd') & ~('_smalowess_' in x))]\n","  # Add Median if missing\n","  if ('median_rzstd' in corpus_texts_dt[atext].columns):\n","    pass\n","  else:\n","    corpus_texts_dt[atext]['median_rzstd'] = corpus_texts_dt[atext][models_rzstd_ls].median()\n","    models_rzstd_ls.append('median_rzstd')\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  for j, amodel_rzstd in enumerate(models_rzstd_ls):\n","    temp_np = corpus_texts_dt[atext][amodel_rzstd].rolling(win_10per, center=True, min_periods=0).mean()\n","    if amodel_rzstd == 'median_rzstd':\n","      _ = ax.plot(temp_np, label=amodel_rzstd, color='r', linewidth=3, alpha=1)\n","    else:\n","      _ = ax.plot(temp_np, label=amodel_rzstd, alpha=0.3)\n","\n","  # Put a legend to the right of the current axis\n","  _ = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  _ = ax.grid(True, alpha=0.3)\n","  _ = ax.set_title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window=10%)\\nClipped with IQR + zScore Standardized')\n","  plt.show();"],"metadata":{"id":"Ls3hEV5FyL-m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_smalowess_rzstd_ls = [x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if x.endswith('_smalowess_rzstd')]\n","corpus_texts_dt[corpus_titles_ls[0]][models_smalowess_rzstd_ls].plot(alpha=0.3)\n","corpus_texts_dt[corpus_titles_ls[0]][models_smalowess_rzstd_ls].median(axis=1).plot(color='r', linewidth=5)"],"metadata":{"id":"e63LIO_a43VK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate MCC Metric for each corpus:model combination\n","\n","subdir_out = 'data_corpora_all'\n","\n","median_model_area_ls = []\n","mcc_ls = []                   # Model-Corpus Compatibility (MCC) \n","\n","corpora_median_area_dt = {}\n","corpora_mcc_dt = {}\n","\n","for i, atext in enumerate(corpora_ls):\n","  print(f'Processing Corpus #{i}: {acorpus}')\n","\n","  # median_model_area_ls = []\n","  mcc_ls = []\n","\n","  for j, amodel_z in enumerate(model_z_cols_ls):\n","    print(f'  with Model #{j}: {amodel_z}')\n","\n","    median_model_area = np.sum(np.abs(corpora_all_dt[acorpus][amodel_z] - corpora_all_dt[acorpus]['median_z']))\n","    print(f'    Area between Median: {median_model_area}')\n","\n","    mcc = 1 / (median_model_area / corpora_all_dt[acorpus].shape[0])\n","\n","    # median_model_area_ls.append((amodel_z, median_model_area))\n","    mcc_ls.append((amodel_z, mcc)) \n","    # print(f'      Growing list: {median_model_area_ls}')\n","    print(f'      Growing mcc_ls: {mcc_ls}')\n","\n","  # median_model_area_sorted_ls = copy.deepcopy(median_model_area_ls) # .sort(key=lambda x:x[1]) #  # .sort(key=lambda x: float(x[1])) # .sort(key=lambda y: y[1]) # .sort(key=lambda y: y[1])\n","  # median_model_area_sorted_ls.sort(key=lambda x:x[1], reverse=True)\n","  # print(f'        Copying sorted list: {median_model_area_sorted_ls}')\n","\n","  mcc_sorted_ls = copy.deepcopy(mcc_ls) # .sort(key=lambda x:x[1]) #  # .sort(key=lambda x: float(x[1])) # .sort(key=lambda y: y[1]) # .sort(key=lambda y: y[1])\n","  # median_model_area_sorted_ls.sort(key=lambda x:x[1], reverse=True)\n","  print(f'        Copying sorted list: {mcc_sorted_ls}')\n","\n","\n","  # corpora_median_area_dt[acorpus] = copy.deepcopy(median_model_area_sorted_ls)\n","\n","  corpora_mcc_dt[acorpus] = copy.deepcopy(mcc_sorted_ls)\n","\n","  # corpora_all_dt[acorpus][model_z_cols_ls].head(2)\n"],"metadata":{"id":"jou9Y-92yL6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.sum(np.abs(corpus_texts_dt[corpus_titles_ls[0]]['vader_z'] - corpus_texts_dt[corpus_titles_ls[0]]['vader_z']))"],"metadata":{"id":"EGcon7K3xz3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"gcQ96uZGxzzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FcOvZw5rAgus"},"source":["#### MCC Ranked Models for Each Corpus"]},{"cell_type":"code","metadata":{"id":"x7b51idbFTF2"},"source":["print(temp_maxmcc_df.model_z.to_list())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdPrLsoB6XTs"},"source":["\n","plt.rcParams[\"figure.figsize\"] = (10, 8)\n","\n","save_plot = False\n","\n","maxmc_dt = {}\n","\n","for acorpus in corpora_ls:\n","\n","  temp_df = corpora_mcc_df[corpora_mcc_df['corpus']==acorpus]\n","  temp_df = temp_df.sort_values('mcc', ascending=False)\n","  temp_df = temp_df[temp_df.model_z != 'median_z']\n","  # print(f'temp_df: {temp_df.shape}')\n","  # if (acorpus == 'cdickens_achristmascarol'):\n","  temp_maxmcc_df = temp_df[['model_z','mcc']].reset_index(drop=True)\n","  maxmc_dt[acorpus] = temp_maxmcc_df.copy(deep=True)\n","  # print(f'For Corpus: {acorpus}:\\n\\n  {maxmc_dt[acorpus]}')\n","  # model_ls = temp_df['model_z']\n","  # mcc_ls = temp_df['mcc']\n","  # merged_list = tuple(zip(model_ls, mcc_ls)) \n","  # print(f'merged_list: {merged_list}')\n","\n","  # temp_maxmcc_df.plot()\n","  # plt.title(f'{acorpus.upper()} MCC Ranked Models')\n","  # plt.legend('off')\n","  # plt.xticks('model_z')\n","\n","  # fig, ax = plt.subplots(1, 1)\n","  # fig = plt.figure()\n","  # ax = fig.add_subplot(111)\n","\n","  ax = temp_maxmcc_df.plot(label=acorpus, linewidth=3)\n","  # ax = corpora_all_dt[acorpus]['median_z'].rolling(win10per, center=True, min_periods=1).mean().plot(label='z-Score Median', style=['r'], linewidth=3, alpha=0.9)\n","\n","  ax.grid(True)\n","  ax.set_title(f'Model Rank by MCC: {acorpus}', fontsize=18)\n","  # ax.set(xlabel='Decade', ylabel='Weighted Percent of Top Songs', fontsize=10)\n","  # ax.set_xlabel('Line Number', fontsize=14)\n","  ax.set_ylabel('MCC Score', fontsize=14)\n","  # ax.set_xticks(df.Date.values)\n","  # xticks_ls = temp_maxmcc_df.model_z.to_list()\n","  # ax.set_xticklabels(temp_maxmcc_df.model_z.values, size=8, rotation=90)\n","  ax.set_xticklabels(temp_maxmcc_df.model_z, rotation=90) # , size=8, rotation=90)\n","  # ax.set_xticklabels(xticks_ls, size=6, rotation=90)\n","  ax.legend('off') # loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10, title='Model', title_fontsize=14);\n","\n","  if save_plot:\n","    filename_plt = f'./{subdir_name}/plt_metric_mcc_ranked_{acorpus}.png'\n","    plt.savefig(filename_plt)\n","    print(f'Saved plot to filepath: {filename_plt}\\n\\n')\n","\n","  plt.show();\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUEknMkM5tiq"},"source":["print(list(corpora_mcc_df.groupby(['corpus'])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GP7lnMUNak9r"},"source":["len(corpora_mcc_dt.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEdFQzb9auag"},"source":["len(list(corpora_mcc_dt.values())[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTfk7C2ec4Bc"},"source":["len(list(corpora_mcc_dt.items()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e80W_c7Sf5yC"},"source":["# Gather the Error/Area between Model zScore and Median zScore for all Corpora in one DataFrame\n","\n","# corpora_median_area_df = pd.DataFrame()\n","corpora_mcc_df = pd.DataFrame()\n","\n","first_loop_fl = True\n","\n","# for acorpus, area_tup_ls in corpora_median_area_dt.items():\n","for acorpus, model_mccls_tup_ls in corpora_mcc_dt.items():\n","  \n","  print(f'\\nCorpus: {acorpus}\\n') #   {area_tup_ls}')\n","  print(f'\\nlen(model_mccls_tup_ls): {len(model_mccls_tup_ls)}\\n') #   {area_tup_ls}')\n","\n","  # areas_ls = [i[1] for i in area_tup_ls]\n","  mcc_ls = [i[1] for i in model_mccls_tup_ls]\n","  models_ls = [i[0] for i in model_mccls_tup_ls]\n","\n","  # print(f'  areas_ls: {areas_ls}')\n","  print(f'  len(models_ls): {len(models_ls)}')\n","  print(f'  len(mcc_ls): {len(mcc_ls)}')\n","\n","  # temp_df = pd.DataFrame({'model_z' : models_ls,'area_z' : areas_ls})\n","  temp_df = pd.DataFrame({'model_z' : models_ls,'mcc' : mcc_ls})\n","  print(f'    temp_df.shape(): {temp_df.shape}')\n","  temp_len = temp_df.shape[0]\n","  # temp_df['area_z_norm'] = temp_df['area_z']/corpora_all_dt[acorpus].shape[0]\n","  model_col = [acorpus] * temp_len\n","  first_ser = pd.Series(model_col)\n","  temp_df = pd.concat([first_ser, temp_df], axis=1)\n","  temp_df.rename(columns={0:'corpus'}, inplace=True)\n","  print(f'     temp_df.shape() after horizontal concat(): {temp_df.shape}')\n","  \n","  temp_df.head()\n","\n","  if first_loop_fl:\n","    print(f'  Adding {acorpus} as first DataFrame')\n","    # corpora_model_area_df = temp_df.copy(deep=True)\n","    corpora_mcc_df = temp_df.copy(deep=True)\n","    first_loop_fl = False\n","  else:\n","    # temp_copy_df = temp_df.copy(deep=True)\n","    print(f'  Adding {acorpus} as successive DataFrame')\n","    # corpora_model_area_df = pd.concat([corpora_model_area_df, temp_df], axis=0)\n","    corpora_mcc_df = pd.concat([corpora_mcc_df, temp_df], axis=0)\n","\n","  # pd.DataFrame(model_col, columns=['corpus'])], axis=1, join='inner')\n","\n","  # lst = pd.Series([0.25,1.24865,2.541,3.1,4.4582]) # <-converted to series\n","  # pd.concat([pd.Series(lst), df], axis=1)\n","\n","  temp_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zx6_pU7EvJS8"},"source":["# corpora_median_area_df.head()\n","\n","corpora_mcc_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ptiPx0dwsKq"},"source":["# should be 875\n","\n","corpora_mcc_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P-xnVhvbwLQ6"},"source":["# corpora_model_area_df.tail()\n","corpora_mcc_df.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9QiJxJFZ4iW"},"source":["corpora_mcc_df[(corpora_mcc_df['corpus']=='vwoolf_tothelighthouse') & (corpora_mcc_df['model_z']=='xgb_z')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4aTa_1AJdrc"},"source":["corpora_ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqfWZQwETwfb"},"source":["model_z_cols_ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MwjqAGyhpXO"},"source":["# should be 875\n","\n","corpora_mcc_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwDLdSlBhqgp"},"source":["# Save MCC for all Models\n","\n","subdir_out = 'data_corpora_all'\n","filename_out = f'models_mcc.csv'\n","\n","fullpath_out = f'./{subdir_out}/{filename_out}'\n","\n","print(f'\\nSaving MCC in file: {fullpath_out}')\n","corpora_mcc_df.to_csv(fullpath_out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"w551z5jV6xWg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_pz2pM7jhYJ"},"source":["#### MCC Statistics"]},{"cell_type":"code","metadata":{"id":"FB-QYJQvkWFf"},"source":["corpora_mcc_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_BwQk3vkf3a"},"source":["# corpora_ls = list(corpora_mcc_df.corpus.unique())\n","corpora_ls\n","print('\\n')\n","len(corpora_ls)\n","print('\\n')\n","type(corpora_ls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BiICyCXNlFqv"},"source":["type(corpora_ls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S57Y3E03nNnD"},"source":["corpora_mcc_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QELMkR8-mS9a"},"source":["temp_df = pd.DataFrame(corpora_mcc_df.groupby('model_z'))\n","temp_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ny7KdidOn0RA"},"source":["# Standardize (MinMax) MCC Metrics\n","\n","corpora_mcc_minmax_df = corpora_mcc_df.pivot_table(index='corpus', columns='model_z', values='mcc').T\n","\n","# Replacing infinite with nan\n","corpora_mcc_minmax_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","  \n","# Dropping all the rows with nan values\n","corpora_mcc_minmax_df.dropna(inplace=True)\n","\n","corpora_mcc_minmax_df\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8V0nxHvrKcr"},"source":["model_ls = list(corpora_mcc_minmax_df.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCkXJBusmvac"},"source":["# corpora_mcc_minmax_df.groupby('model_z').min()\n","\n","# from scipy.stats import zscore\n","# corpora_mcc_std_df.apply(zscore)\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","\n","numeric_cols = corpora_mcc_minmax_df.select_dtypes(include=[np.number]).columns\n","# model_col = list(corpora_mcc_minmax_df.index)\n","\n","corpora_mcc_minmax_df = pd.DataFrame(scaler.fit_transform(corpora_mcc_minmax_df), columns=numeric_cols)\n","corpora_mcc_minmax_df.index = pd.Series(model_ls)\n","corpora_mcc_minmax_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rcyGgVCsUS8"},"source":["corpora_mcc_rank_dt = {}\n","\n","for acorpus in corpora_ls:\n","  corpora_mcc_rank_dt[acorpus] = corpora_mcc_minmax_df[acorpus].rank(ascending=False)\n","\n","corpora_mcc_rank_dt.keys()\n","\n","corpora_mcc_rank_df = pd.DataFrame(corpora_mcc_rank_dt)\n","corpora_mcc_rank_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Bm85PzstuFi"},"source":["corpora_mcc_rank_df['cdickens_achristmascarol'].sort_values()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mqIPtI7ujjz"},"source":["corpora_mcc_rank_df.T.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hscaax8C0JBj"},"source":["# https://lost-stats.github.io/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html \n","\n","\"\"\"\n","\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import matplotlib.dates as mdates\n","\n","plt.rcParams['figure.figsize'] = 20,10\n","\n","# Read in the data\n","df = pd.read_csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Figures/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv', parse_dates=['date'])\n","# df = corpora_mcc_rank_df.T\n","\n","# Create the column we wish to plot\n","title = 'Log of Google Trends Index'\n","df[title] = np.log(df['hits'])\n","\n","# Set a style for the plot\n","plt.style.use('ggplot')\n","plt.style.use('default')\n","\n","# Make a plot\n","fig, ax = plt.subplots()\n","\n","# Add lines to it\n","sns.lineplot(ax=ax, data=df, x=\"corpus\", y=title, hue=\"name\", legend=None)\n","\n","# Add the text--for each line, find the end, annotate it with a label, and\n","# adjust the chart axes so that everything fits on.\n","for line, name in zip(ax.lines, df.columns.tolist()):\n","\ty = line.get_ydata()[-1]\n","\tx = line.get_xdata()[-1]\n","\tif not np.isfinite(y):\n","\t    y=next(reversed(line.get_ydata()[~line.get_ydata().mask]),float(\"nan\"))\n","\tif not np.isfinite(y) or not np.isfinite(x):\n","\t    continue     \n","\ttext = ax.annotate(name,\n","\t\t       xy=(x, y),\n","\t\t       xytext=(0, 0),\n","\t\t       color=line.get_color(),\n","\t\t       xycoords=(ax.get_xaxis_transform(),\n","\t\t\t\t ax.get_yaxis_transform()),\n","\t\t       textcoords=\"offset points\")\n","\ttext_width = (text.get_window_extent(\n","\tfig.canvas.get_renderer()).transformed(ax.transData.inverted()).width)\n","\tif np.isfinite(text_width):\n","\t\tax.set_xlim(ax.get_xlim()[0], text.xy[0] + text_width * 1.05)\n","\n","# Format the date axis to be prettier.\n","ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%d'))\n","ax.xaxis.set_minor_locator(mdates.DayLocator())\n","ax.xaxis.set_major_locator(mdates.AutoDateLocator(interval_multiples=False))\n","plt.tight_layout()\n","plt.show()\n","\n","\"\"\";"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oeYeWl0ZuJ73"},"source":["plt.style.use('default')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLqTzbiMGuXz"},"source":["corpora_mcc_rank_df.T.head()\n","temp_df.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfbS-yvOJWdd"},"source":["temp_df['corpus'] = temp_df.index\n","temp_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"klmcaEuWHDWF"},"source":["temp_df = corpora_mcc_rank_df.T\n","temp_cols = temp_df.columns\n","# Make a plot\n","fig, ax = plt.subplots()\n","sns.lineplot(ax=ax, data=temp_df, x=temp_df.index, y=temp_df.flair_z ,palette='Accent', linewidth=1, alpha=0.9)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76lcg03-JtH3"},"source":["temp_models_ls = temp_df.columns.to_list()\n","temp_models_ls = [x for x in temp_models_ls if x.endswith('_z')]\n","temp_models_ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JN9odX7cKH8L"},"source":["temp_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYXOLcoCvB4J"},"source":["temp_df['corpus'] = temp_df.index\n","temp_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkY1WMWbI72j"},"source":["# pd.melt(df, id_vars='date', value_vars=['AA', 'BB', 'CC'])\n","\n","temp_models_ls = temp_df.columns.to_list()\n","\n","temp_tall_df = pd.melt(temp_df, id_vars='model', value_vars=temp_models_ls)\n","temp_tall_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOxsi30WLImT"},"source":["temp_df.melt(id_vars='corpus')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1P_5w7bJFqlO"},"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import matplotlib.dates as mdates\n","\n","# Read in the data\n","df = pd.read_csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Figures/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv',\n","                 parse_dates=['date'])\n","\n","# Create the column we wish to plot\n","title = 'Log of Google Trends Index'\n","df[title] = np.log(df['hits'])\n","\n","df = temp_df.melt(id_vars='corpus')\n","\n","# Set a style for the plot\n","# plt.style.use('ggplot')\n","plt.style.use('default')\n","plt.rcParams['figure.figsize'] = 10,8\n","# plt.grid(True)\n","\n","# Make a plot\n","fig, ax = plt.subplots()\n","\n","# Add lines to it\n","# sns.lineplot(ax=ax, data=df, x=\"date\", y=title, hue=\"name\", legend=None)\n","# sns.lineplot(ax=ax, data=df, x=\"date\", y=title, hue=\"name\", legend=None)\n","sns.lineplot(ax=ax, data=df, x='value', y='variable', hue='corpus', legend=None) # palette='Accent', linewidth=1, alpha=0.9)\n","\n","# Add the text--for each line, find the end, annotate it with a label, and\n","# adjust the chart axes so that everything fits on.\n","# for line, name in zip(ax.lines, df.columns.tolist()):\n","for line, name in zip(ax.lines, df.columns.tolist()):\n","\ty = line.get_ydata()[-1]\n","\tx = line.get_xdata()[-1]\n","\tif not np.isfinite(y):\n","\t    y=next(reversed(line.get_ydata()[~line.get_ydata().mask]),float(\"nan\"))\n","\tif not np.isfinite(y) or not np.isfinite(x):\n","\t    continue     \n","\ttext = ax.annotate(name,\n","\t\t       xy=(x, y),\n","\t\t       xytext=(0, 0),\n","\t\t       color=line.get_color(),\n","\t\t       xycoords=(ax.get_xaxis_transform(),\n","\t\t\t\t ax.get_yaxis_transform()),\n","\t\t       textcoords=\"offset points\")\n","\ttext_width = (text.get_window_extent(\n","\tfig.canvas.get_renderer()).transformed(ax.transData.inverted()).width)\n","\tif np.isfinite(text_width):\n","\t\tax.set_xlim(ax.get_xlim()[0], text.xy[0] + text_width * 1.05)\n","\n","# Format the date axis to be prettier.\n","# ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%d'))\n","# ax.xaxis.set_minor_locator(mdates.DayLocator())\n","# ax.xaxis.set_major_locator(mdates.AutoDateLocator(interval_multiples=False))\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaiJfStVEPnf"},"source":["\"\"\"\n","\n","seaborn palettes:\n","\n","'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', \n","'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', \n","'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', \n","'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', \n","'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', \n","'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', \n","'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', \n","'coolwarm_r', 'copper', 'copper_r', 'crest', 'crest_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'flare', 'flare_r', 'gist_earth', \n","'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', \n","'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', \n","'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'jet', 'jet_r'\n","\n","\"\"\";"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8C3Mib2AvXVg"},"source":["plt.rcParams['figure.figsize'] = 12,14\n","\n","save_plot = True\n","\n","subdir_name = 'data_corpora_plots'\n","\n","# plt.figure(facecolor='white')\n","\n","ax = sns.lineplot(data=corpora_mcc_rank_df.T, palette='Accent', linewidth=3) # , alpha=0.5)\n","ax.grid(True, alpha=0.3) # True, alpha=0.3)\n","ax.set_title('Model MCC Rank Across Corpora', fontsize=20)\n","# ax.set(xlabel='Decade', ylabel='Weighted Percent of Top Songs', fontsize=10)\n","# ax.set_xlabel('Decade', fontsize=20)\n","ax.set_ylabel('MCC Rank', fontsize=15)\n","ax.set_xticklabels(corpora_mcc_rank_df.columns, size=12, rotation=90)\n","# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n","leg = ax.legend(fontsize=10, title='Model', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc='top')\n","for line in leg.get_lines():\n","  line.set_linewidth(3.0);\n","\n","if save_plot:\n","  filename_plt = f'./{subdir_name}/plt_metric_mcc_line_rank.png'\n","  plt.savefig(filename_plt)\n","  print(f'Saved plot to filepath: {filename_plt}\\n\\n')\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbAE9gCkwrSb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AXhWg_pL-XV"},"source":["plt.style.use('default')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDwbe8GuLtDX"},"source":["plt.rcParams['figure.figsize'] = 12,8\n","\n","ax = corpora_mcc_rank_df.T.plot()\n","ax.grid(True, alpha=0.3)\n","ax.set_title('Model MCC Rank Across Corpora (Starting at Bottom)', fontsize=20)\n","# ax.set(xlabel='Decade', ylabel='Weighted Percent of Top Songs', fontsize=10)\n","# ax.set_xlabel('Decade', fontsize=20)\n","ax.set_ylabel('Rank', fontsize=15)\n","ax.set_xticklabels(corpora_mcc_rank_df.columns, size=10, rotation=90)\n","# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n","ax.legend(fontsize=10, title='Model', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc='upper left');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsS00If7ujbI"},"source":["ax = corpora_mcc_rank_df.T.plot()\n","ax.legend(loc='best')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARPg-A7d037x"},"source":["corpora_mcc_rank_df.T.describe().loc['mean']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ns6Xozls4WH_"},"source":["corpora_mcc_rank_df.T.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oflTLaWUwfp"},"source":["describe_num_df = corpora_mcc_rank_df.T.describe(include=['int64','float64'])\n","describe_num_df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsQUhgcSVfZg"},"source":["describe_num_df.rename(columns={'index':'stat'}, inplace=True)\n","describe_num_df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRuOI1uIVylX"},"source":["# describe_num_df = describe_num_df.set_index('stat')\n","# describe_num_df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJ_tV3XhV_XX"},"source":["describe_num_T_df = describe_num_df.T\n","describe_num_T_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUHqmSMmbFkA"},"source":["describe_num_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jM4Jb1ynv3_-"},"source":["# Boxplot of Model MCC Statistics\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","\n","save_plot = True\n","subdir_name = 'data_corpora_plots'\n","\n","describe_num_sorted_df = describe_num_df.T.sort_values(['mean'])\n","# describe_num_sorted_df.T.boxplot()\n","# plt.xticks(rotation=90)\n","\n","ax = describe_num_sorted_df.T.boxplot()\n","ax.grid(True, alpha=0.3)\n","ax.set_title('Sorted Model MCC Statistics', fontsize=20)\n","# ax.set(xlabel='Decade', ylabel='Weighted Percent of Top Songs', fontsize=10)\n","# ax.set_xlabel('Decade', fontsize=20)\n","ax.set_ylabel('MCC Metric', fontsize=15)\n","ax.set_xticklabels(describe_num_sorted_df.T.columns, size=10, rotation=90)\n","# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n","# ax.legend(fontsize=10, title='Model', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc='upper left');\n","\n","if save_plot:\n","  filename_plt = f'./{subdir_name}/plt_metric_mcc_box_stats.png'\n","  plt.savefig(filename_plt)\n","  print(f'Saved plot to filepath: {filename_plt}\\n\\n')\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T74Tut2BWfRH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lqc5zyWnWfLe"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SJG_Zk-aZWF"},"source":["\n","save_plot = False\n","\n","\n","describe_num_sorted_df = describe_num_df.T.sort_values(['mean'])\n","# describe_num_sorted_df.T.boxplot()\n","# plt.xticks(rotation=90)\n","\n","ax = describe_num_sorted_df.T.boxplot()\n","plt.xticks(rotation=90)\n","\n","ax = corpora_mcc_rank_df.T.plot()\n","ax.grid(True, alpha=0.3)\n","ax.set_title('Model MCC Rank Across Corpora (Starting at Bottom)', fontsize=20)\n","# ax.set(xlabel='Decade', ylabel='Weighted Percent of Top Songs', fontsize=10)\n","# ax.set_xlabel('Decade', fontsize=20)\n","ax.set_ylabel('Rank', fontsize=15)\n","ax.set_xticklabels(corpora_mcc_rank_df.columns, size=10, rotation=90)\n","# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n","ax.legend(fontsize=10, title='Model', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc='upper left');\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5AKBWQrWkxH"},"source":["describe_num_df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZAokrFGY-9E"},"source":["corpora_mcc_rank_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfBLYeYsUdWK"},"source":["describe_num_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPP91gm3UEER"},"source":["describe_num_T_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLG3OlQpVB04"},"source":["describe_num_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZViTtejIUsmq"},"source":["describe_num_df.T.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9a2qDQVVUmi"},"source":["describe_num_df = describe_num_df.sort_values('mean')\n","describe_num_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0UlvA3nDVch7"},"source":["describe_num_T_df = describe_num_df.describe(include=['int64','float64'])\n","describe_num_T_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8hJEZMIgX-nY"},"source":["describe_num_df = describe_num_df.sort_values('mean')\n","\n","describe_num_T_df = describe_num_df.describe(include=['int64','float64'])\n","describe_num_T_df.head(2)\n","\n","# describe_num_df = describe_num_df.sort_values(['mean','std'])\n","# describe_num_T_df.sort_values('mean')\n","describe_num_T_df.T.boxplot()\n","plt.xticks(rotation=90)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JHYmR-RUBLT"},"source":["# https://medium.com/analytics-vidhya/how-to-visualize-pandas-descriptive-statistics-functions-480c3f2ea87c\n","\n","describe_num_df = corpora_mcc_rank_df.T.describe(include=['int64','float64'])\n","\n","# describe_num_df.reset_index(inplace=True)\n","\n","# Make a plot\n","# fig, ax = plt.subplots(5,5)\n","# fig, axes = plt.subplots(5,5, figsize=(15,10))\n","\n","# Add lines to it\n","# sns.lineplot(ax=ax, data=df, x=\"corpus\", y=title, hue=\"name\", legend=None)\n","\n","\n","sns.boxplot(describe_num_df)\n","\n","\"\"\"\n","# To remove any variable from plot\n","# describe_num_df = describe_num_df[describe_num_df['index'] != 'count']\n","num_col = describe_num_df.columns\n","for i,acol in enumerate(num_col):\n","  if acol in ['index','count']:\n","    continue  \n","  ax_row = i // 5\n","  ax_col = i % 5\n","  axes[ax_row, ax_col] = sns.factorplot(x='index', y=acol, data=describe_num_df.reset_index())  \n","  plt.ylabel('Numeric Value')\n","  plt.xlabel('Statistic')\n","  plt.title(f'MCC Descriptive Statistics\\n{acol}')\n","  # sns.regplot(data = df_comments.reset_index(), x = 'index', y = 'score')\n","  plt.show();\n","\"\"\";"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxFMBgYZ3Ag9"},"source":["# https://medium.com/analytics-vidhya/how-to-visualize-pandas-descriptive-statistics-functions-480c3f2ea87c\n","\n","describe_num_df = corpora_mcc_rank_df.T.describe(include=['int64','float64'])\n","\n","# describe_num_df.reset_index(inplace=True)\n","\n","# Make a plot\n","# fig, ax = plt.subplots(5,5)\n","fig, axes = plt.subplots(5,5, figsize=(15,10))\n","\n","# Add lines to it\n","# sns.lineplot(ax=ax, data=df, x=\"corpus\", y=title, hue=\"name\", legend=None)\n","\n","\n","\n","# To remove any variable from plot\n","# describe_num_df = describe_num_df[describe_num_df['index'] != 'count']\n","num_col = describe_num_df.columns\n","for i,acol in enumerate(num_col):\n","  if acol in ['index','count']:\n","    continue  \n","  ax_row = i // 5\n","  ax_col = i % 5\n","  axes[ax_row, ax_col] = sns.factorplot(x='index', y=acol, data=describe_num_df.reset_index())  \n","  plt.ylabel('Numeric Value')\n","  plt.xlabel('Statistic')\n","  plt.title(f'MCC Descriptive Statistics\\n{acol}')\n","  # sns.regplot(data = df_comments.reset_index(), x = 'index', y = 'score')\n","  plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeRTSTZn2BnM"},"source":["corrmat = corpora_mcc_rank_df.T.corr()\n","# print(corrmat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxjqcX4AYlAk"},"source":["plt.rcParams['figure.figsize'] = (20,20)\n","\n","ax = sns.heatmap(corrmat, vmax=1, annot=True, linewidths=.5)\n","\n","# ax.grid(True, alpha=0.3)\n","ax.set_title('Model MCC Correlation Matrix', fontsize=20)\n","# ax.set(xlabel='Decade', ylabel='Weighted Percent of Top Songs', fontsize=10)\n","# ax.set_xlabel('Decade', fontsize=20)\n","# ax.set_ylabel('Rank', fontsize=15)\n","# ax.set_xticklabels(corpora_mcc_rank_df.columns, size=10, rotation=40)\n","# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n","# ax.legend(fontsize=10, title='Model', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc='upper left');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLyEkPK51mp8"},"source":["plt.figure(figsize=(30, 30))\n","\n","sns.heatmap(corrmat, vmax=1, annot=True, linewidths=.5)\n","plt.xticks(rotation=30) # , horizontalalignment=right)\n","plt.title()\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"710rxl5FkYw9"},"source":["temp_df = pd.DataFrame()\n","\n","for acorpus in corpora_ls:\n","  temp_df = corpora_mcc_df.groupby('model_z')\n","  temp_df = temp_df[temp_df['model_z'] == ]\n","  print(f'{acorpus}: \\n\\ntemp_df: {temp_df.head()}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyyis-VCjo7S"},"source":["corpora_mcc_df.groupby('model_z').mean().sort_values('mcc')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmSMkGcFjovx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JhnimzsJwkw7"},"source":["#### MCC Ranking of Models for each Corpus"]},{"cell_type":"code","metadata":{"id":"48vjZnvSfWjw"},"source":["corpora_mcc_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhuKX1oervGw"},"source":["# Plot Rank of Model Area from Median per Corpus\n","\n","save_plot = True\n","\n","plt.rcParams[\"figure.figsize\"] = (8, 8)\n","\n","subdir_name = 'data_corpora_plots'\n","\n","models_all_order_dt = {}\n","for acorpus in corpora_ls:\n","  if acorpus != 'median_z':\n","    models_all_order_dt[acorpus] = []\n","  # for amodel_z in model_z_cols_ls:\n","  #   models_all_order_dt[acorpus][amodel_z] = []\n","\n","temp_df = pd.DataFrame()\n","\n","for i, acorpus in enumerate(corpora_ls):\n","  print(f'Plotting Model-Median Area for Corpus #{i}: {acorpus}')\n","  # temp_df = corpora_model_area_df[corpora_model_area_df.corpus == acorpus]\n","  temp_df = corpora_mcc_df[corpora_mcc_df.corpus == acorpus]\n","\n","  # Drop the 'median_z' row\n","  temp_df = temp_df[temp_df.model_z != 'median_z']\n","\n","  # Sort in place \n","  temp_df.sort_values(by=['mcc'], inplace=True, ascending=False)\n","  # temp_df.head(2) # area_z.plot(kind='bar')\n","  # temp_df.area_z.plot(kind='bar' x=model_z, y=area_z)\n","\n","  # Store order for each Model\n","  models_order_ls = temp_df.model_z.to_list()\n","  # for j, amodel_ord in enumerate(models_order_ls):\n","  #   models_all_order_dt[acorpus][amodel_ord].append(str(j)) \n","  # models_order_ls.reverse()  # Reverses in-place\n","  models_order_ls.sort()\n","  models_all_order_dt[acorpus] = models_order_ls\n","\n","  #ax = temp_df.plot.bar(x='model_z', y='area_z', rot=90)\n","\n","  plt.barh('model_z', 'mcc', data=temp_df)\n","  # plt.xticks(fontsize=20) # , rotation=0)\n","  # plt.yticks(fontsize=20) # , rotation=0)\n","  plt.rcParams.update({'font.size': 8})\n","  plt.title(f'{corpora_full_dt[acorpus]}\\n Model-Corpus Compatibility (MCC) Metric', pad=20, fontdict={'fontsize':10})\n","\n","  if save_plot:\n","    subdir_name = 'data_corpora_plots'\n","    filename_plt = f'./{subdir_name}/plt_mcc_rank_{acorpus}.png'\n","    plt.savefig(filename_plt)\n","\n","  plt.show()\n","  plt.close();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ct6MDByLUmO4"},"source":["print(models_all_order_dt['cdickens_achristmascarol'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zLLhp2j96sOG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7U4ywQ9Iwroc"},"source":["#### Plot Rank and Spread by Model over the Corpora"]},{"cell_type":"code","metadata":{"id":"TWFSxYrrXbgE"},"source":["# Create a Dict (Models) of Lists (Ranks)\n","\n","models_all_rank_dt = {}\n","\n","models_z_rank_ls = models_all_order_dt['cdickens_achristmascarol']\n","for amodel_z in models_z_rank_ls:\n","  models_all_rank_dt[amodel_z] = []\n","\n","for key, values in models_all_order_dt.items():\n","  print(f'Corpus: {key}')\n","  for i, amodel in enumerate(values):\n","    print(f' Model #{i}: {amodel}')\n","    models_all_rank_dt[amodel].append(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTGW9o9SYmBN"},"source":["# models_all_rank_dt.keys()\n","\n","models_z_rank_ls = models_all_order_dt['cdickens_achristmascarol']\n","models_mean_ls = []\n","models_std_ls = []\n","models_max_ls = []\n","models_min_ls = []\n","models_ranks_ls_ls = []\n","\n","for i, amodel_z in enumerate(models_z_rank_ls):\n","  print(f'For Model: {amodel_z}:')\n","  model_mean = np.mean(models_all_rank_dt[amodel_z])\n","  print(f'  Mean: {model_mean}')\n","  models_mean_ls.append(model_mean)\n","\n","  model_std = np.std(models_all_rank_dt[amodel_z])\n","  print(f'  STD: {model_std}')\n","  models_std_ls.append(model_std)\n","\n","  model_min = np.min(models_all_rank_dt[amodel_z])\n","  print(f'  Min: {model_min}')\n","  models_min_ls.append(model_min)\n","\n","  model_max = np.max(models_all_rank_dt[amodel_z])\n","  print(f'  Max: {model_max}')\n","  models_max_ls.append(model_max)\n","\n","  model_ranks_ls = models_all_rank_dt[amodel_z]\n","  print(f' Ranks: {model_ranks_ls}')\n","  models_ranks_ls_ls.append(model_ranks_ls)\n","\n","\n","models_rank_dt = {'model_z': models_z_rank_ls,\n","                  'mean':models_mean_ls,\n","                  'std':models_std_ls,\n","                  'min':models_min_ls,\n","                  'max':models_max_ls,\n","                  'ranks':models_ranks_ls_ls}\n","\n","models_rank_df = pd.DataFrame.from_dict(models_rank_dt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgxCt6Jud1lv"},"source":["models_rank_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tjISpq2GlD7c"},"source":["models_labels_ls = ['-'.join(i.split('_')[:-1]) for i in models_rank_df.model_z.to_list()]\n","models_labels_ls\n","len(models_labels_ls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVZ_nDWumFxv"},"source":["len(models_labels_ls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVezKhsPiEym"},"source":["# models_labels_ls = models_rank_df.model_z.to_list()\n","# models_labels_ls = ['-'.join(i.split('_')[:-1]) for i in models_labels_ls]\n","# models_labels_ls = [w.replace('_', '-') for w in models_labels_ls]\n","# models_labels_ls = [i.split('_')[:-1] for i in models_rank_df.model_z.to_list()]\n","\n","# models_box_ready_df = pd.DataFrame(np.array(models_rank_df.ranks.to_list()).T, columns=models_labels_ls)\n","models_box_ready_df = pd.DataFrame(np.array(models_rank_df.ranks.to_list()).T, columns=models_labels_ls)\n","models_box_ready_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"irsjV5afiEq7"},"source":["# Plot Rank and Spread of Models across all 25 Corpora\n","\n","plt.rcParams[\"figure.figsize\"] = (8, 8)\n","\n","save_plot = True\n","subdir_name = 'data_corpora_plots'\n","\n","ax = models_box_ready_df.plot(kind='box')\n","ax.grid(True, alpha=0.3)\n","\n","ax.set_ylabel('Rank', fontsize=12)\n","ax.set_xticklabels(corpora_mcc_rank_df.columns, size=10, rotation=40)\n","# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n","\n","# ax.invert_yaxis()\n","\n","plt.xticks(fontsize=20, rotation=90)\n","# plt.yticks(fontsize=20) # , rotation=0)\n","# plt.rcParams.update({'font.size': 20})\n","# plt.title(f'{corpora_full_dt[acorpus]}\\n Error Area between zScore Models and Median', pad=20, fontdict={'fontsize':24})\n","plt.grid(alpha=0.3)\n","plt.title('Rank and Spread of Models across all 25 Corpora')\n","\n","ax = corpora_mcc_rank_df.T.plot()\n","\n","ax.set_title('Model MCC Rank Across Corpora (Starting at Bottom)', fontsize=20)\n","# ax.set(xlabel='Decade', ylabel='Weighted Percent of Top Songs', fontsize=10)\n","# ax.set_xlabel('Decade', fontsize=20)\n","ax.set_ylabel('Rank', fontsize=15)\n","ax.set_xticklabels(corpora_mcc_rank_df.columns, size=10, rotation=40)\n","# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n","ax.legend(fontsize=10, title='Model', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc='upper left');\n","\n","\n","if save_plot:\n","  filename_plt = f'./{subdir_name}/plt_models_all_rank_spread.png'\n","  plt.savefig(filename_plt)\n","\n","plt.show()\n","plt.close();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFjeXSt2ho6d"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVWavYihaJSJ"},"source":["\n","models_z_rank_ls = models_all_order_dt['cdickens_achristmascarol']\n","\n","for amodel_z in models_z_rank_ls:\n","  models_all_rank_dt[amodel_z] = []\n","\n","for key, values in models_all_order_dt.items():\n","  print(f'Corpus: {key}')\n","  for i, amodel in enumerate(values):\n","    print(f' Model #{i}: {amodel}')\n","    # models_all_rank_dt[amodel].append(i)\n","\n","  models_all_rank_dt['logreg_cv_z']\n","  print('\\n')\n","\n","  model_median = np.median(models_all_rank_dt['logreg_cv_z'])\n","  print(f'  Model Median: {model_median}')\n","\n","  model_std = np.std(models_all_rank_dt['logreg_cv_z'])\n","  print(f'  Model STD: {model_std}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tn-XJ-XkaJOi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfm-9gljaJI7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eS9ExZhsEKTI"},"source":["# Create DataFrame from Dictionaries\n","\n","corpora_median_area_dt = {}\n","temp_df = pd.DataFrame()\n","first_loop_fl = True\n","\n","for acorpus, area_tup_ls in corpora_median_area_dt.items():\n","  \n","  print(f'\\nCorpus: {acorpus}\\n   {area_tup_ls}')\n","\n","  areas_ls = [i[1] for i in area_tup_ls]\n","  model_ls = [i[0] for i in area_tup_ls]\n","  temp_df = pd.DataFrame(areas_ls, columns=['area_z'] )\n","  print(f'temp_df.head():\\n{temp_df.head()}')\n","  if first_loop_fl:\n","    corpora_median_area_dt[acorpus] = temp_df.copy(deep=True)\n","    first_loop_fl = False\n","  else:\n","    corpora_median_area_dt = pd.merge(corpora_median_area_dt, temp_df, how='inner', on = 'model_z')\n","\n","# corpora_median_area_df = pd.DataFrame(corpora_median_area_dt, columns=['model_z', 'area_z'])\n","\n","\n","\"\"\"\n","for acorpus, area_tup_ls in corpora_median_area_dt.items():\n","  \n","  print(f'Corpus: {acorpus}\\n   {area_tup_ls}')\n","\n","for i, acorpus in enumerate(corpora_ls):\n","  print(f'Processing Corpus #{i}: {acorpus}')\n","\n","  median_model_area_ls = []\n","\n","  for j, amodel_z in enumerate(model_z_cols_ls):\n","    print(f'  with Model #{j}: {amodel_z}')\n","\n","    median_model_area = np.sum(np.abs(corpora_all_dt[acorpus][amodel_z] - corpora_all_dt[acorpus]['median_z']))\n","    print(f'    Area between Median: {median_model_area}')\n","\n","    median_model_area_ls.append((amodel_z, median_model_area))\n","\n","\n","corpora_median_area_dt['cdickens_achristmascarol'].plot(kind=bar)\n","\"\"\";"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5XsT-ogeIr9"},"source":["corpora_median_area_dt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pVZYLfaJy06"},"source":["corpora_median_area_dt.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_uAmi1kJs33"},"source":["corpora_median_area_dt['cdickens_achristmascarol']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sN_ajoJ4G1pW"},"source":["corpora_median_area_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2p6Oa6rvAPRw"},"source":["print(corpora_median_area_dt['cdickens_achristmascarol'][0][1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZrRbAanC09h"},"source":["print(corpora_median_area_dt['cdickens_achristmascarol'].sort(key=lambda x:x[0][1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZaUNdEZ-TZb"},"source":["print(corpora_median_area_dt['cdickens_achristmascarol']) #.sort(key=lambda y: y[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0yb3qxjN8vra"},"source":["mx = max(corpora_median_area_dt['cdickens_achristmascarol'], key=lambda e: int(e[1]))\n","print(mx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"shMPbC_L8vjC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ensemble-Corpus Compatibility (ECC)"],"metadata":{"id":"fvT0J59A7Cil"}},{"cell_type":"code","metadata":{"id":"dcbHQ4IQ7VRB"},"source":["corpora_mcc_minmax_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UlQJi9yd7VLm"},"source":["plt.rcParams[\"figure.figsize\"] = (6, 6)\n","\n","subdir_name = 'data_corpora_plots'\n","\n","corpora_mcc_minmax_df.sum().sort_values().plot(kind='bar')\n","plt.title(f'Ensemble-Corpus Compatility (ECC) Metric by Novel')\n","\n","plt.ylabel('ECC Value')\n","# plt.xticks(fontsize=20) # , rotation=0)\n","# plt.yticks(fontsize=20) # , rotation=0)\n","plt.rcParams.update({'font.size': 8})\n","plt.title(f'{corpora_full_dt[acorpus]}\\n Model-Corpus Compatibility (MCC) Metric', pad=20, fontdict={'fontsize':10})\n","\n","if save_plot:\n","\n","  filename_plt = f'./{subdir_name}/plt_ecc_corpora.png'\n","  plt.savefig(filename_plt)\n","\n","plt.show()\n","plt.close();"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [SKIP]"],"metadata":{"id":"Q2ALGv_bz9RX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"eYfUmuhzz9MF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"r3jxD0Lrz9JM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[atext].iloc[:2][smalowess_ls].sum(axis=1)"],"metadata":{"id":"zX8Y9Z4Vq3xQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[atext][smalowess_ls].std().values"],"metadata":{"id":"h4pKLkPLpnYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["minmax_ser = pd.Series(corpus_texts_dt[atext][smalowess_ls].max(axis=1) - corpus_texts_dt[atext][smalowess_ls].min(axis=1))"],"metadata":{"id":"ILduWrHXr5aI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["atext = corpus_texts_ls[0]\n","\n","win_10per = int(0.10 * corpus_texts_dt[atext].shape[0])\n","\n","smalowess_ls = [x for x in corpus_texts_dt[corpus_texts_ls[0]].columns if x.endswith('_smalowess_rzstd')]\n","smalowess_ls\n","\n","sma_ls = [x.replace('_smalowess_', '_') for x in smalowess_ls]\n","\n","fig = plt.figure()\n","ax = plt.subplot(111)\n","\n","std_ser = pd.Series(corpus_texts_dt[atext][smalowess_ls].std(axis=1).values)\n","# .rolling(win_10per, center=True, min_periods=0).mean()\n","# _ = ax.plot(models_smalowess_rzstd_ls_median, label='Ensemble Median', color='r', linewidth=3) # .rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","_ = ax.plot(std_ser, label='Std', alpha=0.3, linewidth=3)\n","_ = ax.plot(minmax_ser, label='MinMax Range', alpha=0.3)\n","# _ = ax.plot(corpus_texts_dt[atext][sma_ls[:model_ct]].rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', alpha=0.3, linewidth=3)\n","\n","# Put a legend to the right of the current axis\n","ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","plt.grid(True, alpha=0.3)\n","plt.title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {model_ct} Models\\nSmoothed: SMA only (vs) SMA (window=10%) + LOWESS (frac=1./{afrac_inv}\\nClipped with IQR + zScore Standardized')\n","plt.show();  "],"metadata":{"id":"92HyMDJjn1Tt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Family Coherence (MFC)"],"metadata":{"id":"nZKlxQat7JKR"}},{"cell_type":"code","metadata":{"id":"PEyLT2pL8B62"},"source":["corpora_mcc_minmax_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ua6naKuz9G1O"},"source":["corpora_mcc_minmax_df.index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2bxVRW39M-Q"},"source":["model_family_map_dt = {'afinn_z':'lexicon',\n","                   'autogluon_z':'dnn', \n","                   'bing_z':'lexicon', \n","                   'cnn_z':'dnn', \n","                   'fcn_z':'dnn', \n","                   'flair_z':'dnn',\n","                   'flaml_z':'dnn', \n","                   'hinglish_z':'transformer', \n","                   'huggingface_z':'transformer', \n","                   'huliu_z':'lexicon', \n","                   'imdb2way_z':'transformer',\n","                   'jockers_rinker_z':'heuristic', \n","                   'jockers_z':'lexicon', \n","                   'lmcd_z':'heuristic', \n","                   'logreg_cv_z':'ml', \n","                   'logreg_z':'ml',\n","                   'lstm_z':'heuristic', \n","                   'multinb_z':'ml', \n","                   'nlptown_z':'transformer', \n","                   'nrc_z':'heuristic', \n","                   'rf_z':'ml', \n","                   'roberta15lg_z':'transformer',\n","                   'robertaxml8lang_z':'transformer', \n","                   'senticnet_z':'heuristic', \n","                   'sentimentr_z':'lexicon', \n","                   'sentiword_z':'heuristic',\n","                   'stanza_z':'dnn', \n","                   'syuzhet_z':'lexicon', \n","                   't5imdb50k_z':'transformer', \n","                   'textblob_z':'ml', \n","                   'vader_z':'heuristic',\n","                   'xgb_z':'ml', \n","                   'yelp_z':'transformer'\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lQ1nn3LAunS"},"source":["# corpora_mfc_df.drop(columns=['model_z'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5KGxu9X-8c9"},"source":["corpora_mfc_df = pd.DataFrame(corpora_mcc_minmax_df)\n","\n","# corpora_mfc_df.insert(loc=0, column='model_z', value=corpora_mfc_df.index)\n","\n","corpora_mfc_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8RStKfXCmDA"},"source":["# del corpora_mfc_df\n","corpora_mfc_df['model'] = corpora_mfc_df.index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVcrDGITzKtd"},"source":["corpora_mfc_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SF4zZbHZzssj"},"source":["corpora_mfc_df.iloc[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTJjK1MIzPek"},"source":["\n","for index, arow in corpora_mfc_df.iterrows():\n","  print(arow['model'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xM9J7gnZ-8L6"},"source":["\n","zfamily_ls = []\n","\n","for index, arow in corpora_mfc_df.iterrows():\n","  zmodel = arow['model']\n","  zfamily = model_family_map_dt[zmodel]\n","  print(f'Model: {zmodel} belongs to Family: {zfamily}')\n","  zfamily_ls.append(zfamily)\n","\n","print(f'zfamily_ls: {zfamily_ls}')\n","\n","corpora_mfc_df.insert(loc=0, column='family', value=zfamily_ls)\n","corpora_mfc_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yq5jve388-Is"},"source":["mfc_family_df = corpora_mfc_df.groupby('family').describe()\n","mfc_family_df.shape\n","# mfc_family_df.plot() # ['mean'].plot(kind='bar')\n","print('\\n')\n","mfc_family_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-q8Ka4qu8-Ef"},"source":["plt.rcParams[\"figure.figsize\"] = (8, 8)\n","\n","save_plot = True\n","\n","subdir_name = 'data_corpora_plots'\n","\n","family_types_ls = ['lexicon', 'heuristic', 'dnn', 'ml', 'transformer']\n","\n","family_means_ls = []\n","family_mins_ls = []\n","family_maxs_ls = []\n","family_stds_ls = []\n","\n","for afamily in family_types_ls:\n","  family_means_ls.append(mfc_family_df.T[afamily].mean())\n","  family_mins_ls.append(mfc_family_df.T[afamily].min())\n","  family_maxs_ls.append(mfc_family_df.T[afamily].max())\n","  family_stds_ls.append(mfc_family_df.T[afamily].std())\n","\n","\n","# family_means_df = pd.DataFrame()\n","family_stats_df = pd.DataFrame({'zfamily':family_types_ls, \n","                                'zmean':family_means_ls,\n","                                'zmin':family_mins_ls,\n","                                'zmax':family_maxs_ls,\n","                                'zstd':family_stds_ls,\n","                                }) #  = pd.DataFrame(family_means_dt)\n","family_stats_df.index = family_stats_df.zfamily\n","\n","family_stats_df.head()\n","family_stats_df.info()\n","# family_means_df['min'].sort_values().plot(kind='bar')\n","\n","# Plot MFC \n","\n","# plt.rcParams['axes.grid'] = True\n","# plt.rcParams['grid.alpha'] = 1\n","# plt.rcParams['grid.color'] = \"#cccccc\"\n","\n","# fig, ax = plt.subplots()\n","\n","family_stats_df.sort_values(by='zmean').plot(kind='bar')\n","plt.grid(True, alpha=0.3)\n","plt.legend(loc='best')\n","# plt.xlabel('Model Family', size=20)\n","plt.ylabel('MFC Value', size=10)\n","# plt.axis('off')\n","# plt.xlabel.set_visible(False)\n","plt.xticks(size=10, rotation=10)\n","# plt.title('Model Family Coherence (MFC) Over Entire Corpus', size=16)\n","\n","# plt.rcParams.update({'font.size': 8})\n","plt.title(f'{corpora_full_dt[acorpus]}\\n Model-Corpus Compatibility (MCC) Metric', pad=20, fontdict={'fontsize':16})\n","\n","if save_plot:\n","\n","  filename_plt = f'./{subdir_name}/plt_mfc_corpora.png'\n","  plt.savefig(filename_plt)\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q7s_OD2J_QHA"},"source":["## Save Checkpoint"]},{"cell_type":"code","source":["print('Current Working Directory:')\n","!pwd\n","\n","print(f'\\nSaving to subdir=SUBDIR_SENTIMENT_CLEAN:\\n  {SUBDIR_SENTIMENT_CLEAN}\\n')\n","\n","print(f'Saving to filename=FNAME_SENTIMENT_CLEAN:\\n  {FNAME_SENTIMENT_CLEAN}\\n')"],"metadata":{"id":"yWXtK-taKaRo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt.keys()"],"metadata":{"id":"5ueypR0YKbXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Review Models per Text\n","\n","# Unique Model per each Text\n","print('Unique Models for each Text:')\n","[x for x in corpus_texts_dt[corpus_texts_ls[0]].columns if 'vader' in x]\n","print('\\n\\n')\n","\n","# Check for duplicate Cols/Models\n","col_dups_ct = corpus_texts_dt[corpus_texts_ls[0]].columns.duplicated().sum()\n","print(f'[{col_dups_ct}] Duplicated Cols/Models')"],"metadata":{"id":"CCYDuCraq4cM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for duplicate Texts\n","print('Check for duplicated Texts in Corpus:\\n')\n","corpus_texts_dt.keys()\n","\n","# Delete if necessary\n","# corpus_texts_dt['pattern_smalowess_rzlstd'].head()\n","# del corpus_texts_dt['pattern_smalowess_rzstd']"],"metadata":{"id":"K7JwlQfWrRrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# timeit_res = %timeit -n1 -r1 -o sum(range(1000000))\n","\n","# NOTE:\n","\n","# for i, atext in enumerate(corpus_texts_dt.keys()):\n","\n","temp_df = pd.DataFrame()\n","\n","for i, atext in enumerate(corpus_titles_ls):\n","  print(f'Text #{i}: {atext}')\n","\n","  col_models_ls = []\n","  for j, amodel in enumerate(ensemble_ls):\n","  \n","    col_models_ls = [x for x in corpus_texts_dt[corpus_texts_ls[0]].columns if (x.startswith(f'{amodel}_') or x == amodel)]\n","    \n","    print(f'\\n\\natext: {atext}\\namodel: {amodel}\\ncol_models_ls: {col_models_ls}\\n\\n')\n","\n","    temp_df = corpus_texts_dt[atext][col_models_ls]\n","    temp_df.info()\n","    print(f'type(temp_df): {type(temp_df)}')\n","    temp_fname = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_{amodel}_{atext}.csv'\n","    print(f'filename: {temp_fname}')\n","    temp_df.to_csv(f'{SUBDIR_SENTIMENT_CLEAN}{temp_fname}')\n","\n","\n","\"\"\"\n","    # amodel_rstd = f'{amodel}_rstd'\n","    amodel_rzstd = f'{amodel}_rzstd'\n","    amodel_sma_rzstd = f'{amodel}_sma_rzstd'\n","    amodel_smalowess_rzstd = f'{amodel}_smalowess_rzstd'\n","    print(f'  Model #{j}: {amodel} (Model_Std: {amodel_rzstd})')\n","    win_10per = int(0.10 * corpus_texts_dt[atext][amodel].shape[0])\n","    # clip_outliers(corpus_texts_dt[corpus_texts_ls[0]]['vader'])\n","\"\"\";"],"metadata":{"id":"2WlchOwTqIbe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","# Save sentiment values to subdir_sentiments\n","write_dict_dfs(corpus_texts_dt, out_file=FNAME_SENTIMENT_CLEAN, out_dir=SUBDIR_SENTIMENT_CLEAN)\n","print(f'Saving Corpus_Genre: {Corpus_Genre}')\n","print(f'        Corpus_Type: {Corpus_Type}')\n","print('\\n')\n","\n","# Verify Dictionary was saved correctly by reading back the *.json datafile\n","test_dt = read_dict_dfs(in_file=FNAME_SENTIMENT_CLEAN, in_dir=SUBDIR_SENTIMENT_CLEAN)\n","print(f'These Text Titles:')\n","test_dt.keys()\n","print('\\n')\n","\n","corpus_texts_dt[corpus_titles_ls[0]].head()\n","print('\\n')\n","\n","test_dt[corpus_titles_ls[0]].info()\n"],"metadata":{"id":"T0PYdG8PpLe4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [SKIP]"],"metadata":{"id":"GPvoLsWg-JVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"__uv-y57xTWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"X44WDHF3xTSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# ax.plot(x, y_rstd, alpha=0.1) # , label=f'LOWESS (frac={afrac_inv})') # , color='tomato', linewidth=5)\n","\n","ax.plot(corpus_texts_dt[atext][amodel_rstd].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rstd, alpha=0.3)\n","ax.plot(corpus_texts_dt[atext][amodel].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel, alpha=0.3)\n","\n","# Plot statsmodels LOWESS\n","# ax.plot(sm_x, sm_y, label=f'LOWESS (frac={afrac_inv})') # , color='tomato', linewidth=5)\n","# Plot moepy LOWESS\n","ax.plot(x, y_rstd_pred, label=f'Std LOWESS (frac=1./{afrac_inv})') # , color='tomato', linewidth=5)\n","ax.plot(x, y_sma_rstd_pred, label=f'Std+SMA LOWESS (frac=1./{afrac_inv})') # , color='tomato', linewidth=5)\n","\n","atitle = f'{corpus_titles_dt[atext][0]}\\nSentimentArcs Model: {amodel}\\nLOWESS Smoothed (frac={afrac_inv})\\nClipped (2.5*IQR) and Standardized (zScore)'\n","plt.title(atitle)\n","plt.legend()\n","plt.show();"],"metadata":{"id":"F6FKfVKS7PxM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","sm_x, sm_y = sm_lowess(y, x,  frac=1./160., it=5, return_sorted = True).T\n","plt.plot(sm_x, sm_y, color='tomato', linewidth=5)\n","# sm_x, sm_y = sm_lowess(y, x,  frac=1./20., it=5, return_sorted = True).T\n","# plt.plot(sm_x, sm_y, color='green', linewidth=5, alpha=0.3)\n","# plt.plot(x,)\n","plt.plot(x, y, 'k.', alpha=0.1)\n","plt.plot(x, y_roll10, label='SMA 10%')\n","plt.plot(x, df.sentiment_roll10_interp, label='interpolate roll10')\n","plt.plot(sm_x, sm_y, label='interpolate LOWESS')\n","plt.plot(x, df.sentiment_roll10_ewm, label='ewm')\n","plt.legend(loc='best')"],"metadata":{"id":"zX6YQK64yssO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lowess_grid_dt = {}\n","crux_ct_ls = []\n","# temp_df['sent_no'] = pd.Series([x for x in corpus_sents_df['sent_no']])\n","temp_df['avg_stdscaler'] = corpus_sents_df[models_subset_ls].mean()\n","\n","fig = plt.figure()\n","ax = plt.axes()\n","\n","\n","for afrac in range(frac_start_int, frac_end_int, frac_step_int):\n","  print(f'Processing afrac = {afrac}')\n","  # Compute error between subset of models\n","  afrac_fl = afrac/100\n","  temp_df = get_lowess(corpus_sents_df, models_ls=models_subset_ls, text_unit='sentence', afrac=afrac_fl, do_plot=False);\n","  temp_df['minmax_diff'] = temp_df.max(axis=1) - temp_df.min(axis=1)\n","  diff_sum = temp_df['minmax_diff'].sum()\n","  print(f\"  Sum(minmax_diff): {diff_sum}\");\n","  lowess_grid_dt[afrac] = diff_sum\n","  # Compute Crux Points\n","  temp_df['sent_no'] = pd.Series(list(range(temp_df.shape[0])))\n","  crux_ls = get_crux_points(temp_df,\n","                            'median',\n","                            text_type='sentence', \n","                            win_per=5, \n","                            sec_y_labels=False, \n","                            sec_y_height=0, \n","                            subtitle_str=' ', \n","                            do_plot=False,\n","                            save2file=False)\n","  ax.plot(temp_df['sent_no'], temp_df['median'], label=f'frac={afrac}')\n","  # plt.plot(data=temp_df, x='sent_no', y='median', label=f'frac={afrac}')\n","  crux_ct_ls.append(len(crux_ls))\n","  print(f'  {len(crux_ls)} Crux Points')\n","\n","plt.title(f\"{CORPUS_FULL} \\n LOWESS Smoothing Grid Search (frac={Frac_Start} to {Frac_End}\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"JxNT08AYx3tn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[atext][amodel_std].shape[0]"],"metadata":{"id":"HZlk6w3mzdlC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# np.array(corpus_texts_dt[atext][amodel_std]).reshape(-1,1).shape\n","\n","np.array(corpus_texts_dt[atext][amodel_std]).reshape(-1,).shape\n","\n","np.array(corpus_texts_dt[atext][amodel_std]).flatten().shape"],"metadata":{"id":"d45DmgDLzUu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yoZPPF2e7azQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lowess_model.fit(x, y_roll10, frac=0.2)\n","\n","# x_pred = x # np.linspace(0, 6448, 100)\n","y_pred = lowess_model.predict(x)\n","\n","# Plotting\n","plt.plot(x, y_roll10, label='Sin Wave', zorder=2)\n","plt.plot(x, y_pred, '--', label='Estimate', color='k', zorder=3)\n","# plt.scatter(x, y_roll10, label='With Noise', color='C1', s=5, zorder=1)\n","plt.legend(frameon=False)\n","# plt.xlim(0, 5)"],"metadata":{"id":"b-lRehlL75P9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"wyccWVN_koJ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# NOTE:  1m21s @14:30 on 20220308 Colab Pro/CPU (1 Novel/1model using moepy)\n","#        1m21s @14:37 on 20220308 Colab Pro/CPU (1 Novel/1model using moepy)\n","#        1m21s @14:37 on 20220308 Colab Pro/CPU (1 Novel/1model using statsmodels)\n","#        1m21s @14:37 on 20220308 Colab Pro/CPU (1 Novel/1model using statsmodels)\n","\n","atext = 'cmieville_thecityandthecity'\n","amodel = 'afinn'\n","amodel_rstd = 'afinn_rstd'\n","afrac_inv = 10\n","\n","win_10per = int(0.10 * corpus_texts_dt[atext][amodel].shape[0])\n","\n","x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","y = corpus_texts_dt[atext][amodel].to_numpy()\n","y_std = corpus_texts_dt[atext][amodel_std].to_numpy()\n","# print(f'x.shape: {x.shape}\\ny.shape: {y.shape}')\n","# sm_x, sm_y = sm_lowess(y, x,  frac=1./afrac_inv, it=5, return_sorted = True).T\n","\n","# statsmodels\n","#sm_x, sm_y = sm_lowess(y, x,  frac=1./afrac_inv, it=5).T\n","\n","# moepy\n","lowess_moepy.fit(x, y_std, frac=1./afrac_inv)\n","y_pred = lowess_moepy.predict(x)\n","\n","atitle = f'{corpus_titles_dt[atext][0]}\\nSentimentArcs Model: {amodel}\\nLOWESS Smoothed (frac=1./{afrac_inv})\\nClipped (2.5*IQR) and Standardized (zScore)'\n","\n","fig = plt.figure()\n","ax = plt.subplot(111)\n","\n","# ax.plot(x, y_std, alpha=0.1) # , label=f'LOWESS (frac={afrac_inv})') # , color='tomato', linewidth=5)\n","\n","ax.plot(corpus_texts_dt[atext][amodel_rstd].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rstd, alpha=0.3)\n","ax.plot(corpus_texts_dt[atext][amodel].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel, alpha=0.3)\n","\n","# Plot statsmodels LOWESS\n","# ax.plot(sm_x, sm_y, label=f'LOWESS (frac={afrac_inv})') # , color='tomato', linewidth=5)\n","# Plot moepy LOWESS\n","ax.plot(x, y_pred, label=f'LOWESS (frac=1./{afrac_inv})') # , color='tomato', linewidth=5)\n","\n","atitle = f'{corpus_titles_dt[atext][0]}\\nSentimentArcs Model: {amodel}\\nLOWESS Smoothed (frac={afrac_inv})\\nClipped (2.5*IQR) and Standardized (zScore)'\n","plt.title(atitle)\n","plt.legend()\n","plt.show();"],"metadata":{"id":"cBTp1bWJDW02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# NOTE:  1m21s @14:30 on 20220308 Colab Pro/CPU (1 Novel/1model using moepy)\n","#        1m21s @14:37 on 20220308 Colab Pro/CPU (1 Novel/1model using moepy)\n","#        1m21s @14:37 on 20220308 Colab Pro/CPU (1 Novel/1model using statsmodels)\n","#        1m21s @14:37 on 20220308 Colab Pro/CPU (1 Novel/1model using statsmodels)\n","\n","atext = 'cmieville_thecityandthecity'\n","amodel = 'afinn'\n","amodel_rstd = 'afinn_rstd'\n","afrac_inv = 10\n","\n","win_10per = int(0.10 * corpus_texts_dt[atext][amodel].shape[0])\n","\n","x = np.array(range(corpus_texts_dt[atext][amodel].shape[0]))\n","\n","y = corpus_texts_dt[atext][amodel].to_numpy()\n","y_std = corpus_texts_dt[atext][amodel_std].to_numpy()\n","\n","# print(f'x.shape: {x.shape}\\ny.shape: {y.shape}')\n","# sm_x, sm_y = sm_lowess(y, x,  frac=1./afrac_inv, it=5, return_sorted = True).T\n","\n","# statsmodels\n","#sm_x, sm_y = sm_lowess(y, x,  frac=1./afrac_inv, it=5).T\n","\n","# moepy\n","lowess_moepy.fit(x, y_std, frac=1./afrac_inv)\n","y_pred = lowess_moepy.predict(x)\n","\n","atitle = f'{corpus_titles_dt[atext][0]}\\nSentimentArcs Model: {amodel}\\nLOWESS Smoothed (frac=1./{afrac_inv})\\nClipped (2.5*IQR) and Standardized (zScore)'\n","\n","fig = plt.figure()\n","ax = plt.subplot(111)\n","\n","# ax.plot(x, y_std, alpha=0.1) # , label=f'LOWESS (frac={afrac_inv})') # , color='tomato', linewidth=5)\n","\n","ax.plot(corpus_texts_dt[atext][amodel_rstd].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rstd, alpha=0.3)\n","ax.plot(corpus_texts_dt[atext][amodel].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel, alpha=0.3)\n","\n","# Plot statsmodels LOWESS\n","# ax.plot(sm_x, sm_y, label=f'LOWESS (frac={afrac_inv})') # , color='tomato', linewidth=5)\n","# Plot moepy LOWESS\n","ax.plot(x, y_pred, label=f'LOWESS (frac=1./{afrac_inv})') # , color='tomato', linewidth=5)\n","\n","atitle = f'{corpus_titles_dt[atext][0]}\\nSentimentArcs Model: {amodel}\\nLOWESS Smoothed (frac={afrac_inv})\\nClipped (2.5*IQR) and Standardized (zScore)'\n","plt.title(atitle)\n","plt.legend()\n","plt.show();"],"metadata":{"id":"CDdnYVqi3meA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UHBYXcT-1yxf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d5_WhpYp_3Fi"},"source":["\n","# **[STEP 5] Customize Ensemble & Smoothing**\n","\n","* https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb\n"]},{"cell_type":"markdown","source":["## Select Ensemble Subset"],"metadata":{"id":"vVMbOvBKyAEB"}},{"cell_type":"code","source":["ensemble_ls.sort()\n","ensemble_ls"],"metadata":{"id":"5VRKDJ8y2UD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sentence Plotly Interactive/Zoom Sentiment Plots\n","\n","ensemble_subset_ls = []\n","\n","#@markdown **Instructions:**\n","#@markdown <li> Select Models for Ensemble Subset\n","#@markdown <li> Then execute this code cell\n","\n","#@markdown <hr>\n","\n","#@markdown **Lexicon Models**\n","# Lexicon Models from SyuzhetR\n","SyuzhetR_AFINN = False #@param {type:\"boolean\"}\n","SyuzhetR_Bing = True #@param {type:\"boolean\"}\n","SyuzhetR_NRC = False #@param {type:\"boolean\"}\n","SyuzhetR_Syuzhet = True #@param {type:\"boolean\"}\n","\n","if SyuzhetR_AFINN == True:\n","  ensemble_subset_ls.append('syuzhetr_afinn')\n","if SyuzhetR_Bing == True:\n","  ensemble_subset_ls.append('syuzhetr_bing')\n","if SyuzhetR_NRC == True:\n","  ensemble_subset_ls.append('syuzhetr_nrc')\n","if SyuzhetR_Syuzhet == True:\n","  ensemble_subset_ls.append('syuzhetr_syuzhet')\n","\n","# Lexicon Models that are Standalone\n","Pattern = True #@param {type:\"boolean\"}\n","AFINN = True #@param {type:\"boolean\"}\n","\n","if Pattern == True:\n","  ensemble_subset_ls.append('pattern')\n","if AFINN == True:\n","  ensemble_subset_ls.append('afinn')\n","\n","# Lexicons ported from SentimentR to Python\n","pysentimentr_JockerRinker = True #@param {type:\"boolean\"}\n","pysentimentr_HuLiu = True #@param {type:\"boolean\"}\n","pysentimentr_NRC = True #@param {type:\"boolean\"}\n","pysentimentr_SentiWord = True #@param {type:\"boolean\"}\n","pysentimentr_SenticNet = True #@param {type:\"boolean\"}\n","pysentimentr_LMcD = True #@param {type:\"boolean\"}\n","\n","if pysentimentr_JockerRinker == True:\n","  ensemble_subset_ls.append('pysentimentr_jockersrinker')\n","if pysentimentr_HuLiu == True:\n","  ensemble_subset_ls.append('pysentimentr_huliu')\n","if pysentimentr_NRC == True:\n","  ensemble_subset_ls.append('pysentimentr_nrc')\n","if pysentimentr_SentiWord == True:\n","  ensemble_subset_ls.append('pysentimentr_sentiword')\n","if pysentimentr_SenticNet == True:\n","  ensemble_subset_ls.append('pysentimentr_senticnet')\n","if pysentimentr_LMcD == True:\n","  ensemble_subset_ls.append('pysentimentr_lmcd')\n","  \n","# Future -----\n","# https://www.liwc.app/ (LIWC)\n","# LIWC = False #@param {type:\"boolean\"}\n","# https://github.com/nickderobertis/pysentiment (HarvardIV, LMcD)\n","# HarvardIV = False #@param {type:\"boolean\"}\n","# https://mpqa.cs.pitt.edu/ (MPQA)\n","# MPQA_Arc = False #@param {type:\"boolean\"}\n","# http://sentistrength.wlv.ac.uk/ (SentiStrength)\n","# SentiStrength_Arc = False #@param {type:\"boolean\"}\n","\n","Median_Lexicon = True #@param {type:\"boolean\"}\n","\n","if Median_Lexicon == True:\n","  ensemble_subset_ls.append('median_lexicon')\n","\n","\n","#@markdown **Heuristic Models**\n","SentimentR_JockersRinker = True #@param {type:\"boolean\"}\n","SentimentR_Jockers = True #@param {type:\"boolean\"}\n","SentimentR_HuLiu = True #@param {type:\"boolean\"}\n","SentimentR_SenticNet = True #@param {type:\"boolean\"}\n","SentimentR_SentiWord = True #@param {type:\"boolean\"}\n","SentimentR_NRC = True #@param {type:\"boolean\"}\n","SentimentR_LoughranMcDonald = True #@param {type:\"boolean\"}\n","SentimentR_SoCal_Google = True #@param {type:\"boolean\"}\n","VADER = True #@param {type:\"boolean\"}\n","\n","\n","if SentimentR_JockersRinker == True:\n","  ensemble_subset_ls.append('sentimentr_jockersrinker')\n","if SentimentR_Jockers == True:\n","  ensemble_subset_ls.append('sentimentr_jockers')\n","if SentimentR_HuLiu == True:\n","  ensemble_subset_ls.append('sentimentr_huliu')\n","if SentimentR_SenticNet == True:\n","  ensemble_subset_ls.append('sentimentr_senticnet')\n","if SentimentR_SentiWord == True:\n","  ensemble_subset_ls.append('sentimentr_sentiword')\n","if SentimentR_NRC == True:\n","  ensemble_subset_ls.append('sentimentr_nrc')\n","if SentimentR_LoughranMcDonald == True:\n","  ensemble_subset_ls.append('sentimentr_loughran_mcdonald')\n","if SentimentR_SoCal_Google == True:\n","  ensemble_subset_ls.append('sentimentr_socal_google')\n","if VADER == True:\n","  ensemble_subset_ls.append('vader')\n","\n","Median_Heuristic = True #@param {type:\"boolean\"}\n","\n","if Median_Heuristic == True:\n","  ensemble_subset_ls.append('median_heuristic')\n","\n","#@markdown **Statistical ML Models**\n","Logistic_Regression = False #@param {type:\"boolean\"}\n","Logistic_Regression_cv6 = False #@param {type:\"boolean\"}\n","Multinomial_NaiveBayes = False #@param {type:\"boolean\"}\n","Multinomial_NaiveBayes_POS = False #@param {type:\"boolean\"}\n","Random_Forest = False #@param {type:\"boolean\"}\n","XGBoost = False #@param {type:\"boolean\"}\n","AutoML_FLAML = False #@param {type:\"boolean\"}\n","AutoML_AutoGluon = False #@param {type:\"boolean\"}\n","\n","if Logistic_Regression == True:\n","  ensemble_subset_ls.append('logreg')\n","if Logistic_Regression_cv6 == True:\n","  ensemble_subset_ls.append('logreg_cv6')\n","if Multinomial_NaiveBayes == True:\n","  ensemble_subset_ls.append('multinb')\n","if Multinomial_NaiveBayes_POS == True:\n","  ensemble_subset_ls.append('multinb')\n","if Random_Forest == True:\n","  ensemble_subset_ls.append('rf')\n","if XGBoost == True:\n","  ensemble_subset_ls.append('xgboost')\n","if AutoML_FLAML == True:\n","  ensemble_subset_ls.append('flaml')\n","if AutoML_AutoGluon == True:\n","  ensemble_subset_ls.append('autogluon')\n","\n","Median_ML = False #@param {type:\"boolean\"}\n","\n","if Median_ML == True:\n","  ensemble_subset_ls.append('median_ml')\n","\n","#@markdown **DNN Models**\n","FCN = False #@param {type:\"boolean\"}\n","LSTM = False #@param {type:\"boolean\"}\n","CNN = False #@param {type:\"boolean\"}\n","AutoML_Stanza = True #@param {type:\"boolean\"}\n","AutoML_Flair = True #@param {type:\"boolean\"}\n","\n","if FCN == True:\n","  ensemble_subset_ls.append('fcn')\n","if LSTM == True:\n","  ensemble_subset_ls.append('lstm')\n","if CNN == True:\n","  ensemble_subset_ls.append('cnn')\n","if AutoML_Stanza == True:\n","  ensemble_subset_ls.append('stanza')\n","if AutoML_Flair == True:\n","  ensemble_subset_ls.append('flair')\n","\n","Median_DNN = False #@param {type:\"boolean\"}\n","\n","if Median_DNN == True:\n","  ensemble_subset_ls.append('median_dnn')\n","\n","\n","#@markdown **Transformer Models**\n","RoBERTaLg15 = True #@param {type:\"boolean\"}\n","Huggingface = True #@param {type:\"boolean\"}\n","NLPTown = True #@param {type:\"boolean\"}\n","T5IMDB50k = True #@param {type:\"boolean\"}\n","IMDB2way = True #@param {type:\"boolean\"}\n","Yelp = True #@param {type:\"boolean\"}\n","RoBERTaXML8lang = True #@param {type:\"boolean\"}\n","Hinglish = True #@param {type:\"boolean\"}\n","\n","if RoBERTaLg15 == True:\n","  ensemble_subset_ls.append('roberta15lg')\n","if Huggingface == True:\n","  ensemble_subset_ls.append('huggingface')\n","if NLPTown == True:\n","  ensemble_subset_ls.append('nlptown')\n","if T5IMDB50k == True:\n","  ensemble_subset_ls.append('t5imdb50k')\n","if IMDB2way == True:\n","  ensemble_subset_ls.append('imdb2way')\n","if Yelp == True:\n","  ensemble_subset_ls.append('yelp')\n","if RoBERTaXML8lang == True:\n","  ensemble_subset_ls.append('robertaxml8lang')\n","if Hinglish == True:\n","  ensemble_subset_ls.append('hinglish')\n","\n","Median_Transformer = True #@param {type:\"boolean\"}\n","\n","if Median_Transformer == True:\n","  ensemble_subset_ls.append('median_transformer')\n","\n","#@markdown <hr>\n","\n","Median_Ensemble = True #@param {type:\"boolean\"}\n","\n","if Median_Ensemble == True:\n","  ensemble_subset_ls.append('median_ensemble')\n","\n","ensemble_subset_ls\n","\n","print(f'\\n\\n[{len(ensemble_subset_ls)}] Models Selected Above\\n')\n"],"metadata":{"id":"Pu_JEqQyvGd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Smoothing"],"metadata":{"id":"pQudQNt01OFW"}},{"cell_type":"code","source":["#@markdown **Smoothing Technique**\n","Smoothing_Algo = \"SMA\" #@param [\"SMA\", \"LOWESS\", \"Both\"]\n","\n","#@markdown <hr>\n","\n","#@markdown **For SMA Smoothing (default 10%)**\n","Window_Percent = 7 #@param {type:\"slider\", min:3, max:20, step:1}\n","\n","#@markdown <hr>\n","\n","#@markdown **For LOWESS Smoothing (default 0.08)**\n","Frac_Start = 0.08 #@param {type:\"slider\", min:0.01, max:0.3, step:0.01}\n","Frac_End = 0.2 #@param {type:\"slider\", min:0.01, max:0.2, step:0.01}\n","Frac_Step = 0.02 #@param {type:\"slider\", min:0.01, max:0.05, step:0.01}\n","\n","frac_start_int = int(100*Frac_Start)\n","frac_end_int = int(100*Frac_End) + 1\n","frac_step_int = int(100*Frac_Step)"],"metadata":{"id":"nZqaaQFg1OFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ensemble_ls"],"metadata":{"id":"Scsiofnq1OFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_rzstd = [x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if (x.endswith('_rzstd') & ('_smalowess_' not in x))]\n","models_rzstd"],"metadata":{"id":"LnjSxATDqrhm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ensemble_subset_ls = [x for x in models_rzstd if x[0] in ['s','r']]\n","ensemble_subset_ls"],"metadata":{"id":"zOmE4686qPBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[corpus_titles_ls[0]][ensemble_subset_ls].rolling(300, center=True, min_periods=0).mean().plot(alpha=0.3)"],"metadata":{"id":"RwADGjhhqi61"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Select Smoothing and Hyperparameters"],"metadata":{"id":"PFAzY_ytyG8L"}},{"cell_type":"markdown","source":["### Option (a): Simple Moving Average (default window = 10%)"],"metadata":{"id":"NB-vMfT-zDQN"}},{"cell_type":"code","source":["\n","SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}"],"metadata":{"id":"K_j0YA6KzDGX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Option (b): LOWESS (default frac=.08)"],"metadata":{"id":"aAAFK4hUzInm"}},{"cell_type":"code","source":["Frac_Start = 0.08 #@param {type:\"slider\", min:0.01, max:0.3, step:0.01}\n","Frac_End = 0.2 #@param {type:\"slider\", min:0.01, max:0.2, step:0.01}\n","Frac_Step = 0.02 #@param {type:\"slider\", min:0.01, max:0.05, step:0.01}\n","\n","frac_start_int = int(100*Frac_Start)\n","frac_end_int = int(100*Frac_End) + 1\n","frac_step_int = int(100*Frac_Step)\n","\n","print('GRID SEARCH --------------------\\n')\n","\n","lowess_grid_dt = {}\n","crux_ct_ls = []\n","# temp_df['sent_no'] = pd.Series([x for x in corpus_sents_df['sent_no']])\n","temp_df['avg_stdscaler'] = corpus_sents_df[models_subset_ls].mean()\n","\n","fig = plt.figure()\n","ax = plt.axes()\n","\n","\n","for afrac in range(frac_start_int, frac_end_int, frac_step_int):\n","  print(f'Processing afrac = {afrac}')\n","  # Compute error between subset of models\n","  afrac_fl = afrac/100\n","  temp_df = get_lowess(corpus_sents_df, models_ls=models_subset_ls, text_unit='sentence', afrac=afrac_fl, do_plot=False);\n","  temp_df['minmax_diff'] = temp_df.max(axis=1) - temp_df.min(axis=1)\n","  diff_sum = temp_df['minmax_diff'].sum()\n","  print(f\"  Sum(minmax_diff): {diff_sum}\");\n","  lowess_grid_dt[afrac] = diff_sum\n","  # Compute Crux Points\n","  temp_df['sent_no'] = pd.Series(list(range(temp_df.shape[0])))\n","  crux_ls = get_crux_points(temp_df,\n","                            'median',\n","                            text_type='sentence', \n","                            win_per=5, \n","                            sec_y_labels=False, \n","                            sec_y_height=0, \n","                            subtitle_str=' ', \n","                            do_plot=False,\n","                            save2file=False)\n","  ax.plot(temp_df['sent_no'], temp_df['median'], label=f'frac={afrac}')\n","  # plt.plot(data=temp_df, x='sent_no', y='median', label=f'frac={afrac}')\n","  crux_ct_ls.append(len(crux_ls))\n","  print(f'  {len(crux_ls)} Crux Points')\n","\n","plt.title(f\"{CORPUS_FULL} \\n LOWESS Smoothing Grid Search (frac={Frac_Start} to {Frac_End}\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"D7-ytFdhzDB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot Declining Error as a function of LOWESS frac\n","\n","# lowess_grid_dt\n","\n","lists = sorted(lowess_grid_dt.items()) # sorted by key, return a list of tuples\n","\n","x, y = zip(*lists) # unpack a list of pairs into two tuples\n","# plt.plot(x, y, label='Interplot Error')\n","\n","adj_factor = 40\n","crux_ct_adj_ls = [adj_factor * x for x in crux_ct_ls]\n","\n","# create figure and axis objects with subplots()\n","fig,ax = plt.subplots()\n","# make first plot: Error\n","ax.plot(x, y, color=\"red\", label='Coherence Error', marker=\"o\")\n","# set x-axis label\n","ax.set_xlabel(\"LOWESS frac Hyperparemeter\",fontsize=14)\n","# set y-axis label\n","ax.set_ylabel(\"Coherence Error\",color=\"red\",fontsize=14)\n","\n","# twin object for two different y-axis on the sample plot\n","ax2=ax.twinx()\n","\n","# make second plot: Crux Count, with different y-axis using second axis object\n","ax2.plot(x, crux_ct_ls,color=\"blue\",label='Crux Count', marker=\"o\")\n","ax2.set_ylabel(\"Crux Count\",color=\"blue\",fontsize=14)\n","plt.title(f'{CORPUS_FULL} Sentence Sentiment \\n Grid Search for LOWESS [frac] Hyperparemeter')\n","plt.legend(loc='best')\n","plt.show();\n","\"\"\"\n","# save the plot as a file\n","fig.savefig('two_different_y_axis_for_single_python_plot_with_twinx.jpg',\n","            format='jpeg',\n","            dpi=100,\n","            bbox_inches='tight')\n","\"\"\";"],"metadata":{"id":"m7OHCxQS6aYa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Agglomerative Hierarichal Clustering"],"metadata":{"id":"AccUVL-u6ksU"}},{"cell_type":"code","source":[""],"metadata":{"id":"jROx-W64SKLC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 6] Crux Detection and Extraction**"],"metadata":{"id":"Sh5ZSAV4x6NL"}},{"cell_type":"markdown","metadata":{"id":"x8QX8tSKM376"},"source":["## **Search Corpus for Substring**\n","\n","INSTRUCTIONS:\n","\n","* In [Search_for_Substring] enter a Substring to search for in the Corpus\n","\n","* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n","\n","* Substring can contain spaces/punctuation, for example: 'in the garden'"]},{"cell_type":"code","metadata":{"id":"hJYjOu9Ks_pL","cellView":"form"},"source":["# Search Corpus Sentences for Substring\n","\n","Search_for_Substring = \"love\" #@param {type:\"string\"}\n","\n","sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n","\n","for i, asentno in enumerate(sentno_matching_ls):\n","  # sentno, sentraw = asent\n","  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n","  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n","  print(f'    {sent_highlight}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ap_K_gpH0FTm"},"source":["## **Plot Top-n Crux Peaks/Valleys for selected Model**\n","\n","INSTRUCTIONS:\n","\n","* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n","\n","* [Sentiment_Model] Select a Sentiment Analysis model\n","\n","* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n","\n","* Select [Save_to_File] to also save plot to external *.png file"]},{"cell_type":"code","metadata":{"id":"OHPx5a0xvJYb"},"source":["Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n","Baseline_SMA_Model = \"SentiWord\" #@param [\"SentimentR\", \"SyuzhetR\", \"Bing\", \"SenticNet\", \"SentiWord\", \"NRC\", \"AFINN\", \"VADER\", \"TextBlob\", \"Flair\", \"Pattern\", \"Stanza\"]\n","Anomaly_Detection = False #@param {type:\"boolean\"}\n","Vertical_Labels = True #@param {type:\"boolean\"}\n","Vertical_Labels_Height = -0.1 #@param {type:\"slider\", min:-50, max:50, step:0.1}\n","Save_to_Report = False #@param {type:\"boolean\"}\n","\n","if Baseline_SMA_Model == 'SentimentR':\n","  model_selected = f'sentimentr'\n","if Baseline_SMA_Model == 'SyuzhetR':\n","  model_selected = f'syuzhet'\n","if Baseline_SMA_Model == 'Bing':\n","  model_selected = f'bing'\n","if Baseline_SMA_Model == 'SenticNet':\n","  model_selected = f'senticnet'\n","if Baseline_SMA_Model == 'SentiWord':\n","  model_selected = f'sentiword'\n","if Baseline_SMA_Model == 'NRC':\n","  model_selected = f'nrc'\n","if Baseline_SMA_Model == 'AFINN':\n","  model_selected = f'afinn'\n","if Baseline_SMA_Model == 'VADER':\n","  model_selected = f'vader'\n","if Baseline_SMA_Model == 'TextBlob':\n","  model_selected = f'textblob'\n","if Baseline_SMA_Model == 'Flair':\n","  model_selected = f'flair'\n","if Baseline_SMA_Model == 'Pattern':\n","  model_selected = f'pattern'\n","if Baseline_SMA_Model == 'Stanza':\n","  model_selected = f'stanza'\n","\n","if Anomaly_Detection == False:\n","  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n","  model_selected_fullname = f'{model_selected}_stdscaler_{roll_str}'\n","else:\n","  # (b)Use Sentence Raw Sentiment models to detect outliers\n","  model_selected_fullname = f'{model_selected}'\n","\n","\n","# TODO: enable multiple overlay crux points with underlying mean/median arc\n","corpus_models_selected_ls = [model_selected_fullname]\n","\n","# Warning: requires definitions of: x, section_sents_df\n","#          so Baseline models must be run first\n","\n","for amodel in corpus_models_selected_ls:\n","  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_df, \n","                                         col_series=corpus_models_selected_ls, \n","                                         text_type='sentence', \n","                                         win_per=Crux_Window_Percent, \n","                                         sec_y_labels=Vertical_Labels,\n","                                         sec_y_height=Vertical_Labels_Height, \n","                                         subtitle_str= '5% Crux ', \n","                                         do_plot=True, \n","                                         save2file=False);\n","  \n","model_crux_ls = corpus_cruxes_all_dt[amodel]\n","# model_crux_ls;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6PU1zR8vJYf"},"source":["## **Context around Top-n Crux Peaks/Valleys**\n","\n","INSTRUCTIONS:\n","\n","* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n","\n","* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n","\n","* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n","\n","* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n","\n","* Select [Save_to_File] to also save output to external *.txt file"]},{"cell_type":"code","metadata":{"id":"FUoJz_nyvJYh"},"source":["# Crux Point Details\n","Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n","Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n","Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n","\n","# Context Details\n","No_Paragraphs_on_Each_Side = 5 #@param {type:\"slider\", min:0, max:5, step:1}\n","Highlight_Sentence = True #@param {type:\"boolean\"}\n","Save_to_Report = False #@param {type:\"boolean\"}\n","\n","\n","if Sort_by_SentenceNo == True:\n","  sort_on = 'sent_no'\n","else:\n","  sort_on = 'sentiment_val'\n","\n","\n","print(f'Crux Report --------------------\\n')\n","print(f'            Corpus: {CORPUS_FULL}')\n","print(f'            Model: {Baseline_SMA_Model}')\n","print(f'            Crux Win%: {Crux_Window_Percent}')\n","print(f'            SMA Win%: {roll_str}')\n","\n","if Save_to_Report == False:\n","  crux_sortsents_report(model_crux_ls, \n","                        ts_df = corpus_sents_df,\n","                        library_type='baseline', \n","                        top_n=Get_n_Cruxes, \n","                        get_peaks=Get_Peak_Cruxes,\n","                        sort_by = sort_on, # sent_no, or abs(polarity)\n","                        n_sideparags=No_Paragraphs_on_Each_Side,\n","                        sentence_highlight=Highlight_Sentence)\n","else:\n","  # import sys\n","  # with open('filename.txt', 'w') as f:\n","  #   print('This message will be written to a file.', file=f)\n","  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n","  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n","  # stdout_as_str = process.stdout.decode(\"utf-8\")\n","  # print(stdout_as_str)\n","  temp_out = StringIO()\n","  sys.stdout = temp_out\n","  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n","  print(temp_out)\n","  # attempt to save temp_out to generated filename\n","  sys.stdout = sys.__stdout__\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Trmsuob-0GUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJlFM5kFJDdA"},"source":["asent_no = 124\n","corpus_df = corpus_sents_df\n","asent_raw = str(corpus_df[corpus_df['sent_no'] == int(asent_no)]['sent_raw'].values[0])\n","asent_raw"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S-XtE7xovJYj"},"source":["## **Zoom in on Context surrounding a particular Crux Point**\n","\n","INSTRUCTIONS:\n","\n","* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n","\n","* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n","\n","* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n","\n","* Select [Save_to_File] to also save output to external *.txt file"]},{"cell_type":"code","metadata":{"id":"E1By1LTGvJYk"},"source":["# Select details about the Crux Point Context to Retrieve\n","\n","# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n","Crux_Sentence_No =  200#@param {type:\"number\"}\n","No_Paragraphs_on_Each_Side = 4 #@param {type:\"slider\", min:0, max:10, step:1}\n","Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n","Save_to_Report = False #@param {type:\"boolean\"}\n","\n","corpus_sents_len = corpus_sents_df.shape[0]\n","\n","# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n","# get_sentnocontext_report()\n","# try:\n","\n","# get_sentnocontext_report(ts_df = corpus_sents_df, the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n","get_sentnocontext_report(ts_df=corpus_sents_df, the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n","\n","# except:\n","#   print('ERROR')\n","# else:\n","#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_j92DSgyhGc"},"source":["# **END OF NOTEBOOK**"]}],"metadata":{"colab":{"collapsed_sections":["w8IjOF7EHNzJ","_R11BEJ_47yA","QJO7kLz-47yF","o5GqEXyRkPjj","If55aLQYsk_K","umZfB0YCqajW","EA1yTaY_9Qod","7dPPrZwyIIze","CenqyAnJ7NLA","YgQcCuvNetie","d_55gz93LAB3","Q7s_OD2J_QHA"],"name":"sentiment_arcs_part6_analysis.ipynb","provenance":[],"private_outputs":true,"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}