{"cells":[{"cell_type":"markdown","metadata":{"id":"ibHFmIWoU3Vx"},"source":["# **Compute Sentiments Using Python Lexical, ML, DNN and Transformers**\n","\n","By: Jon Chun\n","12 Jun 2021 : Start\n","20 Apr 2022 : Last Edit"]},{"cell_type":"markdown","metadata":{"id":"43oGeYK19Pyq"},"source":["# **[RESTART RUNTIME] May be Required**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FO20GCk2rOh"},"outputs":[],"source":["# [CHECK] (If disconnected) @ometimes you can reconnect to your old machine\n","#         use %whos to see if your old program enviroment still exists. \n","#         If 'Interactive namesapce is empty' you must reexecute everything\n","\n","%whos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fat5hgY9yHTq"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required\n","\n","# !pip install flair"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UF6optLIJT2i"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required\n","\n","# !pip install texthero"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mT-77aUWNOT_"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required\n","\n","# Designed Security Hole in older PyYAML\n","#   must upgrade to use plotly\n","\n","# !pip install pyyaml==5.4.1"]},{"cell_type":"markdown","metadata":{"id":"vgvTrI7bevn2"},"source":["# **[STEP 1] Manual Configuration/Setup**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0_-wb8jb6M0h"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDerblMS6M0h"},"outputs":[],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive', force_remount=True)\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"DKEdKhZ_6M0h"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9z5ZRX_s6a34"},"outputs":[],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n","# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n","# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n","# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n","\n","#@markdown <hr>\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your <b>GDrive folder</b>\n","#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n","\n","#@markdown <hr>\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n","#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_reference'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'\n","else:\n","  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","# PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_TEXT_RAW}'\n","# PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","\n","# TODO: Clean up\n","# SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","print(f'PATH_SENTIMENT_RAW:\\n  [{PATH_SENTIMENT_RAW}]')\n","print(f'SUBDIR_SENTIMENT_RAW:\\n  [{SUBDIR_SENTIMENT_RAW}]')\n","\n","print('\\n')\n","\n","print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')\n","print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')"]},{"cell_type":"markdown","metadata":{"id":"HM4ePEaq6nrW"},"source":["# **[STEP 2] Automatic Configuration/Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPdbnOjw6ycy"},"outputs":[],"source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvMuohQZ6ycz"},"outputs":[],"source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_SENTIMENT_RAW = SUBDIR_SENTIMENT_RAW\n","global_vars.PATH_SENTIMENT_RAW = PATH_SENTIMENT_RAW\n","\n","global_vars.SUBDIR_TEXT_CLEAN = SUBDIR_TEXT_CLEAN\n","global_vars.PATH_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","dir(global_vars)"]},{"cell_type":"markdown","metadata":{"id":"zeLft8mw7moD"},"source":["## (each time) Custom Libraries & Define Globals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2IRi-3z7moE"},"outputs":[],"source":["# Initialize and clean for each iteration of notebook\n","\n","# dir(global_vars)\n","\n","global_vars.corpus_texts_dt = {}\n","global_vars.corpus_titles_dt = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IU1IHzA7moE"},"outputs":[],"source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWZWepaO7moE"},"outputs":[],"source":["# Call SentimentArcs Utility to define Global Variables\n","\n","sa_config.set_globals()\n","\n","# Verify sample global var set\n","print(f'MIN_PARAG_LEN: {global_vars.MIN_PARAG_LEN}')\n","print(f'STOPWORDS_ADD_EN: {global_vars.STOPWORDS_ADD_EN}')\n","print(f'global_vars.TEST_WORDS_LS: {global_vars.TEST_WORDS_LS}')\n","print(f'SLANG_DT: {global_vars.SLANG_DT}')"]},{"cell_type":"markdown","metadata":{"id":"CRb36wyH7moE"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjoIK5U_7moE"},"outputs":[],"source":["# Configure Jupyter\n","\n","# To reload modules under development\n","\n","# Option (a)\n","%load_ext autoreload\n","%autoreload 2\n","# Option (b)\n","# import importlib\n","# importlib.reload(functions.readfunctions)\n","\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"BW6YDyHT7moF"},"source":["## (each time) Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUveIcUOzYav"},"outputs":[],"source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n","\n","global_vars.corpus_titles_dt"]},{"cell_type":"markdown","metadata":{"id":"-pnjdzeSInEY"},"source":["## Install Libraries: Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSkNucf3InEY"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","\n","from glob import glob\n","import copy\n","import json # Installed above in YAML Configuration Section"]},{"cell_type":"markdown","metadata":{"id":"HvaOI_64InEY"},"source":["## Setup Matplotlib Style\n","\n","* https://matplotlib.org/stable/tutorials/introductory/customizing.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0q81NRhInEY"},"outputs":[],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------')\n","print('\\n  (Uncomment to view)')\n","# plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"n2KDazsOInEY"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMECX12r_CNo"},"outputs":[],"source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')\n","\n","\"\"\"\n","# Seaborn: Set Context\n","# sns.set_context(\"notebook\")\n","\n","# Seaborn: Set Theme (Scale of Font)\n","sns.set_theme('paper')  # paper, notebook, talk, poster\n","\n","# Seaborn: Set Style\n","# sns.set_style('ticks') # darkgrid, whitegrid, dark, white, and ticks\n","plt.style.use('seaborn-whitegrid')\n","\n","# sns.set_palette('tab10')\n","# sns.color_palette()\n","\n","# sns.set_palette('tab10')\n","# sns.color_palette()\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hStqTbVlInEZ"},"outputs":[],"source":["\"\"\"\n","# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')\n","# View previous seaborn configuration\n","print('\\n Old Seaborn Configurtion Settings:\\n')\n","sns.axes_style()\n","print('\\n\\n')\n","\n","# Update and View new seaborn configuration\n","print('\\n New Seaborn Configurtion Settings:\\n')\n","# sns.set_style('white')\n","sns.set_context('paper')\n","sns.set_style('white')\n","sns.set_palette('tab10')\n","\n","# Change defaults\n","# sns.set(style='white', context='talk', palette='tab10')\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"tweDF9wMInEZ"},"source":["## Python Utility Functions"]},{"cell_type":"markdown","metadata":{"id":"gjmkC1FbAEpo"},"source":["### (each time) Generate Convenient Data Lists"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TO08GFoGlP3y"},"outputs":[],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQNlQr4_Ckb1"},"outputs":[],"source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.models_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.models_ensemble_dt['lexicon']"]},{"cell_type":"markdown","metadata":{"id":"B3sXwZOq_-0d"},"source":["### File Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JQBWKKcN2Eo"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/file_utils.py'\n","# from utils.file_utils import *\n","\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"]},{"cell_type":"markdown","metadata":{"id":"35PXD0AjJtpQ"},"source":["# **[STEP 3] Read all Preprocessed Novels**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Roq-2Ol8yH5c"},"outputs":[],"source":["# Verify cwd and subdir of Cleaned Corpus Texts\n","\n","print('Current Working Directory:')\n","!pwd\n","\n","print(f'\\nSubdir with all Cleaned Texts of Corpus:\\n  {SUBDIR_TEXT_CLEAN}')\n","\n","print(f'\\n\\nFilenames of Cleaned Texts:\\n')\n","!ls -1 {Path_to_SentimentArcs}{PATH_TEXT_CLEAN}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ClL4-1Gqe7g"},"outputs":[],"source":["# Create a List (preprocessed_ls) of all preprocessed text files\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","try:\n","    preprocessed_ls = glob(f'{PATH_TEXT_CLEAN}/*.csv')\n","    preprocessed_ls = [x.split('/')[-1] for x in preprocessed_ls]\n","    preprocessed_ls = [x.split('.')[0] for x in preprocessed_ls]\n","except IndexError:\n","    raise RuntimeError('No csv file found')\n","\n","print('\\n'.join(preprocessed_ls))\n","print('\\n')\n","print(f'Found {len(preprocessed_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4CqJQY9rNRw"},"outputs":[],"source":["# Read all preprocessed text files into master DataFrame (corpus_dt)\n","\n","corpus_texts_dt = {}\n","\n","for i,atext in enumerate(preprocessed_ls):\n","  print(f'Processing #{i}: {atext}...')\n","  afile_fullpath = f'{PATH_TEXT_CLEAN}/{atext}.csv'\n","  print(f'               {afile_fullpath}')\n","  atext_df = pd.read_csv(afile_fullpath, index_col=[0])\n","  corpus_texts_dt[atext] = atext_df\n","\n","# Verify the Text read into master Dictionary of DataFrames\n","print('\\n')\n","corpus_texts_dt.keys()\n","print('\\n')\n","print(f'There were {len(corpus_texts_dt)} preprocessed Text read into the Dict corpus_texts_dt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JI2z5wCz8zz"},"outputs":[],"source":["# Check if there are any Null strings in the text_clean columns\n","\n","for i, atext in enumerate(list(corpus_texts_dt.keys())):\n","  print(f'\\nText #{i}: {atext}')\n","  nan_ct = corpus_texts_dt[atext].text_clean.isna().sum()\n","  if nan_ct > 0:\n","    print(f'      {nan_ct} Null strings in the text_clean column')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgLyDNrYzTuF"},"outputs":[],"source":["# Fill in all the Null value of text_clean with placeholder 'empty_string'\n","\n","for i, atext in enumerate(list(corpus_texts_dt.keys())):\n","  # print(f'Novel #{i}: {atext}')\n","  # Fill all text_clean == Null with 'empty_string' so sentimentr::sentiment doesn't break\n","  corpus_texts_dt[atext][corpus_texts_dt[atext].text_clean.isna()] = 'empty_string'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7gE08b_rNPH"},"outputs":[],"source":["# Verify DataFrame of first Text in Corpus Dictionary\n","\n","corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"markdown","metadata":{"id":"GuoJERbI0wEJ"},"source":["# **[STEP 4] Get Sentiments**"]},{"cell_type":"markdown","metadata":{"id":"Ov1cUBUm6HJ4"},"source":["## **Lexicons**\n","\n","* https://github.com/trinker/lexicon/tree/master/data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnE4x26nblZ_"},"outputs":[],"source":["# PyReadR enables Python to read R datafiles (e.g. *.rda)\n","\n","!pip install pyreadr\n","\n","import pyreadr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UoM390CcCsC"},"outputs":[],"source":["# Global Dict of all Lexicon Dictionaries\n","\n","lexicons_dt = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQfDHsxGeTmy"},"outputs":[],"source":["def get_lexsent_sentiment(asent_str, lexicon_dt):\n","  '''\n","  Given a Sentence in string form and a Lexicon Dictionary\n","  Return the Sentiment of the Sentence = Sum(Sentiment(all words))\n","  '''\n","\n","  sent_sentiment = 0\n","  word_ls = asent_str.split()\n","  for aword in word_ls:\n","    word_sentiment = lexicon_dt.get(aword)\n","    if word_sentiment != None:\n","      sent_sentiment += float(word_sentiment)\n","\n","  return sent_sentiment"]},{"cell_type":"markdown","metadata":{"id":"KZe-VOc16dYh"},"source":["### **Jockers-Rinker**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ww2IOTXetbV"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_jockersrinker'\n","model_type = 'Lexicon'\n","\n","url = f\"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_{lexicon_name}.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  str_sentiment_fl = get_lexsent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uriyktG9jIyu"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_lexsent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XG3o2yV0jgUc"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9bCl3yjz5Oy"},"outputs":[],"source":["# Verfiy Plausible Sentiment Values via Plot\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"wdP1zLpY6lm_"},"source":["### **HuLiu (aka Bing)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn7EdnHJkDDT"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'huliu'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_huliu'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_huliu.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  str_sentiment_fl = get_lexsent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgkD8_YRkDDV"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_lexsent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h54Q_R4fkDDX"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CG2hYqXckDDY"},"outputs":[],"source":["# Verfiy Plausible Sentiment Values via Plot\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"ZXHUQV-f63j_"},"source":["### **NRC**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mCtNSbmoQ2A"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'nrc'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_nrc'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_nrc.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  str_sentiment_fl = get_lexsent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRVTs8aZoQ2C"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_lexsent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyMa3VZyoQ2G"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn_VpRGJoQ2I"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"1_TKBGnF6ooO"},"source":["### **SentiWord**\n","\n","* https://www.sentic.net/sentic-patterns.pdf\n","* https://www.quora.com/Sentiment-Analysis-How-does-CLiPS-Pattern-calculate-the-polarity-of-a-sentence-What-is-the-maths-involved-in-it \n","* https://github.com/clips/pattern/wiki/pattern-en#sentiment\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMMKo1ehoiWp"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'sentiword'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_sentiword'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_sentiword.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  if (lexicons_dt[lexicon_name].get(aword_str.lower()) is None):\n","    # print(f'ERROR: {aword_str} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {aword_str} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(aword_str.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  str_sentiment_fl = get_lexsent_sentiment(asent_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {asent_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLWd8zF5oiWv"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_lexsent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCa0SKpzoiWz"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaZle71ToiW0"},"outputs":[],"source":["# Verfiy Plausible Sentiment Values via Plot\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"igCCPduG6vjZ"},"source":["### **SenticNet**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NtFDg-dozaZ"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'senticnet'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_senticnet'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_senticnet.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  str_sentiment_fl = get_lexsent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8PDMHBzozaf"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_lexsent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuLUHJunozah"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iFjMRiqozai"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"D7QotExQ6xBB"},"source":["### **Loughran-McDonald**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXTS5DCYpHHE"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'loughran_mcdonald'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_lmcd'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_loughran_mcdonald.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  str_sentiment_fl = get_lexsent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuPqJG8lpHHH"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_lexsent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2JVdy4JpHHJ"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzslktMQpHHK"},"outputs":[],"source":["# Verfiy Plausible Sentiment Values via Plot\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"4EdTcDNj67cK"},"source":["### **(FUTURE) MPQA**\n","\n","* https://mpqa.cs.pitt.edu/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9rae5Dm7AiG"},"outputs":[],"source":["# !wget https://mpqa.cs.pitt.edu/corpora/mpqa_corpus/mpqa_corpus_3_0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vY4Wgq6vrZ5"},"outputs":[],"source":["# !ls -altr\n","# !head -n 10 mpqa_corpus_3_0"]},{"cell_type":"markdown","metadata":{"id":"5BIT9nhu69u_"},"source":["### **(FUTURE) LIWC**\n","\n","* https://github.com/search?q=LIWC\n","* https://github.com/search?q=LIWC"]},{"cell_type":"markdown","metadata":{"id":"b9mEYM-p6kL5"},"source":["### **AFINN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kO2VyxeN6f15"},"outputs":[],"source":["!pip install afinn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRF2b7BnwVC9"},"outputs":[],"source":["from afinn import Afinn\n","afinn = Afinn(language='en')\n","\n","lexicon_name = 'AFINN'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'afinn'\n","model_type = 'Lexicon'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJ_4xTVEpo-Z"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  word_sentiment_fl = afinn.score(aword_str.lower())\n","  print(f'[{word_sentiment_fl: ^8}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  words_ls = asent_str.split()\n","  sent_sentiment_fl = 0.0\n","\n","  for j, atest_word in enumerate(words_ls):\n","    sent_sentiment_fl += afinn.score(atest_word.lower())\n","\n","  print(f'[{sent_sentiment_fl: ^8}]: {asent_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYpUFHOlqt1Z"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: afinn.score(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZ8NMFgwrNPu"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bC5I-fYErNPw"},"outputs":[],"source":["# Verfiy Plausible Sentiment Values via Plot\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"Q7s_OD2J_QHA"},"source":["#### **Save Checkpoint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Durdojkm7zl5"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","#   and destination Subdir for Raw Sentiment Values\n","\n","!pwd\n","print('\\n')\n","\n","print(f'SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n\\n')\n","\n","print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n","\n","subdir_path = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","!ls $subdir_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbiBAtrg9lJe"},"outputs":[],"source":["# Verify Saving Corpus\n","\n","print(f'Saving Corpus_Genre: {Corpus_Genre}')\n","print(f'        Corpus_Type: {Corpus_Type}')\n","print(f'      Corpus_Number: {Corpus_Number}')\n","\n","print(f'\\nThese Text Titles:\\n')\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-Kzyc3FEZ71"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","\n","if Corpus_Type == 'new':\n","  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all_7lex.json'\n","  print(f'Saving to:\\n  {save_filename}')\n","elif Corpus_Type == 'reference':\n","  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_all_7lex.json'\n","  print(f'Saving file:\\n  {save_filename}')\n","else:\n","  save_filename = ''\n","  print(f'ERROR: Illegal value for Corpus_Type: {Corpus_Type}')\n","\n","if len(save_filename) > 0:\n","  print(f'Writing to subdir:\\n  {subdir_path}')\n","  write_dict_dfs(corpus_texts_dt, out_file=save_filename, out_dir=subdir_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KthWAoxYiyd0"},"outputs":[],"source":["# Verify Dictionary was saved correctly \n","\n","!ls -altr $subdir_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTjSK5KX-BQm"},"outputs":[],"source":["# Verify Dictionary was saved correctly by reading back the *.json datafile\n","\n","test_dt = read_dict_dfs(in_file=save_filename, in_dir=subdir_path)\n","test_dt.keys()\n","print('\\n')\n","test_dt[corpus_texts_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"kSACJ2N06MNf"},"source":["## **Lexicons + Heuristics**"]},{"cell_type":"markdown","metadata":{"id":"XQXGPyAT7CMk"},"source":["### **VADER**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIt-cGAMPiv0"},"outputs":[],"source":["!pip install vaderSentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCb8sbP16Gz0"},"outputs":[],"source":["from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","vader_analyzer = SentimentIntensityAnalyzer()\n","\n","lexicon_name = 'VADER'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'vader'\n","model_type = 'Heuristic'\n","\n","test_str = \"The food was great!\"\n","\n","vs = vader_analyzer.polarity_scores(test_str)\n","print(\"{:-<65} {}\".format(test_str, str(vs)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brIvCNIAty6Y"},"outputs":[],"source":["def sent2vader_comp(asent_str):\n","  '''\n","  Given a Sentence as a text string\n","  Return a Sentiment = sum(VADER sentiments for each word)\n","  '''\n","\n","  words_ls = asent_str.split()\n","  sent_sentiment_fl = 0.0\n","\n","  for j, atest_word in enumerate(words_ls):\n","    sent_sentiment_fl += vader_analyzer.polarity_scores(atest_word.lower())['compound']\n","\n","  return sent_sentiment_fl\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk62GVwHsIR6"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  # print(f'Looking up VADER sentiment for {aword_str}')\n","  word_sentiment_fl = vader_analyzer.polarity_scores(aword_str.lower())['compound']\n","  print(f'[{word_sentiment_fl: ^8.3f}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  sent_sentiment_fl = sent2vader_comp(asent_str)\n","  print(f'[{sent_sentiment_fl: ^8.3f}]: {asent_str}\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyLw15CUtrnI"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: sent2vader_comp(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvxCFDrgtrnM"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_eKXyCTtrnQ"},"outputs":[],"source":["# Verfiy Plausible Sentiment Values via Plot\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"mT0Vp_f_09ak"},"source":["#### **Save Checkpoint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hzzj2MzU09al"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","#   and destination Subdir for Raw Sentiment Values\n","\n","!pwd\n","print('\\n')\n","\n","print(f'SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n\\n')\n","\n","print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n","\n","subdir_path = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","!ls $subdir_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daAha6Ww09am"},"outputs":[],"source":["# Verify Saving Corpus\n","\n","print(f'Saving Corpus_Genre: {Corpus_Genre}')\n","print(f'        Corpus_Type: {Corpus_Type}')\n","print(f'      Corpus_Number: {Corpus_Number}')\n","\n","print(f'\\nThese Text Titles:\\n')\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_PplRGt09am"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","\n","if Corpus_Type == 'new':\n","  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all_1heu.json'\n","  print(f'Saving to:\\n  {save_filename}')\n","elif Corpus_Type == 'reference':\n","  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_all_1hur.json'\n","  print(f'Saving file:\\n  {save_filename}')\n","else:\n","  save_filename = ''\n","  print(f'ERROR: Illegal value for Corpus_Type: {Corpus_Type}')\n","\n","if len(save_filename) > 0:\n","  print(f'Writing to subdir:\\n  {subdir_path}')\n","  write_dict_dfs(corpus_texts_dt, out_file=save_filename, out_dir=subdir_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PR8gyaWa09an"},"outputs":[],"source":["# Verify Dictionary was saved correctly \n","\n","!ls -altr $subdir_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQAh_zSq09an"},"outputs":[],"source":["# Verify Dictionary was saved correctly by reading back the *.json datafile\n","\n","test_dt = read_dict_dfs(in_file=save_filename, in_dir=subdir_path)\n","test_dt.keys()\n","print('\\n')\n","test_dt[corpus_texts_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"rH0_9K_x7L1E"},"source":["## **Embeddings**\n","\n","* https://neptune.ai/blog/document-classification-small-datasets\n","* https://neptune.ai/blog/sentiment-analysis-python-textblob-vs-vader-vs-flair (TB,VADER,Flair)"]},{"cell_type":"markdown","metadata":{"id":"EJsKKRNT3jfI"},"source":["### **(FUTURE) FastText**\n","\n","* https://github.com/facebookresearch/fastText\n","* https://medium.com/@lope.ai/sentiment-analysis-example-using-fasttext-6b1b4d334c53\n","* https://colab.research.google.com/drive/1bb2OWQcDDolESwhkATD0el0RvF33fenZ#scrollTo=X5PWbhOzZ3ze\n","* https://fasttext.cc/docs/en/english-vectors.html (embeddings)\n","* https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb\n","* https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/\n","* https://github.com/jatinmandav/Neural-Networks/tree/master/Sentiment-Analysis (Universal Sentence Encoder 77%, fastText 69%, word2vec 69%)\n","* https://github.com/search?q=fasttext+sentiment\n","\n","Code:\n","* https://github.com/charlesmalafosse/FastText-sentiment-analysis-for-tweets/blob/master/betsentiment_sentiment_analysis_fasttext.py (tweets)\n","* https://gist.github.com/hiteshn97/8f222a2773e11d6921b937abaa21ab75 (fastText,  keras)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ueVJ4OdG3jMV"},"outputs":[],"source":["!wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n","!unzip v0.9.2.zip\n","%cd fastText-0.9.2\n","!make"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BJ2LBlT365R"},"outputs":[],"source":["%%time\n","\n","train = open('tweets.train','w')  \n","test = open('tweets.valid','w')  \n","# with open('../sentiment140.1600000.csv', mode='r', encoding = \"ISO-8859-1\") as csv_file:  \n","with open('../sentiment140.csv', mode='r', encoding = \"ISO-8859-1\") as csv_file:  \n","    csv_reader = csv.DictReader(csv_file, fieldnames=['target', 'id', 'date', 'flag', 'user', 'text'])\n","    line = 0\n","    for row in csv_reader:\n","        # Clean the training data\n","        # First we lower case the text\n","        text = row[\"text\"].lower()\n","        # remove links\n","        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n","        #Remove usernames\n","        text = re.sub('@[^\\s]+','', text)\n","        # replace hashtags by just words\n","        text = re.sub(r'#([^\\s]+)', r'\\1', text)\n","        #correct all multiple white spaces to a single white space\n","        text = re.sub('[\\s]+', ' ', text)\n","        # Additional clean up : removing words less than 3 chars, and remove space at the beginning and teh end\n","        text = re.sub(r'\\W*\\b\\w{1,3}\\b', '', text)\n","        text = text.strip()\n","        line = line + 1\n","        # Split data into train and validation\n","        if line%16 == 0:\n","            print(f'__label__{row[\"target\"]} {text}', file=test)\n","        else:\n","            print(f'__label__{row[\"target\"]} {text}', file=train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyqYDJNp4t_0"},"outputs":[],"source":["%%time\n","\n","!./fasttext supervised -input tweets.train -output model_tweet\n","# !./fasttext supervised -input tweets.train -output model_tweet -epoch 30 -lr 0.1\n","# !./fasttext supervised -input tweets.train -output model_tweet -dim 300 -label __label__ -pretrainedVecctors wiki.ar.vec # Arabic for Netflix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8W2OKSnx4t4y"},"outputs":[],"source":["%%time\n","\n","!./fasttext test model_tweet.bin tweets.valid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPELBLRrV2_-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29mKvcVm5iu7"},"outputs":[],"source":["!pip install fasttext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G78xUCbg4tzL"},"outputs":[],"source":["from fasttext import load_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwEoQbMu5gIb"},"outputs":[],"source":["classifier = load_model('model_tweet.bin')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJyjaQ925gC3"},"outputs":[],"source":["text_ls = ['Ugghhh... Not happy at all! sorry', 'Happyyyyyyy', 'OH yeah! lets rock.']\n","labels = classifier.predict(text_ls)\n","print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hz-EMuLT7n8-"},"outputs":[],"source":["with open('test.txt','w') as fp:\n","  fp.write(\"\\n\".join(text_ls))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEXO_n957Aq-"},"outputs":[],"source":["!cat test.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsnuxOT68MaA"},"outputs":[],"source":["!./fasttext predict model_tweet.bin test.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G43FzjuP6tGs"},"outputs":[],"source":["!./fasttext predict-prob model_tweet.bin test.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zirGXVaF6OH8"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBVa-CKW6SQP"},"outputs":[],"source":["!ls ../"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYhVWbJ_6HKC"},"outputs":[],"source":["!head -n 10 tweets.train\n","\n","!cat tweets.train | wc -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOLyKOgR82Az"},"outputs":[],"source":["# Load Different Embeddings\n","\n","!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQa8KVjB-3fS"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1dE0ZqO8158"},"outputs":[],"source":["!unzip crawl-300d-2M-subword.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYZCsiQx-nvE"},"outputs":[],"source":["!ls "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKiZtpQe81wZ"},"outputs":[],"source":["import io\n","\n","fname = 'crawl-300d-2M-subword.vec'\n","\n","def load_vectors(fname):\n","    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    n, d = map(int, fin.readline().split())\n","    data = {}\n","    for line in fin:\n","        tokens = line.rstrip().split(' ')\n","        data[tokens[0]] = map(float, tokens[1:])\n","    return data\n","\n","load_vectors(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFYH2ctz9Zls"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmQFnTNn6HEN"},"outputs":[],"source":["%cd .."]},{"cell_type":"markdown","metadata":{"id":"GRGZrCnB7NnW"},"source":["### **TextBlob**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmg-Ru7jltY1"},"outputs":[],"source":["from textblob import TextBlob\n","\n","lexicon_name = 'TextBlob'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'textblob'\n","model_type = 'Heuristic'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7IsU2pbvwfxa"},"outputs":[],"source":["def sent2textblob(asent_str):\n","  '''\n","  Given a Sentence as a text string\n","  Return a Sentiment = sum(TextBlob sentiments for each word)\n","  '''\n","\n","  words_ls = asent_str.split()\n","  sent_sentiment_fl = 0.0\n","\n","  for j, atest_word in enumerate(words_ls):\n","    sent_sentiment_fl += TextBlob(atest_word.lower()).sentiment.polarity\n","\n","  return sent_sentiment_fl\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0CI3lnbwIh5"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  # print(f'Looking up VADER sentiment for {aword_str}')\n","  word_sentiment_fl = TextBlob(aword_str.lower()).sentiment.polarity\n","  print(f'[{word_sentiment_fl: ^8.3f}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  sent_sentiment_fl = sent2textblob(asent_str)\n","  print(f'[{sent_sentiment_fl: ^8.3f}]: {asent_str}\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDYKKgkZwIh7"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: sent2textblob(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJnbDLrVwIiI"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytrRoVcuwIiK"},"outputs":[],"source":["# Verfiy Plausible Sentiment Values via Plot\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"-KeOHItI1a5y"},"source":["#### **Save Checkpoint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYdScs9l1a5z"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","#   and destination Subdir for Raw Sentiment Values\n","\n","!pwd\n","print('\\n')\n","\n","print(f'SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n\\n')\n","\n","print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n","\n","subdir_path = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","!ls $subdir_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCXQWoYu1a5z"},"outputs":[],"source":["# Verify Saving Corpus\n","\n","print(f'Saving Corpus_Genre: {Corpus_Genre}')\n","print(f'        Corpus_Type: {Corpus_Type}')\n","print(f'      Corpus_Number: {Corpus_Number}')\n","\n","print(f'\\nThese Text Titles:\\n')\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxGedJra1a5z"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","\n","if Corpus_Type == 'new':\n","  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all_1emb.json'\n","  print(f'Saving to:\\n  {save_filename}')\n","elif Corpus_Type == 'reference':\n","  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_all_1emb.json'\n","  print(f'Saving file:\\n  {save_filename}')\n","else:\n","  save_filename = ''\n","  print(f'ERROR: Illegal value for Corpus_Type: {Corpus_Type}')\n","\n","if len(save_filename) > 0:\n","  print(f'Writing to subdir:\\n  {subdir_path}')\n","  write_dict_dfs(corpus_texts_dt, out_file=save_filename, out_dir=subdir_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnHQAaue1a5z"},"outputs":[],"source":["# Verify Dictionary was saved correctly \n","\n","!ls -altr $subdir_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHv25kxI1a5z"},"outputs":[],"source":["# Verify Dictionary was saved correctly by reading back the *.json datafile\n","\n","test_dt = read_dict_dfs(in_file=save_filename, in_dir=subdir_path)\n","test_dt.keys()\n","print('\\n')\n","test_dt[corpus_texts_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"b1YbJ36B77Jy"},"source":["## **Linguistic Models**"]},{"cell_type":"markdown","metadata":{"id":"D0cBuDeT8ABx"},"source":["### **Pattern**\n","\n","* https://github.com/clips/pattern/blob/master/examples/03-en/07-sentiment.py\n","\n","* https://github.com/clips/pattern/wiki/pattern-en#sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cW0EnGst74Cm"},"outputs":[],"source":["!pip install pattern"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JeqRykx8O8b"},"outputs":[],"source":["from pattern.en import sentiment, polarity, subjectivity, positive\n","\n","lexicon_name = 'Pattern'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pattern'\n","model_type = 'Linguistic'\n","\n","# Sentiment analysis (or opinion mining) attempts to determine if\n","# a text is objective or subjective, positive or negative.\n","# The sentiment analysis lexicon bundled in Pattern focuses on adjectives.\n","# It contains adjectives that occur frequently in customer reviews,\n","# hand-tagged with values for polarity and subjectivity.\n","\n","# The polarity() function measures positive vs. negative, as a number between -1.0 and +1.0.\n","# The subjectivity() function measures objective vs. subjective, as a number between 0.0 and 1.0.\n","# The sentiment() function returns an averaged (polarity, subjectivity)-tuple for a given string.\n","for word in (\"amazing\", \"horrible\", \"public\"):\n","    print(word, sentiment(word))\n","\n","print(\"\")\n","print(sentiment(\n","    \"The movie attempts to be surreal by incorporating time travel and various time paradoxes,\"\n","    \"but it's presented in such a ridiculous way it's seriously boring.\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5pOXkW-85OW"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n","\n","  # print(f'Looking up VADER sentiment for {aword_str}')\n","  word_sentiment_fl = polarity(aword_str.lower())\n","  print(f'[{word_sentiment_fl: ^8.3f}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n","\n","  sent_sentiment_fl = polarity(asent_str)\n","  print(f'[{sent_sentiment_fl: ^8.3f}]: {asent_str}\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3icn6UiW-EXS"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: polarity(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NeiLvBYn-EXT"},"outputs":[],"source":["corpus_texts_dt[corpus_texts_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbDHAD6d-EXT"},"outputs":[],"source":["# Verfiy Plausible Sentiment Values via Plot\n","\n","text_indx = 0\n","\n","text_title_str = corpus_texts_ls[text_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{global_vars.corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.grid(True)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"3zEY65BP1vQG"},"source":["#### **Save Checkpoint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bt7S1kK_1vQG"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","#   and destination Subdir for Raw Sentiment Values\n","\n","!pwd\n","print('\\n')\n","\n","print(f'SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n\\n')\n","\n","print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n","\n","subdir_path = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","!ls $subdir_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d7yv0Vo1vQG"},"outputs":[],"source":["# Verify Saving Corpus\n","\n","print(f'Saving Corpus_Genre: {Corpus_Genre}')\n","print(f'        Corpus_Type: {Corpus_Type}')\n","print(f'      Corpus_Number: {Corpus_Number}')\n","\n","print(f'\\nThese Text Titles:\\n')\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NK90MOip1vQH"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","\n","if Corpus_Type == 'new':\n","  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all_1ling.json'\n","  print(f'Saving to:\\n  {save_filename}')\n","elif Corpus_Type == 'reference':\n","  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_all_1ling.json'\n","  print(f'Saving file:\\n  {save_filename}')\n","else:\n","  save_filename = ''\n","  print(f'ERROR: Illegal value for Corpus_Type: {Corpus_Type}')\n","\n","if len(save_filename) > 0:\n","  print(f'Writing to subdir:\\n  {subdir_path}')\n","  write_dict_dfs(corpus_texts_dt, out_file=save_filename, out_dir=subdir_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mbwjn2dp1vQH"},"outputs":[],"source":["# Verify Dictionary was saved correctly \n","\n","!ls -altr $subdir_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDFCBI8v1vQH"},"outputs":[],"source":["# Verify Dictionary was saved correctly by reading back the *.json datafile\n","\n","test_dt = read_dict_dfs(in_file=save_filename, in_dir=subdir_path)\n","test_dt.keys()\n","print('\\n')\n","test_dt[corpus_texts_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"pWJPszHQFsUY"},"source":["## **Single ML Models**\n","\n","* Linear Regression\n","* Logistic Regression\n","* Random Forest Classifier\n","* Linear SVC\n","* MultinomialNB\n","\n","Tutorials\n","\n","* https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Tutorial) \n","* https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb (github)\n","* https://wellsr.com/python/python-sentiment-analysis-with-sklearn/\n","* https://colab.research.google.com/drive/186bOdu08nv4xHe6VeBgt_aIk9_fziqsX#scrollTo=hpDp3V0Lg-sw"]},{"cell_type":"markdown","metadata":{"id":"GYWqur2AGZRq"},"source":["### **Load Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAh_pmRSIHIE"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from wordcloud import WordCloud,STOPWORDS\n","from bs4 import BeautifulSoup\n","import re,string,unicodedata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eNELxlEGZDc"},"outputs":[],"source":["from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB"]},{"cell_type":"markdown","metadata":{"id":"z8OsKDwm-pVr"},"source":["### **Get 50k IMDB Dataset**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Q0uNBpLz84B"},"outputs":[],"source":["# A large IMDB dataset will be created in a subdiretory here \n","\n","os.chdir(Path_to_SentimentArcs)\n","!pwd"]},{"cell_type":"markdown","metadata":{"id":"fABJnTRX0y0B"},"source":["#### **Option(a): Get from Stanford (slow ~15mins, no login)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G97EyubGzruy"},"outputs":[],"source":["%%time\n","\n","# NOTE:    12m47s @02:40 on 20220421 Colab Pro CPU\n","\n","# https://gdcoder.com/sentiment-clas/\n","\n","# chdir to data subdir\n","os.chdir(f'{Path_to_SentimentArcs}/data')\n","\n","!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!gunzip aclImdb_v1.tar.gz\n","!tar -xvf aclImdb_v1.tar\n","\n","# delete left over *.tar file (284.4M)\n","!rm aclImdb_v1.tar\n","\n","# chdir back to SentimentArcs root project folder\n","os.chdir(Path_to_SentimentArcs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daKk5t9HzrqH"},"outputs":[],"source":["# Explore downloaded/unzipped IMDB raw text files\n","\n","PATH_ACLIMDB='data/aclImdb/'\n","names = ['neg','pos']\n","!ls {PATH_ACLIMDB}\n","!ls {PATH_ACLIMDB}train\n","!ls {PATH_ACLIMDB}train/pos | head\n","\n","#Similar for the test folder\n","!ls {PATH_ACLIMDB}test\n","!ls {PATH_ACLIMDB}test/pos | head"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMbR30g1A49l"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0bQrwn8B18n"},"outputs":[],"source":["!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYe_EcwyA79i"},"outputs":[],"source":["!ls data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NOR_uN_BY0Y"},"outputs":[],"source":["temp_str = f'./{PATH_ACLIMDB}train'\n","temp_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0GL5nMtBj7z"},"outputs":[],"source":["PATH_ACLIMDB = 'aclImdb/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zw9hbiLIB9a5"},"outputs":[],"source":["!ls $PATH_ACLIMDB"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vfmmd-31Aonk"},"outputs":[],"source":["temp_str = f'./{PATH_ACLIMDB}train'\n","!ls $temp_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqlkDIJ1zrmx"},"outputs":[],"source":["%%time\n","\n","# NOTE:  1m31 @02:47 on 20220421 Colab Pro\n","\n","# Load text from files\n","\n","def load_texts_labels_from_folders(path, folders):\n","    texts,labels = [],[]\n","    for idx,label in enumerate(folders):\n","        for fname in glob(os.path.join(path, label, '*.*')):\n","            texts.append(open(fname, 'r').read())\n","            labels.append(idx)\n","    # stored as np.int8 to save space \n","    return texts, np.array(labels).astype(np.int8)\n","\n","train_x,train_y = load_texts_labels_from_folders(f'{PATH_ACLIMDB}train',names)\n","test_x,test_y = load_texts_labels_from_folders(f'{PATH_ACLIMDB}test',names)\n","\n","len(train_x),len(train_y),len(test_x),len(test_y)\n","\n","len(train_y[train_y==1]),len(test_y[test_y==1])\n","\n","np.unique(train_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfyxJT7P0j1Z"},"outputs":[],"source":["# View sample IDBM file\n","\n","print(train_x[0])\n","print()\n","print(f\"Review's label: {train_y[0]}\")\n","# 0 represent a negative review"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Ipez_0IEtdD"},"outputs":[],"source":["len(train_x + test_x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DekTg88SFRGI"},"outputs":[],"source":["len(test_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKdPzCGZFWgN"},"outputs":[],"source":["len(all_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-GFeNbOI0jx4"},"outputs":[],"source":["all_x = train_x + test_x\n","print(f'len(all_x): {len(all_x)}')\n","all_y = train_y + test_y\n","print(f'len(all_y): {len(all_y)}')\n","\n","imdb_df = pd.DataFrame({'review':all_x, 'sentiment':all_y})\n","imdb_df['review'] = imdb_df['review'].astype('string')\n","imdb_df.head()\n","imdb_df.info()"]},{"cell_type":"markdown","metadata":{"id":"FcKN5xG-0_WQ"},"source":["#### **Option(b): Get from Kaggle (immediate, login)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIFJk0wH79c9"},"outputs":[],"source":["# https://www.kaggle.com/code/derrelldsouza/imdb-sentiment-analysis-eda-ml-lstm-bert/notebook\n","\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgWnvY-00jux"},"outputs":[],"source":["# Upload Kaggle API Authorization Tokens (kaggle.json file)\n","\n","os.chdir(f'{Path_to_SentimentArcs}/data')\n","\n","# create hidden directory\n","!mkdir ~/.kaggle\n","\n","from google.colab import files\n","uploaded = files.upload()\n","\n","!mv kaggle.json ~/.kaggle\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","os.chdir(Path_to_SentimentArcs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXe3-ZYDzrkE"},"outputs":[],"source":["# Get IMDB dataset\n","\n","os.chdir(f'{Path_to_SentimentArcs}/data')\n","\n","!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","# SECURITY: delete kaggle.json file with private credentials each time\n","!rm -rf ~/.kaggle\n","\n","os.chdir(Path_to_SentimentArcs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E44-X40B9JuH"},"outputs":[],"source":["# SECURITY: Ensure hidden kaggle subdir with kaggle.json credential file is deleted\n","\n","# EXPECTED OUTPUT: \"ls: cannot access '/root/.kaggle': No such file or directory\"\n","\n","!ls ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f45d6o-Y-jaR"},"outputs":[],"source":["# Unzip datafile\n"," \n","os.chdir(f'{Path_to_SentimentArcs}/data')\n","\n","!unzip imdb-dataset-of-50k-movie-reviews.zip \n","!mv IMDB\\ Dataset.csv imdb_dataset.csv\n","\n","os.chdir(Path_to_SentimentArcs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sv3LzgN297Ne"},"outputs":[],"source":["# read IMDB dataset\n","\n","imdb_df=pd.read_csv('./data/imdb_dataset.csv')\n","imdb_df['review'] = imdb_df['review'].astype('string')\n","imdb_df.head()\n","imdb_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sI_T2dv97I5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYZEOe4qFEIO"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkl4398e-sXm"},"outputs":[],"source":["# Split X/y into Training and Testing Datasets\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ilM-luCnDPn8"},"outputs":[],"source":["X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yFUG5IuDTyy"},"outputs":[],"source":["y_train"]},{"cell_type":"markdown","metadata":{"id":"9oq5hbRsGA1M"},"source":["### **Remove Nulls and Duplicate Rows**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KV_3oIgGGmgD"},"outputs":[],"source":["# Summary Statistics\n","\n","imdb_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_Y5XjzkG06T"},"outputs":[],"source":["# Null Values\n","\n","null_values = imdb_df.isnull().sum() #identifying missing values\n","null_values\n","\n","null_values.index[0]\n","print('There are {} missing values for {} and {} missing values for {}.'.format(null_values[0],null_values.index[0],null_values[1],null_values.index[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZI4tbevVG01I"},"outputs":[],"source":["# Duplicate Values\n","\n","num_duplicates = imdb_df.duplicated().sum() #identify duplicates\n","print('There are {} duplicate reviews present in the dataset'.format(num_duplicates))\n","\n","#view duplicate reviews\n","review = imdb_df['review']\n","duplicated_review = imdb_df[review.isin(review[review.duplicated()])].sort_values(\"review\")\n","duplicated_review.head()\n","\n","#drop duplicate reviews\n","imdb_df.drop_duplicates(inplace = True)\n","\n","print('The dataset contains {} rows and {} columns after removing duplicates'.format(imdb_df.shape[0],imdb_df.shape[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJP4ngdjX8uz"},"outputs":[],"source":["# Bootstrap for Unbalanced Sentiment Classes\n","\n","\"\"\"\n","# IMDB is balanced 25k/25k so no need to adjust\n","\n","train, test = train_test_split(df_training, test_size=0.3, random_state=1)t_1 = train[train['sentiment']==1].sample(800,replace=True)\n","t_2 = train[train['sentiment']==2].sample(800,replace=True)\n","t_3 = train[train['sentiment']==0].sample(800,replace=True)\n","training_bs = pd.concat([t_1, t_2, t_3])print train.shape\n","print training_bs.shape\n","print test.shape# sanity check \n","df_training.shape[0] == (train.shape[0] + test.shape[0])\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"qQtavz3OH5En"},"source":["### **Text Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tmrZzP7IMVX"},"outputs":[],"source":["# Get Stopwords\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","stop = stopwords.words('english')\n","wl = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Slh4YheHS-a"},"outputs":[],"source":["mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n","           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n","           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n","           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n","           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n","           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n","           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n","           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n","           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n","           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n","           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n","           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n","           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n","           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n","           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n","           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n","           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n","           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n","           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n","           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n","           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n","           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n","           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n","           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n","           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n","           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n","           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n","           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n","           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n","           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n","           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n","           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n","           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n","           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n","           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n","           \"you're\": \"you are\", \"you've\": \"you have\" }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyDXdYMDHSw9"},"outputs":[],"source":["#function to clean data\n","\n","def clean_text(text,lemmatize = True):\n","    soup = BeautifulSoup(text, \"html.parser\") #remove html tags\n","    text = soup.get_text()\n","    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) #expanding chatwords and contracts clearing contractions\n","    emoji_clean= re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    text = emoji_clean.sub(r'',text)\n","    text = re.sub(r'\\.(?=\\S)', '. ',text) #add space after full stop\n","    text = re.sub(r'http\\S+', '', text) #remove urls\n","    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #remove punctuation\n","    #tokens = re.split('\\W+', text) #create tokens\n","    if lemmatize:\n","        text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()]) #lemmatize\n","    else:\n","        text = \" \".join([word for word in text.split() if word not in stop and word.isalpha()]) \n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yu4cLnEoHSqL"},"outputs":[],"source":["imdb_copy_df = imdb_df.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rga9DrCWF7Au"},"outputs":[],"source":["# Clean Text and Numerically encode sentiment\n","\n","imdb_df['review']=imdb_df['review'].apply(clean_text,lemmatize = True)\n","\n","#converting target variable to numeric labels\n","imdb_df.sentiment = [ 1 if each == \"positive\" else 0 for each in imdb_df.sentiment]\n","\n","#after converting labels\n","imdb_df.head()"]},{"cell_type":"markdown","metadata":{"id":"KsDsAHd_JRZf"},"source":["### **EDA**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSMpAdeaJPgI"},"outputs":[],"source":["# Check for Imbalanced Data with Count Plot\n","\n","sns.set(style = \"whitegrid\" , font_scale = 1.2)\n","_ = sns.countplot(imdb_df.sentiment,palette = ['green','red'],order = [1,0])\n","_ = plt.xticks(ticks = np.arange(2),labels = ['positive','negative'])\n","_ = plt.title('Target count for IMBD reviews')\n","plt.show()\n","\n","# Print Summary \n","print('Positive reviews are', (round(imdb_df['sentiment'].value_counts()[0])),'i.e.', round(imdb_df['sentiment'].value_counts()[0]/len(imdb_df) * 100,2), '% of the dataset')\n","print('Negative reviews are', (round(imdb_df['sentiment'].value_counts()[1])),'i.e.',round(imdb_df['sentiment'].value_counts()[1]/len(imdb_df) * 100,2), '% of the dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yip7ruNkJxyV"},"outputs":[],"source":["%%time\n","\n","# NOTE: 1m05s @04:06 on 20220421 Colab CPU\n","\n","# Word Cloud for positive reviews\n","\n","positive_data = imdb_df[imdb_df.sentiment == 1]['review']\n","positive_data_string = ' '.join(positive_data)\n","_ = plt.figure(figsize = (20,20))\n","wc = WordCloud(max_words = 2000, width=1200, height=600,background_color=\"white\").generate(positive_data_string)\n","plt.imshow(wc , interpolation = 'bilinear')\n","plt.axis('off')\n","plt.title('Word cloud for positive reviews',fontsize = 20)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8canwwGKBfn"},"outputs":[],"source":["%%time\n","\n","# NOTE: 1m05s @04:06 on 20220421 Colab CPU\n","\n","# Word Cloud for negative reviews\n","\n","negative_data = imdb_df[imdb_df.sentiment == 0]['review']\n","negative_data_string = ' '.join(negative_data)\n","_ = plt.figure(figsize = (20,20))\n","wc = WordCloud(max_words = 2000, width=1200, height=600,background_color=\"white\").generate(negative_data_string)\n","plt.imshow(wc , interpolation = 'bilinear')\n","plt.axis('off')\n","plt.title('Word cloud for negative reviews',fontsize = 20)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Az1f0AqJKW9Y"},"outputs":[],"source":["# Character Counts per Review Type\n","\n","_ = fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n","text_len=positive_data.str.len()\n","_ = ax1.hist(text_len,color='green')\n","_ = ax1.set_title('Positive Reviews')\n","_ = ax1.set_xlabel('Number of Characters')\n","_ = ax1.set_ylabel('Count')\n","text_len=negative_data.str.len()\n","_ = ax2.hist(text_len,color='red')\n","_ = ax2.set_title('Negative Reviews')\n","_ = ax2.set_xlabel('Number of Characters')\n","_ = ax2.set_ylabel('Count')\n","_ = fig.suptitle('Number of characters in texts')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0AITcCUvKW58"},"outputs":[],"source":["# Word Count per Review Type\n","\n","_ = fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n","\n","text_len=positive_data.str.split().map(lambda x: len(x))\n","_ = ax1.hist(text_len,color='green')\n","_ = ax1.set_title('Positive Reviews')\n","_ = ax1.set_xlabel('Number of Words')\n","_ = ax1.set_ylabel('Count')\n","text_len=negative_data.str.split().map(lambda x: len(x))\n","_ = ax2.hist(text_len,color='red')\n","_ = ax2.set_title('Negative Reviews')\n","_ = ax2.set_xlabel('Number of Words')\n","_ = ax2.set_ylabel('Count')\n","_ = fig.suptitle('Number of words in texts')\n","_ = plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xHpkbOjK3HZ"},"outputs":[],"source":["# Word Count Distribution per Review Type\n","\n","_ = fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\n","word = positive_data.str.split().apply(lambda x : len(x) )\n","_ = sns.distplot(word, ax=ax1,color='green')\n","_ = ax1.set_title('Positive Reviews')\n","_ = ax1.set_xlabel('Number of words per review')\n","word = negative_data.str.split().apply(lambda x :len(x) )\n","_ = sns.distplot(word,ax=ax2,color='red')\n","_ = ax2.set_title('Negative Reviews')\n","_ = ax2.set_xlabel('Number of words per review')\n","_ = fig.suptitle('Distribution of number of words per reviews')\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2Ma4UruK2yw"},"outputs":[],"source":["# Word Length Distribution per Review Type\n","\n","_ = fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\n","word = positive_data.str.split().apply(lambda x : [len(i) for i in x] )\n","_ = sns.distplot(word.map(lambda x: np.mean(x)), ax=ax1,color='green')\n","_ = ax1.set_title('Positive Reviews')\n","_ = ax1.set_xlabel('Average word length per review')\n","word = negative_data.str.split().apply(lambda x : [len(i) for i in x] )\n","_ = sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\n","_ = ax2.set_title('Negative Reviews')\n","_ = ax2.set_xlabel('Average word length per review')\n","_ = fig.suptitle('Distribution of average word length in each review')\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXCLqcGDLZzU"},"outputs":[],"source":["# Get List of Corpus Vocab\n","\n","def get_corpus(text):\n","    words = []\n","    for i in text:\n","        for j in i.split():\n","            words.append(j.strip())\n","    return words\n","\n","corpus = get_corpus(imdb_df.review)\n","corpus[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbUreW2lLnZg"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kg34_TyGLZsQ"},"outputs":[],"source":["# Get Word Freq\n","\n","counter = Counter(corpus)\n","\n","most_common = counter.most_common(10)\n","most_common = pd.DataFrame(most_common,columns = ['corpus','countv'])\n","most_common"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYNtr5hjLvRE"},"outputs":[],"source":["# Plot Most Common Words\n","\n","most_common = most_common.sort_values('countv')\n","\n","_ = plt.figure(figsize =(10,10))\n","_ = plt.yticks(range(len(most_common)), list(most_common.corpus))\n","_ = plt.barh(range(len(most_common)), list(most_common.countv),align='center',color = 'blue')\n","_ = plt.title('Most common words in the dataset')\n","plt.show();\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybO8MzjyLvNf"},"outputs":[],"source":["# Get nGrams\n","\n","def get_ngrams(review, n, g):\n","    vec = CountVectorizer(ngram_range=(g, g)).fit(review)\n","    bag_of_words = vec.transform(review) #sparse matrix of count_vectorizer\n","    sum_words = bag_of_words.sum(axis=0) #total number of words\n","    sum_words = np.array(sum_words)[0].tolist() #convert to list\n","    words_freq = [(word, sum_words[idx]) for word, idx in vec.vocabulary_.items()] #get word freqency for word location in count vec\n","    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) #key is used to perform sorting using word_freqency \n","    return words_freq[:n]\n","\n","_ = fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\n","uni_positive = get_ngrams(positive_data,20,1)\n","uni_positive = dict(uni_positive)\n","temp = pd.DataFrame(list(uni_positive.items()), columns = [\"Common_words\" , 'Count'])\n","_ = sns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\n","_ = ax1.set_title('Positive reviews')\n","uni_negative = get_ngrams(negative_data,20,1)\n","uni_negative = dict(uni_negative)\n","temp = pd.DataFrame(list(uni_negative.items()), columns = [\"Common_words\" , 'Count'])\n","_ = sns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\n","_ = ax2.set_title('Negative reviews')\n","_ = fig.suptitle('Unigram analysis for positive and negative reviews')\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nQDkAvQLvIY"},"outputs":[],"source":["%%time\n","\n","# NOTE: 1m\n","\n","# Bigrams\n","\n","_ = fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\n","bi_positive = get_ngrams(positive_data,20,2)\n","bi_positive = dict(bi_positive)\n","temp = pd.DataFrame(list(bi_positive.items()), columns = [\"Common_words\" , 'Count'])\n","_ = sns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\n","_ = ax1.set_title('Positive reviews')\n","bi_negative = get_ngrams(negative_data,20,2)\n","bi_negative = dict(bi_negative)\n","temp = pd.DataFrame(list(bi_negative.items()), columns = [\"Common_words\" , 'Count'])\n","_ = sns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\n","_ = ax2.set_title('Negative reviews')\n","_ = fig.suptitle('Bigram analysis for positive and negative reviews')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25XuyTtuLvE5"},"outputs":[],"source":["%%time\n","\n","# NOTE: 55s\n","\n","# Trigrams\n","\n","_ = fig,(ax1,ax2)=plt.subplots(1,2,figsize=(30,15))\n","tri_positive = get_ngrams(positive_data,20,3)\n","tri_positive = dict(tri_positive)\n","temp = pd.DataFrame(list(tri_positive.items()), columns = [\"Common_words\" , 'Count'])\n","_ = sns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax1)\n","_ = ax1.set_title('Positive reviews')\n","tri_negative = get_ngrams(negative_data,20,3)\n","tri_negative = dict(tri_negative)\n","temp = pd.DataFrame(list(tri_negative.items()), columns = [\"Common_words\" , 'Count'])\n","_ = sns.barplot(data = temp, x=\"Count\", y=\"Common_words\", orient='h',ax = ax2)\n","_ = ax2.set_title('Negative reviews')\n","fig.suptitle('Trigram analysis for positive and negative reviews')\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"2ex0mBh7M0Am"},"source":["### **Split & Vectorize Dataset (TF-IDF)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUnXSAAlMXTz"},"outputs":[],"source":["#splitting into train and test\n","\n","train, test= train_test_split(imdb_df, test_size=0.2, random_state=42)\n","Xtrain, ytrain = train['review'], train['sentiment']\n","Xtest, ytest = test['review'], test['sentiment']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5f-J9pIiR4KX"},"outputs":[],"source":["type(Xtrain)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UDgdwu8MXQU"},"outputs":[],"source":["#Vectorizing data\n","\n","tfidf_vect = TfidfVectorizer() #tfidfVectorizer\n","Xtrain_tfidf = tfidf_vect.fit_transform(Xtrain)\n","Xtest_tfidf = tfidf_vect.transform(Xtest)\n","\n","\n","count_vect = CountVectorizer() # CountVectorizer\n","Xtrain_count = count_vect.fit_transform(Xtrain)\n","Xtest_count = count_vect.transform(Xtest)"]},{"cell_type":"markdown","metadata":{"id":"ngBTbHcjNSsh"},"source":["## **OLD**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jls0zfQZNLHn"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPY34w3ENLDi"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5_XIItRNLAC"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBVV6EbWNK7x"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"uums5JYP3IcS"},"source":["### **Vectorize with (a) TF-IDF or (b) BOW**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aS2rCt_23bUW"},"source":["##### **Option (a) TF-IDF**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BU4pMi-FtbS"},"outputs":[],"source":["  from nltk.corpus import stopwords\n","  stopwords_en = stopwords.words('english') # + stopwords.words('french')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOQpAcZMEsAc"},"outputs":[],"source":["def tfidf_embeddings(df, text_clean_col='text_clean', label_col='polarity', ngrams=3, features=1000):\n","  '''\n","  Given a DataFrame and col name with clean text along with hyperparameters: max#ngrams and max#features\n","  Return DataFrame X with word embeddings and Series y.polarity\n","  '''\n","\n","  # Hyperparamters\n","\n","  NGRAM_MAX = ngrams\n","  FEATURES_MAX = features\n","\n","\n","  # Vectorize Training dataset with TF-IDF\n","  # NOTE: 1m11s\n","  vectorizer = TfidfVectorizer(ngram_range=(1,NGRAM_MAX), stop_words=stopwords_en, max_features=FEATURES_MAX)\n","\n","  # Select 'text_clean' or 'text_raw'\n","  embeddings = vectorizer.fit_transform(df[text_clean_col])  \n","  # embeddings = vectorizer.fit_transform(df.text_raw)\n","\n","  emb_words_df = pd.DataFrame(embeddings.toarray(), columns=vectorizer.get_feature_names())\n","  # emb_words_df.head()\n","\n","  # Split text features/vectorized samples from labels\n","  X = emb_words_df\n","  # y = df.polarity # sentiment\n","  if label_col == '':\n","    y = pd.Series([0]*df.shape[0])\n","  elif label_col == 'polarity':\n","    y = df[label_col] # sentiment\n","  else:\n","    print(f'ERROR: illegal value for label_col={label_col}')\n","\n","  return X, y, vectorizer, embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4iS_RD1GMDI"},"outputs":[],"source":["%%time\n","\n","# NOTE: 59s to 1m34s\n","\n","X, y, vecs, embeds = tfidf_embeddings(training_df, text_clean_col='text_clean')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_b1-wBl0G6bg"},"outputs":[],"source":["X.shape\n","print('\\n')\n","y.shape\n","type(y[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q50zsws73ovb"},"outputs":[],"source":["# Hyperparamters\n","\n","# NGRAM_MAX = 3\n","# FEATURES_MAX = 1000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nghy6iGH3YiA"},"outputs":[],"source":["%%time\n","\n","# Vectorize Training dataset with TF-IDF\n","\n","# NOTE: 1m11s\n","\n","\"\"\"\n","from nltk.corpus import stopwords\n","stopwords_en = stopwords.words('english') # + stopwords.words('french')\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1,NGRAM_MAX), stop_words=stopwords_en, max_features=FEATURES_MAX)\n","\n","# Select 'text_clean' or 'text_raw'\n","embeddings = vectorizer.fit_transform(training_df.text_clean)  \n","# embeddings = vectorizer.fit_transform(training_df.text_raw)\n","\n","emb_words_df = pd.DataFrame(embeddings.toarray(), columns=vectorizer.get_feature_names())\n","emb_words_df.head()\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiSM7UYk4ovd"},"outputs":[],"source":["# Split text features/vectorized samples from labels\n","\n","\"\"\"\n","X = emb_words_df\n","y = training_df.polarity # sentiment\n","X.shape\n","print('\\n')\n","y.shape\n","type(y[0])\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"qNIMwLvsBccH"},"source":["##### **Option (b) BOW**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GITuWTTBfJB"},"outputs":[],"source":["countvector=CountVectorizer(ngram_range=(2,2))\n","traindataset=countvector.fit_transform(training_df['text_clean'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skO_9n6NBhND"},"outputs":[],"source":["# Separate text from labels\n","\n","X = words_df\n","y = training_df.sentiment\n","X.shape\n","print('\\n')\n","y.shape\n","type(y[0])"]},{"cell_type":"markdown","metadata":{"id":"5U6w8mktNgzQ"},"source":["## **Single ML Models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"puwC6qvXQ4QW"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1cLNhc0Qo07"},"outputs":[],"source":["#Vectorizing data\n","\n","tfidf_vect = TfidfVectorizer() #tfidfVectorizer\n","Xtrain_tfidf = tfidf_vect.fit_transform(Xtrain)\n","Xtest_tfidf = tfidf_vect.transform(Xtest)\n","\n","\n","count_vect = CountVectorizer() # CountVectorizer\n","Xtrain_count = count_vect.fit_transform(Xtrain)\n","Xtest_count = count_vect.transform(Xtest)"]},{"cell_type":"markdown","metadata":{"id":"HV-27uu47W9h"},"source":["### **Multinomial Naive Bayes (multinb)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBBAGFJPJ82N"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgAz1RSGEKEj"},"outputs":[],"source":["%%time\n","\n","# Naive Bayes\n","\n","model = MultinomialNB()\n","model.fit(X_train, y_train)\n","\n","metrics(model, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJIYLZBHHCGq"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FeWJg6_JCOHT"},"outputs":[],"source":["training_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3veP92ZCkSu"},"outputs":[],"source":["# analyzer is the parameter that the vectorizer reads the input data in word unit or character unit to create a matrix\n","# vocabulary is the parameter that the vectorizer creates the matrix by using only input data or some other source \n","# Other parameters are self-explanatory and already mentioned in other notebooks.\n","\n","tfidf_vectorizer = TfidfVectorizer(\n","                    ngram_range = (1,3),\n","                    sublinear_tf = True,\n","                    max_features = 1000)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jzADkYYN7qE"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0AGVaf9gNyTM"},"outputs":[],"source":["%%time\n","\n","X_corpus, y, vecs, embeds = tfidf_embeddings(corpus_sents_df, text_clean_col='sent_clean', label_col='')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGTIzmztCkSw"},"outputs":[],"source":["%%time\n","\n","# NOTE: 1m27s 40000 max_features\n","#       1m09s  1000 max_features\n","\n","# Handle with care especially when you transform the test dataset. (Wrong: fit_transform(X_test))\n","\n","# train_features = tfidf_vectorizer.fit_transform(training_df['text_clean']) # X_train['clean'])\n","# test_features = tfidf_vectorizer.transform(corpus_sents_df['sent_clean']) # X_test['clean'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSlnY4MoCN8C"},"outputs":[],"source":["%%time\n","\n","# Fit a naive bayes model to the training data.\n","# This will train the model using the word counts we computer, and the existing classifications in the training set.\n","\n","# model = MultinomialNB()\n","# model.fit(train_features, training_df['polarity']) # [int(r[1]) for r in reviews])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zn3hsRSPrsC"},"outputs":[],"source":["corpus_filename"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoeKC_TaCNth"},"outputs":[],"source":["%%time\n","\n","# NOTE: 0m33s on 20210916 at 06:12 (jausten_prideandprejudice)\n","\n","# Now we can use the model to predict classifications for our test features.\n","\n","predictions_corpus = model.predict(X_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udBON1OxDy8H"},"outputs":[],"source":["type(predictions_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82G9GY9zDyzY"},"outputs":[],"source":["predictions_corpus.size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXZ-TtUFDysd"},"outputs":[],"source":["corpus_sents_df['multinb'] = predictions_corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oor-BMRcEd2n"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdTexXP0Rwi5"},"outputs":[],"source":["# corpus_sents_df['multinb_std'] = corpus_sents_df['multinb'].apply(lambda x: \n","                                                                \n","standardize_tsls(corpus_sents_df, ['multinb'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_ss_iSPESOH"},"outputs":[],"source":["corpus_sents_df['multinb'].apply(lambda x: 4*(x-0.6)).rolling(900, center=True).mean().plot(label='MultinomialNB')\n","corpus_sents_df['sentimentr_stdscaler'].rolling(900, center=True).mean().plot(label='SentimentR')\n","corpus_sents_df['vader_stdscaler'].rolling(900, center=True).mean().plot(label='VADER')\n","plt.legend(loc='best');\n","plt.title(f'{CORPUS_FULL}\\nMultinomial Naive Bayes (Default w/IMDB) SMA=10%');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxuD_7rGESF0"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5MX-jB-ER9V"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XhwPRtGER2B"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlOGnJZ_2SjP"},"outputs":[],"source":["# tfidf_embeddings(training_df, text_clean_col='text_clean')\n","\n","training_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tPlTGuB25Of"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBI55RL6x_X1"},"outputs":[],"source":["text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', MultinomialNB())])\n","\n","tuned_parameters = {\n","    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n","    'tfidf__use_idf': (True, False),\n","    'tfidf__norm': ('l1', 'l2'),\n","    'clf__alpha': [1, 1e-1, 1e-2]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyuUvNqJzE5o"},"outputs":[],"source":["X_train.shape\n","y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZYPDwGly0LP"},"outputs":[],"source":["%%time\n","\n","score = 'f1_macro'\n","print(\"# Tuning hyper-parameters for %s\" % score)\n","print()\n","np.errstate(divide='ignore')\n","clf = GridSearchCV(text_clf, tuned_parameters, cv=10, scoring=score)\n","clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meJo0y-VyOMq"},"outputs":[],"source":["# Results of grid search for hyperparameters\n","\n","print(\"Best parameters set found on development set:\")\n","print()\n","print(clf.best_params_)\n","print()\n","print(\"Grid scores on development set:\")\n","print()\n","for mean, std, params in zip(clf.cv_results_['mean_test_score'], \n","                             clf.cv_results_['std_test_score'], \n","                             clf.cv_results_['params']):\n","    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n","print()\n","\n","print(\"Detailed classification report:\")\n","print()\n","print(\"The model is trained on the full development set.\")\n","print(\"The scores are computed on the full evaluation set.\")\n","print()\n","print(classification_report(y_test, clf.predict(x_test), digits=4))\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3S1Gx6ktyOFq"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xH-iHwde1YZu"},"outputs":[],"source":["# analyzer is the parameter that the vectorizer reads the input data in word unit or character unit to create a matrix\n","# vocabulary is the parameter that the vectorizer creates the matrix by using only input data or some other source \n","# Other parameters are self-explanatory and already mentioned in other notebooks.\n","\n","tfidf_vectorizer = TfidfVectorizer(\n","                    ngram_range = (1,3),\n","                    sublinear_tf = True,\n","                    max_features = 40000)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U317I1DA1omw"},"outputs":[],"source":["%%time\n","\n","# NOTE: 1m27s\n","\n","# Handle with care especially when you transform the test dataset. (Wrong: fit_transform(X_test))\n","\n","train_features = tfidf_vectorizer.fit_transform(training_df['text_clean']) # X_train['clean'])\n","test_features = tfidf_vectorizer.transform(corpus_sents_df['sent_clean']) # X_test['clean'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn6FgSjy1YRj"},"outputs":[],"source":["# Create the list of vocabulary used for the vectorizer.\n","\n","vocab = tfidf_vectorizer.get_feature_names()\n","print(vocab[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwQDsJ8Q1YJv"},"outputs":[],"source":["# Print vocabulary length\n","\n","print(\"Vocabulary length:\", len(vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYbpChaF1YDN"},"outputs":[],"source":["# Print frequency of each word\n","\n","dist = np.sum(train_features, axis=0)\n","word_dist_df = pd.DataFrame(dist,columns = vocab)\n","word_dist_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0zK_IMP3auQ"},"outputs":[],"source":["print('Training dim:',train_features.shape, 'Test dim:', test_features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UjMz9oE37oH"},"outputs":[],"source":["text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', MultinomialNB())])\n","\n","tuned_parameters = {\n","    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n","    'tfidf__use_idf': (True, False),\n","    'tfidf__norm': ('l1', 'l2'),\n","    'clf__alpha': [1, 1e-1, 1e-2]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rQkSLId7QsT"},"outputs":[],"source":["%%time\n","\n","# Naive Bayes\n","\n","model = MultinomialNB()\n","\n","model.fit(train_features, training_df['text_clean'] )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw7earNw84rq"},"outputs":[],"source":["%whos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CFrbOiT38ehS"},"outputs":[],"source":["%%time\n","\n","predictions = model.predict(test_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXMAo6KI3al6"},"outputs":[],"source":["# Naive Bayes\n","\n","# sv = LinearSVC(random_state=2018)\n","model = MultinomialNB()\n","\n","param_grid = {\n","    'vect__ngram_range': [(1, 1), (1, 2)], # , (2, 2)],\n","    # 'tfidf__use_idf': (True, False),\n","    'tfidf__norm': ('l1', 'l2'),\n","    'alpha': [1, 1e-1, 1e-2]\n","}\n","\n","param_grid2 = {\n","    'loss':['squared_hinge'],\n","    'class_weight':[{1:4}],\n","    'C': [0.2]\n","}\n","kfold=5\n","\n","# gs_sv = GridSearchCV(sv, param_grid = [param_grid2], verbose = 1, cv = kfold, n_jobs = 1, scoring = 'roc_auc')\n","gs_model = GridSearchCV(model, param_grid = [param_grid], verbose = 1, cv = kfold, n_jobs = 1, scoring = 'roc_auc')\n","gs_model.fit(train_tv, training_df['polarity'])\n","gs_model_best = gs_model.best_estimator_\n","print(gs_model.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTVjuVTh3uLE"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLlFRk_H3uBc"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0o40tLieKXWv"},"outputs":[],"source":["  # Vectorize Training dataset with TF-IDF\n","  # NOTE: 1m11s\n","  vectorizer = TfidfVectorizer(ngram_range=(1,NGRAM_MAX), stop_words=stopwords_en, max_features=FEATURES_MAX)\n","\n","  # Select 'text_clean' or 'text_raw'\n","  embeddings = vectorizer.fit_transform(corpus_sents_df['sent_clean'])  \n","  # embeddings = vectorizer.fit_transform(df.text_raw)\n","\n","  emb_words_df = pd.DataFrame(embeddings.toarray(), columns=vectorizer.get_feature_names())\n","  # emb_words_df.head()\n","\n","  # Split text features/vectorized samples from labels\n","  X = emb_words_df\n","  y = df.polarity # sentiment\n","\n","  return X, y, vectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVwTxGW6Jlow"},"outputs":[],"source":["# Hyperparamters\n","\n","NGRAM_MAX = 3\n","FEATURES_MAX = 1000\n","\n","\n","# Vectorize Training dataset with TF-IDF\n","# NOTE: 1m11s\n","vec = TfidfVectorizer(ngram_range=(1,NGRAM_MAX), stop_words=stopwords_en, max_features=FEATURES_MAX)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMWmD-JAIIBQ"},"outputs":[],"source":["model.predict(vec.transform(['Love this app simply awesome!']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gvegj2I0Eb4S"},"outputs":[],"source":["X_corpus, y = tfidf_embeddings(training_df, text_clean_col='text_clean')\n","\n","X, y = tfidf_embeddings(training_df)"]},{"cell_type":"markdown","metadata":{"id":"jyi7Pi-u7cVK"},"source":["### **Support Vector Machine (svm)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qs-GJ2UBKQ8W"},"outputs":[],"source":["from sklearn.svm import LinearSVC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIHGKeni7QmH"},"outputs":[],"source":["%%time\n","\n","# Create and train a linear support vector classifier (LinearSVC)\n","# SST2: 20s \n","# IMDB50k: 1.7s\n","\n","# Linear SVC\n","\n","model = LinearSVC()\n","model.fit(X_train, y_train)\n","\n","metrics(model, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YC-ArHHK7W4y"},"outputs":[],"source":["%%time\n","\n","# Create and train a linear support vector classifier (LinearSVC)\n","# SST2: 20s \n","# IMDB50k: 1.7s\n","\n","# Linear SVC\n","\n","# https://www.kaggle.com/derrelldsouza/imdb-sentiment-analysis-eda-ml-lstm-bert#5.-Predictive-Modelling-using-Deep-Learning\n","model = LinearSVC(penalty='l2',loss = 'hinge')\n","model.fit(X_train, y_train)\n","\n","metrics(model, X_test, y_test)\n"]},{"cell_type":"markdown","metadata":{"id":"4gOBlOeB7fa9"},"source":["### **Linear Regression (linreg)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmg0hVJy7Qhe"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BX-n0nRKkgZ"},"outputs":[],"source":["%%time\n","\n","# Create and train a logistic regression\n","# SST2: 1800s/1000it, 20s/10it (default, max_iter=1000)\n","# IMDB50k: 1000it, 1.14s\n","\n","model = LogisticRegression(C=1e9, solver='lbfgs', max_iter=10)\n","model.fit(X_train, y_train)\n","\n","metrics(model, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"LAXVHHQ77hH_"},"source":["### **Logistic Regression (logreg)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1mwyx8rVYJt"},"outputs":[],"source":["Xtrain_tfidf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4r02DRjIVe6P"},"outputs":[],"source":["ytrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-5GIC_fO-z1"},"outputs":[],"source":["# logistic regression\n","\n","model_name = 'lr'\n","\n","# model = LogisticRegression()\n","model = LogisticRegression(C=1e9, solver='lbfgs', max_iter=10)\n","\n","model.fit(Xtrain_tfidf,ytrain)\n","p1=model.predict(Xtest_tfidf)\n","s1=accuracy_score(ytest,p1)\n","print(\"\\nLogistic Regression Accuracy :\", \"{:.2f}%\".format(100*s1))\n","_ = plot_confusion_matrix(model, Xtest_tfidf, ytest,cmap = 'Blues')\n","plt.grid(False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enmRdRMLTBGf"},"outputs":[],"source":["\"\"\"\n","text_sa_clf = Pipeline([('tfidf', TfidfVectorizer()),\n","                     ('clf', LogisticRegression()),]) # model),])\n","text_sa_clf.fit(Xtrain_tfidf,ytrain)\n","predictions = text_sa_clf.predict(Xtrain_tfidf)\n","accuracy_train = accuracy_score(ytrain, predictions)\n","print('Accuracy Score for train :',accuracy_train)\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9P25H-96aJ4r"},"outputs":[],"source":["imdb_df['review'] = imdb_df['review'].astype('string')\n","imdb_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-DnX7AdZ9WX"},"outputs":[],"source":["tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None, use_idf=True, norm='l2', smooth_idf=True)\n","\n","y = imdb_df.sentiment.values\n","X = tfidf.fit_transform(imdb_df['review'].values)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, shuffle=True)\n","# clf = LogisticRegressionCV(cv=5, scoring=\"accuracy\", random_state=1, n_jobs=-1, verbose=3,max_iter=300).fit(X_train, y_train)\n","clf = LogisticRegression(C=1e9, solver='lbfgs', max_iter=10)\n","clf.fit(X_train, y_train)\n","# yhat = clf.predict(X_test)\n","\n","\n","# print(\"\\nTrain Accuracy:\")\n","# print(clf.score(X_test, y_test))\n","\n","yhat=clf.predict(X_test)\n","# s1=accuracy_score(X_test,yhat)\n","s1 = clf.score(y_test, yhat)\n","# print(\"\\nTest Accuracy:\")\n","# print(clf.score(X_test, y_test))\n","\n","print(\"\\nLogistic Regression Accuracy :\", \"{:.2f}%\".format(100*s1))\n","_ = plot_confusion_matrix(clf, X_test, yhat,cmap = 'Blues')\n","plt.grid(False)\n","\n","# model_performance(X_train, y_train, X_test, y_test, clf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOAXnSEuZgNe"},"outputs":[],"source":["tfidf2 = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None, tokenizer='fill', use_idf=True, norm='l2', smooth_idf=True)\n","# y = df.sentiment.values\n","Xjoker = tfidf2.transform(jokerData)\n","\n","yhat = Clf.predict(Xjoker)\n","\n","# 25,584"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfGAyPurYmGb"},"outputs":[],"source":["new_reviews = ['Old version of python useless', 'Very good effort, but not five stars', 'Clear and concise']\n","X_new = tfidf_vect.transform(new_reviews)\n","model.predict(X_new)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRqrBSRLReWQ"},"outputs":[],"source":["temp_df = copy.deepcopy(corpus_texts_dt['cmieville_thecityandthecity'])\n","temp_df.drop(columns=['text_raw'], inplace=True)\n","temp_df['text_clean'] = temp_df['text_clean'].astype('string')\n","temp_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBPxXvmbXGCc"},"outputs":[],"source":["Xtemp_tfidf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vZ6tAjMXUm-"},"outputs":[],"source":["Xtest_tfidf = tfidf_vect.transform(Xtest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcEZAGAUW2kH"},"outputs":[],"source":["Xtemp_tfidf = tfidf_vect.transform(temp_df) # ['text_clean'])\n","preds_model =model.predict(Xtemp_tfidf)\n","preds_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHXc5b-_THv2"},"outputs":[],"source":["#splitting into train and test\n","\n","train, test= train_test_split(temp, test_size=0.2, random_state=42)\n","Xtrain, ytrain = train['review'], train['sentiment']\n","Xtest, ytest = test['review'], test['sentiment']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Nbirs6oTHfH"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2uh6R-qRoSd"},"outputs":[],"source":["tfidf_vect.fit_transform('I love lint')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGeBTSbJO-wm"},"outputs":[],"source":["# Fill in all the Null value of text_clean with placeholder 'empty_string'\n","\n","for i, atext in enumerate(list(corpus_texts_dt.keys())):\n","  # print(f'Novel #{i}: {atext}')\n","  # Fill all text_clean == Null with 'empty_string' so sentimentr::sentiment doesn't break\n","  sents_tfidf_ser = tfidf_vect.fit_transform(corpus_texts_dt[atext]['text_raw'])\n","  corpus_texts_dt[atext][model_name] = lr.predict(sents_tfidf_ser)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6YM-DSRO-rd"},"outputs":[],"source":["#Vectorizing data\n","\n","tfidf_vect = TfidfVectorizer() #tfidfVectorizer\n","Xtrain_tfidf = tfidf_vect.fit_transform(Xtrain)\n","Xtest_tfidf = tfidf_vect.transform(Xtest)\n","\n","\n","count_vect = CountVectorizer() # CountVectorizer\n","Xtrain_count = count_vect.fit_transform(Xtrain)\n","Xtest_count = count_vect.transform(Xtest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGYQ7dfcH0sd"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Hc4F5697Qd9"},"outputs":[],"source":["%%time\n","\n","# Create and train a logistic regression\n","# SST2: 1800s/1000it, 20s/10it (default, max_iter=1000)\n","# IMDB50k: 1000it, 1.14s\n","\n","model = LogisticRegression(C=1e9, solver='lbfgs', max_iter=10)\n","model.fit(X_train, y_train)\n","\n","metrics(model, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oSxS0JpFiva"},"outputs":[],"source":["%%time\n","\n","# Logistic Regression\n","\n","logreg = LogisticRegression(C=0.1, solver='sag')\n","logreg.fit(X_train, y_train)\n","metrics(logreg, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"rScF40DE5v8y"},"source":["## **Ensemble ML Models**"]},{"cell_type":"markdown","metadata":{"id":"8YVN75-47lLr"},"source":["### **Random Forest (rforest)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxNh-o0vK38Y"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"miuuMkE56GjA"},"outputs":[],"source":["%%time\n","\n","# Create and train a random forest classifier\n","# SST2: 27m-1500s/50n_est  36s/5n_est (default n_estimators=50)\n","# IMDB50k: 46s (n_est=50)\n","\n","# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 (Hyperparm search)\n","# https://github.com/WillKoehrsen/Machine-Learning-Projects/tree/master/random_forest_explained (github/Jupyter)\n","\n","model = RandomForestClassifier(n_estimators=50)\n","model.fit(X, y)\n","metrics(model, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrsBAoIv_Bbs"},"outputs":[],"source":["%%time\n","\n","# Create and train a random forest classifier\n","# SST2: 27m-1500s/50n_est  36s/5n_est (default n_estimators=50)\n","# IMDB50k: 46s (n_est=50)\n","\n","# https://www.kaggle.com/itsmanas/stock-sentiment-analysis-85-9-accuracy/data#Train-Test-Splitting\n","\n","model = RandomForestClassifier(n_estimators=200,criterion='entropy')\n","model.fit(X, y)\n","metrics(model, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"M1ze4Wd-7osb"},"source":["### **XGBoost (xgb)**\n","\n","* https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/ (Gradient Boosting)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7PNXPrNLW4P"},"outputs":[],"source":["from xgboost.sklearn import XGBClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gih9TYsY7oa-"},"outputs":[],"source":["%%time\n","\n","model = XGBClassifier()\n","model.fit(X_train, y_train)\n","metrics(model, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"Gea1Gw_yPCO9"},"source":["### **LightGBM (lgbm)**\n","\n","* https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzVzYQ8IPHDp"},"outputs":[],"source":["!pip install lightgbm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5Wx1r1AfV0P"},"outputs":[],"source":["from lightgbm import LGBMClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N42_h_WOPZqa"},"outputs":[],"source":["# lightgbm for classification\n","\n","from numpy import mean\n","from numpy import std\n","\n","# from sklearn.datasets import make_classification\n","\n","# from sklearn.model_selection import cross_val_score\n","# from sklearn.model_selection import RepeatedStratifiedKFold\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7y-7VJBhPO_R"},"outputs":[],"source":["# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n","# evaluate the model\n","model = LGBMClassifier()\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n","print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","# fit the model on the whole dataset\n","model = LGBMClassifier()\n","model.fit(X, y)\n","# make a single prediction\n","row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n","yhat = model.predict(row)\n","print('Prediction: %d' % yhat[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWTvIeMxPO51"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iF9bJBBOi1zd"},"outputs":[],"source":["!pip install scikit-plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZO4DYdJPO0c"},"outputs":[],"source":["import scikitplot as skplt"]},{"cell_type":"markdown","metadata":{"id":"S07--vKs7rP0"},"source":["### **CATBoost (catb)**\n","\n","* https://www.kaggle.com/aryantiwari123/tweets-sentiment-analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2hI61po7oXR"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7MQpjBONNXY"},"outputs":[],"source":["# y_1d = np.squeeze(y, -1)\n","y_1d.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geXEWLqgNLec"},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAJXNs5eha73"},"outputs":[],"source":["X_np = X.to_numpy()\n","X_np.shape\n","type(X_np)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldyesnn3ggVJ"},"outputs":[],"source":["y_np = y.to_numpy()\n","# type(np)\n","print(y_np.shape)\n","y_np_s = np.squeeze(y_np)\n","y_np_s.shape\n","type(y_np_s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuHBOr8ufq3K"},"outputs":[],"source":["def model_classify(model, X, y):\n","\n","    # train test split\n","    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, shuffle=True, stratify=y)\n","\n","    # model training\n","    # pipeline_model = Pipeline([('vect', CountVectorizer()),\n","    #                           ('tfidf', TfidfTransformer()),\n","    #                           ('clf', model)])\n","    \n","    pipeline_model = Pipeline([('clf',model)])\n","\n","    pipeline_model.fit(x_train, y_train)\n","    print('done: fit')\n","    \n","    print('Accuracy:', pipeline_model.score(x_test, y_test)*100)\n","    \n","    print(\"Training Score:\\n\",pipeline_model.score(x_train,y_train)*100)\n","\n","\n","    y_pred = pipeline_model.predict(x_test)\n","    y_probas =pipeline_model.predict_proba(x_test)\n","    skplt.metrics.plot_roc(y_test,y_probas,figsize=(10,6),title_fontsize=14,text_fontsize=12)\n","    plt.show()\n","    skplt.metrics.plot_precision_recall(y_test,y_probas,figsize=(10,6),title_fontsize=14,text_fontsize=12)\n","    plt.show()\n","    skplt.estimators.plot_learning_curve(pipeline_model, X,y,figsize=(10,6),title_fontsize=14,text_fontsize=12)\n","    plt.show()\n","    skplt.metrics.plot_lift_curve(y_test,y_probas,figsize=(10,6),title_fontsize=14,text_fontsize=12)\n","    plt.show()\n","    skplt.metrics.plot_confusion_matrix(y_test,y_pred,figsize=(10,6),title_fontsize=14,text_fontsize=12,cmap=plt.cm.Pastel1)\n","    plt.show()\n","    print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNx8ovXhMDBc"},"outputs":[],"source":["%time\n","\n","# NOTE: 15m\n","\n","model = AdaBoostClassifier(base_estimator = None)\n","model_classify(model, X_np, y_np)\n","metrics(model, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"2ojvbqvb7tvb"},"source":["### **AdaBoost (adab)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHIZIzgn7tl2"},"outputs":[],"source":["%time\n","\n","model = AdaBoostClassifier(base_estimator = None)\n","model.fit(X_train, y_train)\n","metrics(model, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xehOgn4qo0e7"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"RZB6O8_BFaA-"},"source":["#### Setup and Configuration\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZS-7DdAstqfg"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from scipy.sparse import save_npz, load_npz  # load/save sparse matrices\n","from joblib import dump, load # load/save sklearn objects efficiently\n","\n","from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ym-yOrRbq5N"},"outputs":[],"source":["# Vectorizing text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# Validation: https://www.kaggle.com/pocooo/types-of-cross-validation-all-you-need\n","\n","# Simple Cross Fold Validation\n","from sklearn.model_selection import KFold\n","# model=DecisionTreeClassifier()\n","kfold_validation=KFold(10)\n","\n","# import numpy as np\n","from sklearn.model_selection import cross_val_score\n","# results=cross_val_score(model,X,y,cv=kfold_validation)\n","# print(results)\n","# print(np.mean(results))\n","\n","# Stratified CV \n","from sklearn.model_selection import StratifiedKFold\n","skfold=StratifiedKFold(n_splits=5)\n","# model=DecisionTreeClassifier()\n","# scores=cross_val_score(model,X,y,cv=skfold)\n","# print(np.mean(scores))\n","\n","# LOO CV\n","from sklearn.model_selection import LeaveOneOut\n","# model=DecisionTreeClassifier()\n","leave_validation=LeaveOneOut()\n","# results=cross_val_score(model,X,y,cv=leave_validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcLbN-4SFiQM"},"outputs":[],"source":["from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TyQL5V2N44X_"},"outputs":[],"source":["# TODO: Next few cells unneeded?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuzrvLoAqfDV"},"outputs":[],"source":["nlp = spacy.blank('en')\n","\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","# nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPgwPT39GC7t"},"outputs":[],"source":["stopwords_custom = ['bazinga', 'hoohaw', 'pating']\n","stopwords_en.extend(stopwords_custom)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSLqFGzPBCHB"},"outputs":[],"source":["# Utility Functions for ML Model Metrics\n","\n","# https://www.kaggle.com/aditya6040/7-models-on-imdb-dataset-best-score-88-2/notebook\n","\n","def metrics(model,x,y):\n","    y_pred = model.predict(x)\n","    acc = accuracy_score(y, y_pred)\n","    f1=f1_score(y, y_pred)\n","    cm=confusion_matrix(y, y_pred)\n","    report=classification_report(y,y_pred)\n","    plt.figure(figsize=(4,4))\n","    sns.heatmap(cm,annot=True,cmap='Blues',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","    plt.xlabel(\"Predicted\",fontsize=16)\n","    plt.ylabel(\"Actual\",fontsize=16)\n","    plt.show()\n","    print(\"\\nAccuracy: \",round(acc,2))\n","    print(\"\\nF1 Score: \",round(f1,2))\n","#     print(\"\\nConfusion Matrix: \\n\",cm)\n","    print(\"\\nReport:\",report)\n","\n","def lexicon_metrics(y, y_pred):\n","    acc = accuracy_score(y, y_pred)\n","    f1=f1_score(y, y_pred)\n","    cm=confusion_matrix(y, y_pred)\n","    report=classification_report(y, y_pred)\n","    plt.figure(figsize=(4,4))\n","    sns.heatmap(cm,annot=True,cmap='Blues',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","    plt.xlabel(\"Predicted\",fontsize=16)\n","    plt.ylabel(\"Actual\",fontsize=16)\n","    plt.show()\n","    print(\"\\nAccuracy: \",round(acc,2))\n","    print(\"\\nF1 Score: \",round(f1,2))\n","#     print(\"\\nConfusion Matrix: \\n\",cm)\n","    print(\"\\nReport:\",report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdO8oGyOMrQk"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","text_clf_SGDClassifier = Pipeline([('vect', CountVectorizer(ngram_range=(2,4), stop_words='english',lowercase=True)),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', SGDClassifier()),\n","])\n","text_clf_SGDClassifier.fit(X, y)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATqWwSe0wYpA"},"outputs":[],"source":["!ls ./data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ai2efaWnOWHv"},"outputs":[],"source":["# TEST\n","fld = './data/'\n","\n","[os.path.join(fld, f) for f in os.listdir() if os.path.isfile(os.path.join(fld, f))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hH62PhR6OWDI"},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import plotly\n","import plotly.graph_objs as go\n","# import plotly.offline as ply\n","# plotly.offline.init_notebook_mode()\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","from subprocess import check_output\n","# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHd2hh9SMrMB"},"outputs":[],"source":["classifier_names = ['Naive Bayes', \n","                    'Decision Tree', \n","                    'Random Forest', \n","                    'Nearest Neighbors', \n","                    'Neural Network']\n","\n","classifiers = [GaussianNB(),\n","               DecisionTreeClassifier(max_depth=10),\n","               RandomForestClassifier(max_depth=10),\n","               KNeighborsClassifier(5),\n","               MLPClassifier()]\n","\n","plot_data=[]\n","\n","clf_data=zip(classifier_names, classifiers)\n","\n","for clf_name, clf in clf_data:\n","    print('Running '+clf_name)\n","    kf=StratifiedKFold(n_splits=10, shuffle=True)\n","    scores=cross_val_score(clf, X, y, cv=kf)\n","    print(scores)\n","    plot_data.append(\n","        go.Scatter(\n","            x=[i+1 for i in range(10)],\n","            y=scores,\n","            mode='lines',\n","            name=clf_name\n","        )\n","    )\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uCxhjm8BFxZu"},"source":["#### Download Sentiment IMDB Training Dataset (if necessary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMDev_d1Fqtb"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIx0eQ5351ac"},"outputs":[],"source":["# Check if IMDB datafile exists, download if missing\n","\n","filepath_imdb = f\"{SUBDIR_DATA}imdb-dataset-of-50k-movie-reviews.zip\"\n","print(f'filepath: {filepath_imdb}')\n","\n","my_file = Path(filepath_imdb)\n","if my_file.is_file():\n","  print('\\n[SKIP] to the next Section [Load IMDB Dataset]\\n       IMDB training dataset already exists\\n')\n","else:\n","  print('\\n[CONTINUE] executing code cells')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEG1gTB_Fw33"},"outputs":[],"source":["!mkdir ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV3AjO_pFwzV"},"outputs":[],"source":["from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pk-GVEkhGM17"},"outputs":[],"source":["!mv kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8GzXNBXGMw3"},"outputs":[],"source":["# Get IMDB Dataset\n","\n","%cd ./data\n","!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fuf_6fzGcDR"},"outputs":[],"source":["!unzip imdb-dataset-of-50k-movie-reviews.zip\n","!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TrwYRBK-8w4Q"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","!pwd"]},{"cell_type":"markdown","metadata":{"id":"ej9sAyl07k9f"},"source":["### Prepare IMDB Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfZ-yckW9En7"},"outputs":[],"source":["# Verify in project root Directory\n","\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAcrC6QZGWMQ"},"outputs":[],"source":["imdb50k_df = pd.read_csv(f\"{SUBDIR_DATA}IMDB Dataset.csv\")\n","imdb50k_df[\"polarity\"] = imdb50k_df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n","imdb50k_df[\"text_raw\"] = imdb50k_df[\"review\"].astype('string')\n","imdb50k_df.drop(columns=['sentiment', 'review'], inplace=True)\n","\n","# supervised_db = 'imdb50k'\n","\n","imdb50k_df.head()\n","imdb50k_df.info()"]},{"cell_type":"markdown","metadata":{"id":"fqMeeO7_HGpB"},"source":["#### Clean and Split Training Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wo_6LdRZki1Z"},"outputs":[],"source":["import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaeXdyROkdOL"},"outputs":[],"source":["import texthero as hero\n","from texthero import preprocessing\n","\n","stem_pipeline = [preprocessing.fillna,\n","                 preprocessing.lowercase,\n","                 preprocessing.remove_digits,\n","                 preprocessing.remove_punctuation,\n","                 preprocessing.remove_diacritics,\n","                 preprocessing.remove_stopwords,\n","                 preprocessing.remove_whitespace,\n","                 preprocessing.stem]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfRBFMSOnp-3"},"outputs":[],"source":["%%time\n","\n","# NOTE: 3m55s @02:54 on 20220302 Colab Pro\n","#        \n","\n","imdb50k_df['text_clean'] = imdb50k_df['text_raw'].pipe(hero.clean, stem_pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZsgdHprlxyn"},"outputs":[],"source":["# Saved Cleaned IMDB Training Text Datafile\n","\n","fname_imdb50k_clean = f'{SUBDIR_DATA}imdb50k_clean.csv'\n","imdb50k_df.to_csv(fname_imdb50k_clean, index=False)\n","!ls -altr "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6ez71a0nc7B"},"outputs":[],"source":["!ls ./data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DvzzCG6AZuh"},"outputs":[],"source":["TRAIN_TEST_SPLIT_PER = 0.8\n","\n","data_len = imdb50k_df.shape[0]\n","split_indx = int(TRAIN_TEST_SPLIT_PER * data_len)\n","\n","train_df = imdb50k_df[:split_indx]\n","test_df = imdb50k_df[split_indx:]\n","\n","print(f'Splitting {TRAIN_TEST_SPLIT_PER*100:.2f}% of Training Data for Testing')\n","print(f'  There are {data_len} examples, {split_indx} of them used for Testing, {data_len - split_indx} for Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4fdW-Msn8pb"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivBGtsFLpf1S"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from joblib import dump, load # used for saving and loading sklearn objects\n","from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzU1BtJUsstX"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","from sklearn import datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_cZAjGFnwYE"},"outputs":[],"source":["%%time\n","\n","# NOTE: \n","\n","# Vectorize IMDB Review (Representation: Trigram TF-IDF)\n","\n","# Step 1: Bigram\n","bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=30000)\n","bigram_vectorizer.fit(train_df['text_clean'].values)\n","\n","# X_train_bigram = bigram_vectorizer.transform(train_df['text_clean'].values)\n","\n","\n","# Step 2: TF-IDF\n","# Texthero removes stopwords\n","# CountVectorizer includes high-freq bigrams\n","# CountVectorizer limits max_features/token vocabulary size\n","# without max features IMDB Training (40k reviews), X_train_bigram_tf_idf shape = (40000, 2211883)\n","# bigram_tf_idf_transformer = TfidfTransformer(ngram_range=(1,2), stop_words=stopwords_en_ls, max_features=1000)\n","bigram_tf_idf_transformer = TfidfTransformer()\n","bigram_tf_idf_transformer.fit(X_train_bigram)\n","\n","X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n","X_train = X_train_bigram_tf_idf\n","y_train = train_df['polarity']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sErYfMvnwT5"},"outputs":[],"source":["type(X_train_bigram_tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4GURyUOpkf0"},"outputs":[],"source":["%%time\n","\n","# NOTE: 50s\n","\n","# Save checkpoint\n","\n","!pwd\n","filename_save = f'{SUBDIR_DATA}imdb50k_stems.csv'\n","imdb50k_df.to_csv(filename_save, encoding='utf-8', index=False)\n","!ls -altr $SUBDIR_DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x95hV6IGpkbk"},"outputs":[],"source":["!ls ./data"]},{"cell_type":"markdown","metadata":{"id":"JEGy9bMWAuxd"},"source":["#### Vectorize IMDB Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXOXs_mokM2U"},"outputs":[],"source":["%%time\n","\n","# NOTE: \n","\n","vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stopwords_en_ls, max_features=1000)\n","vectors = vectorizer.fit_transform(imdb50k_df.text_clean)\n","words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n","words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfbuCCdnpd4W"},"outputs":[],"source":["type(train_df.iloc[0]['polarity'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m1CVu_5j3PJ"},"outputs":[],"source":["# For Statistical ML Models, prepare Dataset\n","#   separate text examples (X) from labels (y)\n","\n","X = X_train_bigram_tf_idf # words_df\n","y = train_df['polarity'].values # imdb50k_df.polarity\n","X.shape\n","print('\\n')\n","y.shape\n","type(y[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_xXzoZ0sv56"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLiTqdV1AS7P"},"outputs":[],"source":["# Split labeled dataset into training, validation and test sets\n","# e.g. for IMDB 50k reviews: Out of 50k dataset, 36k for training, 4k for Validationa and 10k for testing\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,test_size=0.1, random_state=42)\n","\n","print(\"Dataset Splits: Train, Valid, Test\")\n","[x.shape for x in [X_train,X_valid,X_test]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPtmUM8nAp-_"},"outputs":[],"source":["X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTin01FPAp06"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEPOwVW2I-ta"},"outputs":[],"source":["# Clean text_raw\n","\n","for i, atext_str in enumerate(corpus_titles_ls):\n","  print(f'Processing #{i}: {atext_str}')\n","\n","  corpus_texts_dt[atext_str].head()\n","  # df['text_clean'] = df['text_raw'].pipe(hero.clean)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAQbmVzhGWH_"},"outputs":[],"source":["# Remove non-alphanumeric chacters\n","# imdb50k_df['text_lower'] = imdb50k_df['text_raw']\n","\n","\"\"\"\n","pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n","imdb50k_df['text_clean'] = imdb50k_df['text_raw'].str.lower().str.strip().str.findall(pattern).str.join(' ')\n","imdb50k_df.head(1)\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44YvUyvMGWEM"},"outputs":[],"source":["import spacy\n","\n","nlp = spacy.blank('en')\n","\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","# nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVlhR2c1HNXO"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPrpL1wyNPva"},"outputs":[],"source":["%%time\n","\n","# NOTE: (no stem) 4m09s\n","#       (w/ stem) 4m24s\n","\n","i = 0\n","\n","for key_novel, atext_df in corpus_texts_dt.items():\n","\n","  print(f'Processing Novel #{i}: {key_novel}...')\n","\n","  atext_df['text_clean'] = clean_text(atext_df, 'text_raw', text_type='formal')\n","\n","  atext_df['text_clean'] = lemma_pipe(atext_df['text_clean'])\n","  atext_df['text_clean'] = atext_df['text_clean'].astype('string')\n","\n","  # TODO: Fill in all blank 'text_clean' rows with filler semaphore\n","  atext_df.text_clean = atext_df.text_clean.fillna('this_blank')\n","\n","  atext_df.head(2)\n","\n","  print(f'  shape: {atext_df.shape}')\n","\n","  i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXuwwg2_XgZu"},"outputs":[],"source":["# Verify the first Text in Corpus is cleaned\n","\n","corpus_texts_dt[corpus_texts_ls[0]].head(20)\n","corpus_texts_dt[corpus_texts_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"Bth9NSLloqIl"},"source":["### Simple Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BuQ4EDrovrN"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18DSN-W-FHOV"},"outputs":[],"source":["seed = 42\n","dataset = datasets.load_wine()\n","X = dataset.data; y = dataset.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n","kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16IBK46OuVHX"},"outputs":[],"source":["LinearDiscriminantAnalysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8O9M825yFHOW"},"outputs":[],"source":["models = []\n","models.append(('LR', LogisticRegression()))\n","models.append(('LDA', LinearDiscriminantAnalysis()))\n","models.append(('KNN', KNeighborsClassifier()))\n","models.append(('CART', DecisionTreeClassifier()))\n","models.append(('NB', GaussianNB()))\n","models.append(('SVM', SVC()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wer7VYtiFHOW"},"outputs":[],"source":["results = []\n","names = []\n","seed=42\n","\n","scoring = 'accuracy'\n","for name, model in models:\n","      kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n","      cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","      results.append(cv_results)\n","      names.append(name)\n","      msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","      print(msg)\n","\n","\"\"\"\n","\n","LR: 0.862886 (0.005822)\n","LDA: 0.859657 (0.006562)\n","KNN: 0.728571 (0.008688)\n","CART: 0.710829 (0.007842)\n","NB: 0.817286 (0.007659)\n","\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"4hwOKRyhozNT"},"source":["#### Linear Regression (linreg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LF0-5Vo6ozDP"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"ehixX4vio0fL"},"source":["#### Logistic Regression (logreg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHlud7XavKPZ"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"CQ6gfaG0o02z"},"source":["#### Logistic Regression, 6-Fold CV (logreg_cv6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JLX5Kj0oyu1"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"d0Tx3aTpo-gB"},"source":["#### Random Forest (rf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJSh3j4Zo9_N"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"G-P_6ZMspCW5"},"source":["#### Linear SVC (lin_svc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Go180gL9o-Em"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"NpwZRvT-pEhe"},"source":["#### Multinomial Naive Bayes (multi_nb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBHhuO0MHNRQ"},"outputs":[],"source":["\n","\n","from sklearn.pipeline import Pipeline\n","\n","text_clf_SGDClassifier = Pipeline([('vect', CountVectorizer(ngram_range=(1,3), stop_words='english',lowercase=True)),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', SGDClassifier()),\n","])\n","text_clf_SGDClassifier.fit(X_train, y_train)\n","clf_linreg = LinearRegression()\n","\n","clf_linreg.fit(X, y)\n"]},{"cell_type":"markdown","metadata":{"id":"-V0WogLvBFWu"},"source":["### Ensemble Models"]},{"cell_type":"markdown","metadata":{"id":"vQc5OIw4BJt5"},"source":["#### XGBoost (xgb)\n","\n","* https://xgboost.readthedocs.io/en/stable/\n","\n","* https://github.com/dmlc/xgboost\n","\n","* https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlCgXB3aBTsY"},"outputs":[],"source":["!pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wS_5p6nOHNNa"},"outputs":[],"source":["import xgboost\n","print(xgboost.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQqNOF34HNJD"},"outputs":[],"source":["from xgboost import XGBClassifier\n","\n","clf_xgb = XGBClassifier()"]},{"cell_type":"markdown","metadata":{"id":"8BELQIFXBRed"},"source":["#### CatBoost (catb)\n","\n","* https://github.com/catboost/tutorials/blob/master/classification/classification_tutorial.ipynb\n","\n","* https://github.com/catboost/tutorials/\n","\n","* https://catboost.ai/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCs4Y1rNBNU1"},"outputs":[],"source":["!pip install catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vydhfR-BNPd"},"outputs":[],"source":["import catboost\n","print(catboost.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqaAwDHfBZU3"},"outputs":[],"source":["from catboost import CatBoostClassifier, Pool   # https://catboost.ai/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FPanpPm1BZRQ"},"outputs":[],"source":["\n","\n","model = CatBoostClassifier(\n","    iterations=5,\n","    learning_rate=0.1,\n","    # loss_function='CrossEntropy'\n",")\n","model.fit(\n","    X_train, y_train,\n","    # cat_features=cat_features,\n","    eval_set=(X_validation, y_validation),\n","    verbose=False\n",")\n","print('Model is fitted: ' + str(model.is_fitted()))\n","print('Model params:')\n","print(model.get_params())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tgesFdnBZNL"},"outputs":[],"source":["%%time\n","\n","# NOTE: \n","\n","# test_data = catboost_pool = Pool(X_train, y_train)\n","\n","clf_catb = CatBoostClassifier(iterations=50,\n","                              random_seed=rand_seed,\n","                              depth=6,\n","                              learning_rate=0.5,\n","                              loss_function='Logloss',\n","                              verbose=5)\n","\n","# train the model\n","clf_catb.fit(\n","    X_train, y_train,\n","    eval_set=(X_valid, y_valid),\n","    verbose=True\n",")\n","\n","print('Model is fitted: ' + str(clf_catb.is_fitted()))\n","print('Model params:')\n","print(clf_catb.get_params())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMV6VLpRBZJl"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"oJ1BMO8JBk46"},"source":["#### LightGBM (lgbm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcm0rKE0BlqF"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"-8RRQVLzBn6K"},"source":["#### AdaBoost (adab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNx3ADxRBqGt"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8F_aBPZE5mKm"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"s2kOREmw9j52"},"source":["## **AutoML**"]},{"cell_type":"markdown","metadata":{"id":"guLrPnb-o1pb"},"source":["### **AutoML: Optuna**\n","\n","* https://github.com/optuna/optuna (20210826 5k) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvYlORpso0Ic"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"9ETsJ3Hlyig4"},"source":["### **AutoML: Scikit-Optimize**\n","\n","* https://github.com/scikit-optimize/scikit-optimize (20210622 2.2k) Sequential model-based optimization with a `scipy.optimize` interface\n","* https://scikit-optimize.github.io/stable/auto_examples/index.html "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pa5qVHjLyiFT"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"9dZqsvpyEpB-"},"source":["### **AutoML: Auto-Sklearn**\n","\n","* https://www.youtube.com/watch?v=CF-GZ9tK_Ik\n","* https://github.com/GauravSahani1417/Kaggle-Datasets/blob/master/Heart_failure_prediction_using_Auto_Sklearn_%F0%9F%A9%BA.ipynb \n","* https://github.com/bhattbhavesh91/auto-sklearn-tutorial/blob/master/autosklearn-classification.ipynb (20200929 5s) (pip install Cython numpy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGIAjgAREs2O"},"outputs":[],"source":["!pip install auto-sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_YBiQfhEs2O"},"outputs":[],"source":["!pip install scipy==1.7.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5-v66GeEs2P"},"outputs":[],"source":["import scipy\n","print(scipy.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azEvd3N7Es2P"},"outputs":[],"source":["from autosklearn.classification import AutoSklearnClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkHXL7GgEs2P"},"outputs":[],"source":["clf = AutoSklearnClassifier(time_left_for_this_task=1000, \n","                              per_run_time_limit=9, \n","                              ensemble_size=1, \n","                              initial_configurations_via_metalearning=0)\n","# Init training\n","clf.fit(X_train, y_train)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7wzRywvEs2Q"},"outputs":[],"source":["clf.score(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSyLaU3-Es2Q"},"outputs":[],"source":["clf.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03ZVMIYnEs2Q"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","y_pred = clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHChFQr9Es2R"},"outputs":[],"source":["conf_matrix = confusion_matrix(y_pred, y_test)\n","sns.heatmap(conf_matrix, annot=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"93UDEuhEEs2R"},"outputs":[],"source":["#Performance Measures\n","tn = conf_matrix[0,0]\n","fp = conf_matrix[0,1]\n","tp = conf_matrix[1,1]\n","fn = conf_matrix[1,0]\n","\n","total = tn + fp + tp + fn\n","real_positive = tp + fn\n","real_negative = tn + fp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RjyXdv7Es2R"},"outputs":[],"source":["accuracy  = (tp + tn) / total # Accuracy Rate\n","precision = tp / (tp + fp) # Positive Predictive Value\n","recall    = tp / (tp + fn) # True Positive Rate\n","f1score  = 2 * precision * recall / (precision + recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yM62Ij6dEs2R"},"outputs":[],"source":["print(f'Accuracy    : {accuracy}')\n","print(f'Precision   : {precision}')\n","print(f'Recall      : {recall}')\n","print(f'F1 score    : {f1score}')"]},{"cell_type":"markdown","metadata":{"id":"iNmqm5BD7wBC"},"source":["### **AutoML: FLAML by Microsoft**\n","\n","* pip install flaml[notebook] requires restart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8mYZolvjrVp"},"outputs":[],"source":["from flaml import AutoML\n","\n","from sklearn.datasets import load_iris"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5FNECtwtlfN"},"outputs":[],"source":["# Create test_flaml subdirectory to store log files\n","\n","!mkdir test_flaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4k64B_wZ-kMS"},"outputs":[],"source":["# Initialize an AutoML instance\n","automl = AutoML()\n","\n","# Specify automl goal and constraint\n","automl_settings = {\n","    \"time_budget\": 10,  # in seconds\n","    \"metric\": 'accuracy',\n","    \"task\": 'classification',\n","    \"log_file_name\": \"test_flaml/flaml_10s.log\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8OVXv-8EdiA"},"outputs":[],"source":["# X_train, y_train using TF-IDF of IMDB 50k\n","\n","# List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'lrl1']\n","\n","# Train with labeled input data\n","automl.fit(X_train=X_train, y_train=y_train,\n","           **automl_settings)\n","# Predict\n","print(automl.predict_proba(X_train))\n","\n","# Export the best model\n","print(automl.model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5LFN6e-EdiA"},"outputs":[],"source":["y_train.hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYZNZlT1EdiA"},"outputs":[],"source":["# Set the Time Limit\n","\n","TIME_LIM = 3600 # unit seconds (e.g. 600 = 10mins)\n","\n","settings = {\n","    \"time_budget\": TIME_LIM,  # total running time in seconds\n","    \"metric\": 'accuracy',  # primary metrics can be chosen from: ['accuracy','roc_auc','f1','log_loss','mae','mse','r2']\n","    \"task\": 'classification',  # task type    \n","    \"log_file_name\": 'test_flaml/flaml_1200s.log',  # flaml log file\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bImbt4gzEdiB"},"outputs":[],"source":["# Search for the best models by fitting training data with hyperparameter settings\n","\n","# NOTE: 33 iterations with 600s budget (10m), Best accuracy on validation data: 0.868\n","\n","automl.fit(X_train=X_train, y_train=y_train, **settings)\n","\n","\"\"\"\n","14 Sep 2021 1:24pm\n","\n","[flaml.automl: 09-14 17:23:37] {1735} INFO - iteration 114, current learner lrl1\n","[flaml.automl: 09-14 17:23:52] {1920} INFO -  at 3595.3s,\tbest lrl1's error=0.1310,\tbest lrl1's error=0.1310\n","[flaml.automl: 09-14 17:23:52] {2021} INFO - selected model: LogisticRegression(C=0.6701935610138009, n_jobs=-1, penalty='l1', solver='saga')\n","[flaml.automl: 09-14 17:24:10] {2084} INFO - retrain lrl1 for 17.9s\n","[flaml.automl: 09-14 17:24:10] {2088} INFO - retrained model: LogisticRegression(C=0.6701935610138009, n_jobs=-1, penalty='l1', solver='saga')\n","[flaml.automl: 09-14 17:24:10] {1529} INFO - fit succeeded\n","[flaml.automl: 09-14 17:24:10] {1531} INFO - Time taken to find the best model: 2468.841588973999\n","\n","===\n","\n","14 Sep 2021 12:20pm\n","\n","[flaml.automl: 09-14 15:45:13] {1735} INFO - iteration 42, current learner lrl1\n","[flaml.automl: 09-14 15:45:29] {1920} INFO -  at 1798.0s,\tbest lrl1's error=0.1320,\tbest lrl1's error=0.1320\n","[flaml.automl: 09-14 15:45:29] {2021} INFO - selected model: LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","[flaml.automl: 09-14 15:45:47] {2084} INFO - retrain lrl1 for 18.0s\n","[flaml.automl: 09-14 15:45:47] {2088} INFO - retrained model: LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","[flaml.automl: 09-14 15:45:47] {1529} INFO - fit succeeded\n","[flaml.automl: 09-14 15:45:47] {1531} INFO - Time taken to find the best model: 1653.2159299850464\n","[flaml.automl: 09-14 15:45:47] {1545} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n","\n","===\n","\n","14 Sep 2021 11:05am\n","\n","[flaml.automl: 09-14 14:41:05] {1735} INFO - iteration 33, current learner lrl1\n","[flaml.automl: 09-14 14:41:51] {1920} INFO -  at 621.9s,\tbest lrl1's error=0.1320,\tbest lrl1's error=0.1320\n","[flaml.automl: 09-14 14:41:51] {2021} INFO - selected model: LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","[flaml.automl: 09-14 14:42:07] {2084} INFO - retrain lrl1 for 16.5s\n","[flaml.automl: 09-14 14:42:07] {2088} INFO - retrained model: LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","[flaml.automl: 09-14 14:42:07] {1529} INFO - fit succeeded\n","[flaml.automl: 09-14 14:42:07] {1531} INFO - Time taken to find the best model: 562.4823379516602\n","[flaml.automl: 09-14 14:42:07] {1545} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n","\n","===\n","\n","[flaml.automl: 08-18 05:53:11] {1461} INFO - selected model: LGBMClassifier(colsample_bytree=0.7733707792852584,\n","               learning_rate=0.11190988982157068, max_bin=128,\n","               min_child_samples=62, n_estimators=701, num_leaves=12,\n","               objective='binary', reg_alpha=0.001291764523034099,\n","               reg_lambda=0.5058442385321611, verbose=-1)\n","[flaml.automl: 08-18 05:53:11] {1184} INFO - fit succeeded\n","[flaml.automl: 08-18 05:53:11] {1185} INFO - Time taken to find the best model: 567.884330034256\n","\n","\n","[flaml.automl: 08-18 05:31:22] {1411} INFO -  at 291.7s,\tbest extra_tree's error=0.2469,\tbest lgbm's error=0.1504\n","[flaml.automl: 08-18 05:31:27] {1438} INFO - retrain extra_tree for 5.0s\n","[flaml.automl: 08-18 05:31:27] {1253} INFO - iteration 46, current learner lrl1\n","No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'.\n","[flaml.automl: 08-18 05:31:41] {1411} INFO -  at 310.7s,\tbest lrl1's error=0.1454,\tbest lrl1's error=0.1454\n","[flaml.automl: 08-18 05:31:41] {1461} INFO - selected model: LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","[flaml.automl: 08-18 05:31:41] {1184} INFO - fit succeeded\n","[flaml.automl: 08-18 05:31:41] {1185} INFO - Time taken to find the best model: 310.6792550086975\n","[flaml.automl: 08-18 05:31:41] {1191} WARNING - Time taken to find the best model is 104% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJbAOLNAEdiB"},"outputs":[],"source":["# Retrieve the best model and configuration\n","\n","print('Best ML leaner:', automl.best_estimator)\n","print('Best hyperparmeter config:', automl.best_config)\n","print('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\n","print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n","\n","\"\"\"\n","14 Sep 2021 1:24pm\n","\n","Best ML leaner: lrl1\n","Best hyperparmeter config: {'C': 0.6701935610138009}\n","Best accuracy on validation data: 0.869\n","Training duration of best run: 20.48 s\n","\n","===\n","14 Sep 2021 12:20pm\n","\n","Best ML leaner: lrl1\n","Best hyperparmeter config: {'C': 1.0}\n","Best accuracy on validation data: 0.868\n","Training duration of best run: 16.45 s\n","\n","===\n","\n","14 Sep 2021 11:05am\n","\n","Best ML leaner: lgbm\n","Best hyperparmeter config: {'n_estimators': 701, 'num_leaves': 12, 'min_child_samples': 62, 'learning_rate': 0.11190988982157068, 'subsample': 1.0, 'log_max_bin': 8, 'colsample_bytree': 0.7733707792852584, 'reg_alpha': 0.001291764523034099, 'reg_lambda': 0.5058442385321611}\n","Best accuracy on validation data: 0.8516\n","Training duration of best run: 14.68 s\n","\n","===\n","\n","Best ML leaner: lrl1\n","Best hyperparmeter config: {'C': 1.0}\n","Best accuracy on validation data: 0.8546\n","Training duration of best run: 13.9 s\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXXU3xyvEdiC"},"outputs":[],"source":["# Retrieve the best model(hyperparameters)\n","\n","print(automl.model.estimator)\n","\n","\"\"\"\n","14 Sep 2021 1:24pm\n","LogisticRegression(C=0.6701935610138009, n_jobs=-1, penalty='l1', solver='saga')\n","\n","14 Sep 2021 12:20pm\n","LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","\n","14 Sep 2021 11:05am\n","LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","\n","===\n","\n","LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbPojBy_EdiC"},"outputs":[],"source":["# Use best model(hyperparameters) to compute predictions of testing dataset\n","\n","y_pred = automl.predict(X_test)\n","print('Predicted labels', y_pred)\n","print('True labels', y_test)\n","y_pred_proba = automl.predict_proba(X_test)[:,1]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YG5-qFIMEdiC"},"outputs":[],"source":["from sklearn.metrics import matthews_corrcoef\n","\n","mcc_y_test_predict = matthews_corrcoef(y_test, y_pred)\n","print(mcc_y_test_predict)\n","\n","\"\"\"\n","14 Sep 2021 1:24pm\n","0.7204752708450008\n","\n","14 Sep 2021 12:20pm\n","0.7204752708450008\n","\n","14 Sep 2021 11:05am\n","0.7204752708450008\n","\n","===\n","\n","0.7274299237200138\n","\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"ctu9Af3a8mh4"},"source":["### **AutoML: AutoVIML**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oWg_3Lb-kCt"},"outputs":[],"source":["# https://www.kaggle.com/khotijahs1/using-autonlp-for-sentiment-analysis\n","\n","from autoviml.Auto_NLP import Auto_NLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-8BPKFl8yS_"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ke3krghx8yNQ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y871_e4E8yIV"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"6l0zzf4I7zRe"},"source":["### **AutoML: HyperOpt-Sklearn**\n","\n","* https://github.com/hyperopt/hyperopt-sklearn\n","* https://github.com/hyperopt/hyperopt-sklearn/blob/master/notebooks/Demo-Iris.ipynb \n","\n","svc\n","svc_linear\n","svc_rbf\n","svc_poly\n","svc_sigmoid\n","liblinear_svc\n","\n","knn\n","\n","ada_boost\n","gradient_boosting\n","\n","random_forest\n","extra_trees\n","decision_tree\n","\n","sgd\n","\n","xgboost_classification\n","\n","multinomial_nb\n","gaussian_nb\n","\n","passive_aggressive\n","\n","linear_discriminant_analysis\n","quadratic_discriminant_analysis\n","\n","one_vs_rest\n","one_vs_one\n","output_code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxlEwn8P7teE"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"KZb_79A59c8r"},"source":["# **END**"]},{"cell_type":"markdown","metadata":{"id":"JWRZ7aHOd0R3"},"source":["# **[OLD STARTING POINT]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oiapgHzZdm4x"},"outputs":[],"source":["groups_ls = ['models_baseline_ls',\n","                'models_sentimentr_ls',\n","                'models_syuzhetr_ls',\n","                'models_transformer_ls']\n","\n","# Could add suffix '_sst2' if classifiers trained on SST2 (currently requires 30m on Colab Pro/GPU+RAM)\n","models_supervised_ls = ['linreg_imdb50k',\n","                   'svc_imdb50k',\n","                   'logreg_imdb50k',\n","                   'dforest_imdb50k',\n","                   'multinb_imdb50k']\n","\n","models_baseline_ls = ['sentimentr',\n","                      'syuzhet',\n","                      'bing',\n","                      'sentiword',\n","                      'senticnet',\n","                      'nrc',\n","                      'afinn',\n","                      'vader',\n","                      'textblob',\n","                      'flair',\n","                      'pattern',\n","                      'stanza']\n","\n","models_sentimentr_ls = ['jockers_rinker',\n","                        'jockers',\n","                        'huliu',\n","                        'senticnet',\n","                        'sentiword',\n","                        'nrc',\n","                        'lmcd']\n","\n","models_syuzhetr_ls = ['syuzhet',\n","                      'bing',\n","                      'afinn',\n","                      'nrc']\n","\n","models_transformer_ls = ['roberta15lg', \n","                         'nlptown', \n","                         'yelp', \n","                         'hinglish',\n","                         'imdb2way', \n","                         'huggingface', \n","                         't5imdb50k', \n","                         'robertaxml8lang']\n","\n","# Temporarily redefine from English to French Transformer Models\n","# models_transformer_ls = ['flaubert', 'nlptown', 'robertaxml8lang']"]},{"cell_type":"markdown","metadata":{"id":"MOPa6HH-OjZp"},"source":["**Install Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drpZJilASHUN"},"outputs":[],"source":["# fast detection of character set encoding for text/files\n","\n","!pip install cchardet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TA7Nw-SA_si1"},"outputs":[],"source":["!pip install pysbd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEjSzsusOOJ-"},"outputs":[],"source":["# common ML code\n","\n","!pip install sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZ0UVdasuTTS"},"outputs":[],"source":["%pip install contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9MbHfUCz6qTQ"},"outputs":[],"source":["!pip install pysbd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENUk4UsK6qTV"},"outputs":[],"source":["!pip install spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4Nis-KA6qTY"},"outputs":[],"source":["import pysbd\n","import spacy\n","from pysbd.utils import PySBDFactory\n","\n","# Conditionally loads english or french PySBD/NLTK Sentence tokenizers \n","#   in parags2sents()\n","\n","# nlp = spacy.blank('en')\n","\n","# nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","# nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxLPTsA_6qTa"},"outputs":[],"source":["\n","\n","# or you can use it implicitly with keyword\n","# pysbd = nlp.create_pipe('pysbd')\n","# nlp.add_pipe(pysbd)\n","\n","# doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n","# print(list(doc.sents))\n","# [My name is Jonas E. Smith., Please turn to p. 55.]"]},{"cell_type":"markdown","metadata":{"id":"cmtqmvu6OlR9"},"source":["**Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7bf4lfgwMEz"},"outputs":[],"source":["import os\n","import sys\n","import io\n","import glob\n","import json\n","import contextlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOmyq4h7OOFi"},"outputs":[],"source":["# IMPORT LIBRARIES\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OslLdEsvOuFU"},"outputs":[],"source":["import re\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YelenXz5BcmE"},"outputs":[],"source":["from itertools import cycle  # For plotly\n","\n","import collections\n","from collections import OrderedDict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Suximbjnw8D"},"outputs":[],"source":["# Import libraries for logging\n","\n","import logging\n","from datetime import datetime\n","import time                     # (TODO: check no dependencies and delete)\n","from time import gmtime, strftime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPZmScjVDYyw"},"outputs":[],"source":["import nltk\n","\n","# Download for sentence tokenization\n","import nltk.data\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","# Download for nltk/VADER sentiment analysis\n","nltk.download('vader_lexicon')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u34kPKO0_xF_"},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm') # Load the English Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMl2mfF8Haw8"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n","from sklearn.preprocessing import StandardScaler # To Standardize time series: center(sub mean) and rescale within 1 SD (only for well-behaved guassian distributions)\n","from sklearn.preprocessing import RobustScaler   # To Standardize time series: center(sub median) and rescale within 25%-75% (1st-3rd) IQR (better for noisy, outliers distributions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nckwluDXwa1c"},"outputs":[],"source":["minmax_scaler = MinMaxScaler()\n","mean_std_scaler = StandardScaler()\n","median_iqr_scaler = RobustScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U589lvKXmFV-"},"outputs":[],"source":["# Zoom interpolates new datapoints between existing datapoints to expand a time series \n","\n","from scipy.ndimage.interpolation import zoom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wcZfSOuBlW7"},"outputs":[],"source":["from scipy import interpolate\n","from scipy.interpolate import CubicSpline\n","from scipy import signal\n","from scipy.signal import argrelextrema"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CY3UyvYjAvDN"},"outputs":[],"source":["from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n","from statsmodels import robust"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02LJQlYpgQGs"},"outputs":[],"source":["corpus_sects_df = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"UcSc4jsggSy2"},"source":["**Define Library-Dependent Objects**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjGN2sN3uRpN"},"outputs":[],"source":["import contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AllIwMngDGC3"},"outputs":[],"source":["# Necessary to define before defining Utility Functions using these DataFrames\n","\n","corpus_sents_df = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"Kwl0MBDyOwtX"},"source":["**Configure Jupyter Notebook**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APCau-T26XQ3"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","def my_css():\n","   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n","\n","get_ipython().events.register('pre_run_cell', my_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nD1cyqWsfjxA"},"outputs":[],"source":["# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzfybE5kfmE-"},"outputs":[],"source":["# Configure matplotlib and seaborn\n","\n","# Plotting pretty figures and avoid blurry images\n","# %config InlineBackend.figure_format = 'retina'\n","# Larger scale for plots in notebooks\n","# sns.set_context('talk')\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = [16, 8]\n","plt.rcParams['figure.dpi'] = 100\n","plt.rc('figure', facecolor='white')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIIjSbyeP2fg"},"outputs":[],"source":["# Configure Jupyter\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from ipywidgets import widgets, interactive\n","\n","# Configure Google Colab\n","\n","%load_ext google.colab.data_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS_El2PiQlyP"},"outputs":[],"source":["# Text wrap\n","\n","from IPython.display import HTML\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuM_qnOHUil5"},"outputs":[],"source":["from IPython.display import HTML\n","\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import plotly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxQtrH196gl3"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","def my_css():\n","   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n","\n","get_ipython().events.register('pre_run_cell', my_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIPzbt5Ikldp"},"outputs":[],"source":["# with pd.option_context('display.max_colwidth', None):\n","#   display(corpus_transformer_df['sent_raw'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDR54Pbg5zqz"},"outputs":[],"source":["# with pd.option_context('display.max_colwidth', None):\n","#   display(corpus_sentimentr_df.iloc[:10]['sent_raw'])"]},{"cell_type":"markdown","metadata":{"id":"4dLkfn4KFmDf"},"source":["**Configuration Details Snapshot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FNPovQBFZky"},"outputs":[],"source":["# Snap Shot of Time, Machine, Data and Library/Version Blueprint\n","# TODO:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBPCBVkuzw_2"},"outputs":[],"source":["!pip list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HKj6KbIzoyM"},"outputs":[],"source":["# !pip install watermark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpMRnqjzzjS5"},"outputs":[],"source":["# %load_ext watermark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5uwS7H_zhaC"},"outputs":[],"source":["# %watermark"]},{"cell_type":"markdown","metadata":{"id":"y7CfH00OFkQV"},"source":["# **Either (a) Load Precomputed Sentiment Series or (b) Calculate Sentiment Values**\n","\n","Sentiment Models\n","\n","* VADER [-1.0 to 1.0] zero peak\n","* TextBlob [-1.0 to 1.0] zero peak\n","* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n","* AFINN [-14 (-8 to 8) 20] discrete\n","* SentimentR 11,710 [-5.4 to 8.8] norm\n","* Syuzhet [-5.4 to 8.8] norm\n","* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n","* Pattern [-1.0 to 1.0] norm\n","* SentiWord [-3.8 to 4.4] norm\n","* SenticNet [-3.8 to 10] norm\n","* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers"]},{"cell_type":"markdown","metadata":{"id":"CsEbvCoCX7HY"},"source":["## **(b) Compute Baseline Sentiments (Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RQAiwjNRGUb"},"outputs":[],"source":["plot_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLkfeBvTQZYh"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bY4o2FgRQbOu"},"outputs":[],"source":["layout = go.Layout(\n","    xaxis=dict(\n","        title='Fold no.'\n","    ),\n","    yaxis=dict(\n","        range=[np.min([i['y'] for i in plot_data]), 1],\n","        title='Accuracy'\n","    )\n",")\n","fig=go.Figure(data=plot_data, layout=layout)\n","ply.iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAQ3P3lnOQpX"},"outputs":[],"source":["layout = go.Layout(\n","    xaxis=dict(\n","        title='Fold no.'\n","    ),\n","    yaxis=dict(\n","        range=[np.min([i['y'] for i in plot_data]), 1],\n","        title='Accuracy'\n","    )\n",")\n","fig=go.scatter(data=plot_data, layout=layout)\n","fig.show()\n","# ply.iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWlJS6T8PXOP"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4o4HDc3SL8F"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn import model_selection\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","plt.style.use('ggplot')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dslGduteUACr"},"outputs":[],"source":["    KNeighborsClassifier(3),\n","    SVC(kernel=\"rbf\", C=0.025, probability=True),\n","    NuSVC(probability=True),\n","    DecisionTreeClassifier(),\n","    RandomForestClassifier(),\n","    AdaBoostClassifier(),\n","    GradientBoostingClassifier(),\n","    GaussianNB(),\n","    LinearDiscriminantAnalysis(),\n","    QuadraticDiscriminantAnalysis(),\n","    # MultinomialNB(),\n","    MLPClassifier()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Spqkf9VUSL29"},"outputs":[],"source":["seed = 50\n","dataset = datasets.load_wine()\n","X = dataset.data; y = dataset.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n","kfold = model_selection.KFold(n_splits=10, random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k08_iXqkSLxk"},"outputs":[],"source":["models = []\n","models.append(('LR', LogisticRegression()))\n","models.append(('LDA', LinearDiscriminantAnalysis()))\n","models.append(('KNN', KNeighborsClassifier()))\n","models.append(('CART', DecisionTreeClassifier()))\n","models.append(('NB', GaussianNB()))\n","models.append(('SVM', SVC()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC2tlurpTOP4"},"outputs":[],"source":["results = []\n","names = []\n","scoring = 'accuracy'\n","for name, model in models:\n","      kfold = model_selection.KFold(n_splits=10, random_state=seed)\n","      cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","      results.append(cv_results)\n","      names.append(name)\n","      msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","      print(msg)\n","\n","\"\"\"\n","\n","LR: 0.862886 (0.005822)\n","LDA: 0.859657 (0.006562)\n","KNN: 0.728571 (0.008688)\n","CART: 0.710829 (0.007842)\n","NB: 0.817286 (0.007659)\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbRGacBAnDyT"},"outputs":[],"source":["models2 = []\n","\n","models2.append(('SVM', SVC()))\n","models2.append(('MultiNB', MultinomialNB()))\n","models2.append(('RFC', RandomForestClassifier()))\n","models2.append(('MultiLP', MLPClassifier()))\n","\n","models2.append(('NuSVC', NuSVC(probability=True)))\n","models2.append(('AdaBC', AdaBoostClassifier()))\n","models2.append(('GradBC', GradientBoostingClassifier()))\n","models2.append(('LinDA', LinearDiscriminantAnalysis()))\n","models2.append(('QuadDA', QuadraticDiscriminantAnalysis()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpKeO3CcnFcj"},"outputs":[],"source":["results2 = []\n","names2 = []\n","scoring = 'accuracy'\n","for name, model in models2:\n","      kfold = model_selection.KFold(n_splits=10, random_state=seed)\n","      cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","      results2.append(cv_results)\n","      names2.append(name)\n","      msg = \"%s: %f (%f)\" % (name2, cv_results.mean(), cv_results.std())\n","      print(msg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGom-DXdTROf"},"outputs":[],"source":["fig = plt.figure(figsize=(10,10))\n","fig.suptitle('How to compare sklearn classification algorithms')\n","ax = fig.add_subplot(111)\n","plt.boxplot(results)\n","ax.set_xticklabels(names)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kJ6HRtBSLsX"},"outputs":[],"source":["results = []\n","names = []\n","scoring = 'accuracy'\n","for name, model in models:\n","      kfold = model_selection.KFold(n_splits=10, random_state=seed)\n","      cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","      results.append(cv_results)\n","      names.append(name)\n","      msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","      print(msg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWcwr4sRSLnK"},"outputs":[],"source":["fig = plt.figure(figsize=(10,10))\n","fig.suptitle('How to compare sklearn classification algorithms')\n","ax = fig.add_subplot(111)\n","plt.boxplot(results)\n","ax.set_xticklabels(names)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVJkMiprSLhG"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5o9P-qLSLb2"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlvJf9cuSLXO"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iW_cTC3KPigo"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import StratifiedShuffleSplit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZz-jk5hPXJI"},"outputs":[],"source":["sss = StratifiedShuffleSplit(labels, 10, test_size=0.2, random_state=23)\n","\n","for train_index, test_index in sss:\n","    X_train, X_test = train.values[train_index], train.values[test_index]\n","    y_train, y_test = labels[train_index], labels[test_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OVgc73dQAHn"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WFleRcSOeb_"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC, NuSVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","classifiers = [\n","    KNeighborsClassifier(3),\n","    SVC(kernel=\"rbf\", C=0.025, probability=True),\n","    NuSVC(probability=True),\n","    DecisionTreeClassifier(),\n","    RandomForestClassifier(),\n","    AdaBoostClassifier(),\n","    GradientBoostingClassifier(),\n","    GaussianNB(),\n","    LinearDiscriminantAnalysis(),\n","    QuadraticDiscriminantAnalysis(),\n","    # MultinomialNB(),\n","    MLPClassifier()]\n","\n","# Logging for Visual Comparison\n","log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n","log = pd.DataFrame(columns=log_cols)\n","\n","for clf in classifiers:\n","    clf.fit(X_train, y_train)\n","    name = clf.__class__.__name__\n","    \n","    print(\"=\"*30)\n","    print(name)\n","    \n","    print('****Results****')\n","    train_predictions = clf.predict(X_test)\n","    acc = accuracy_score(y_test, train_predictions)\n","    print(\"Accuracy: {:.4%}\".format(acc))\n","    \n","    train_predictions = clf.predict_proba(X_test)\n","    ll = log_loss(y_test, train_predictions)\n","    print(\"Log Loss: {}\".format(ll))\n","    \n","    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n","    log = log.append(log_entry)\n","    \n","print(\"=\"*30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4QHEHqgOeW-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCgNcZExqfDc"},"outputs":[],"source":["# WARNING: This will execute without error even if SpaCy model not loaded (silent error)\n","#          Must choose between En/Fr and consider PySBD sentence segementation as part of nlp pipeline\n","\n","def lemmatize(text):\n","    \"\"\"Perform lemmatization and stopword removal in the clean text\n","       Returns a list of lemmas\n","    \"\"\"\n","    text = ''.join([c for c in text if c.isascii()])\n","    doc = nlp(text)\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stopwords_en]\n","    return lemma_list\n","\n","# Test\n","print(lemmatize('I was running late and decided to to stop drinking.'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQqNyV16OQk2"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUUDZ8H6qfDf"},"outputs":[],"source":["%%time\n","\n","# Method #1: Lemmatize with Pandas apply()\n","\n","# Note: on C.Dickens' Great Expectations\n","\n","# imdb50k_df['text_lemma1'] = imdb50k_df['text_clean'].apply(lemmatize)\n","# imdb50k_df.head(3)\n","\n","\n","# Save checkpoint\n","\n","# imdb50k_df.to_csv(f'mlimdb50k_lemma3_sents_df.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')\n","\n","# files.download('mlimdb50k_lemma3_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGISHWdKqfDn"},"outputs":[],"source":["# Method #2: Lemmatize with spacy nlp.pipe\n","\n","def lemmatize_pipe(doc):\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stopwords_en] \n","    return lemma_list\n","\n","def preprocess_pipe(texts):\n","    preproc_pipe = []\n","    for doc in nlp.pipe(texts, batch_size=20):\n","        preproc_pipe.append(lemmatize_pipe(doc))\n","    return preproc_pipe\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwnFq1V-qfDp"},"outputs":[],"source":["%%time\n","\n","# Method #2: Lemmatize with spacy nlp.pipe\n","# Note: 22min\n","\n","# imdb50k_df['text_lemma2'] = preprocess_pipe(imdb50k_df['text_clean'])\n","# imdb50k_df.head(3)\n","\n","# imdb50k_df.rename(columns={'text_lemma3':'text_lemma'}, inplace=True)\n","# imdb50k_df.drop(columns=['text_lemma2'], inplace=True)\n","\n","# Save checkpoint\n","\n","# imdb50k_df.to_csv(f'mlimdb50k_lemma2_sents_df.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WkbBU9YqfDs"},"outputs":[],"source":["!pip install joblib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNVjmuGbqfDt"},"outputs":[],"source":["# Method #3: Lemmatize with joblib parallelization\n","\n","from joblib import Parallel, delayed\n","\n","def chunker(iterable, total_length, chunksize):\n","    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n","\n","def flatten(list_of_lists):\n","    \"Flatten a list of lists to a combined list\"\n","    return [item for sublist in list_of_lists for item in sublist]\n","\n","def process_chunk(texts):\n","    preproc_pipe = []\n","    for doc in nlp.pipe(texts, batch_size=20):\n","        preproc_pipe.append(lemmatize_pipe(doc))\n","    return preproc_pipe\n","\n","def preprocess_parallel(texts, chunksize=100):\n","    executor = Parallel(n_jobs=2, backend='multiprocessing', prefer=\"processes\")\n","    do = delayed(process_chunk)\n","    tasks = (do(chunk) for chunk in chunker(texts, len(imdb50k_df), chunksize=chunksize))\n","    result = executor(tasks)\n","    return flatten(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOWTIHg_qfDu"},"outputs":[],"source":["%%time\n","\n","# Method #3: Lemmatize with joblib parallelization\n","# NOTE: 17m13s \n","\n","# imdb50k_df['text_lemma3'] = preprocess_parallel(imdb50k_df['text_clean'], chunksize=1000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WEb4RV0guDX"},"outputs":[],"source":["\"\"\"\n","\n","if ML_Models_Arc == True:\n","  model_base = 'vader'\n","  model_name = 'vader_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n","\n","if VADER_Arc == True:\n","  # Sentiment evaluation function\n","  sid = SentimentIntensityAnalyzer()\n","\n","  # Test\n","  sid.polarity_scores('hello world'\n","\n","\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"zJT8Z7p1bE-N"},"source":["#### **Preparing Labeled Sentiment Dataset**\n","\n","* SST-2/SST-5\n","* IMDB\n","* Yelp\n","* Sentiment140"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HE1mTRBenRYh"},"outputs":[],"source":["# Upload kaggle credentials *.json file\n","\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiwD6YVXnRUf"},"outputs":[],"source":["!pwd\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Vxm1Pc4GdKg"},"outputs":[],"source":["!mkdir /root/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8C5ikUVngGS"},"outputs":[],"source":["!cp kaggle.json /root/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snpmtQv2GHzz"},"outputs":[],"source":["!ls /root/.kaggle/\n"]},{"cell_type":"markdown","metadata":{"id":"SoAbKdAPmagz"},"source":["**IMDB 50k (Movie)**\n","\n","* https://huggingface.co/datasets/imdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pg-Ce-V5Endk"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSTgJsroErYK"},"outputs":[],"source":["!mkdir data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjNs9ErqEu2K"},"outputs":[],"source":["%cd ./data\n","!pwd\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6DzdXynmXzZ"},"outputs":[],"source":["!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmQ6ePHdE0Gf"},"outputs":[],"source":["!tar -xvf aclImdb_v1.tar.gz\n","!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jU86X4wOE_Yl"},"outputs":[],"source":["!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YX4HcIUbE_Vb"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNWAauOKE_R6"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YWjGurjNxGz_"},"outputs":[],"source":["supervised_db = 'imdb50k'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqKUuGKOHOXD"},"outputs":[],"source":["!ls -altr *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKUUzsTrHJw1"},"outputs":[],"source":["# Option A: Load in imdb_df\n","\n","imdb50k_df = pd.read_csv(\"sa_train_lemma_imdb50k.csv\")\n","imdb50k_df.head(1)\n","imdb50k_df.info()\n","\n","\"\"\"\n","imdb50k_df[\"polarity\"] = imdb50k_df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n","imdb50k_df[\"text_raw\"] = imdb50k_df[\"review\"].astype('string')\n","imdb50k_df.drop(columns=['sentiment', 'review'], inplace=True)\n","\n","supervised_db = 'imdb50k'\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uA1Rq6Y2qhwV"},"outputs":[],"source":["# Option B: Create IMDB_df\n","\n","!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKQg7q3ary2i"},"outputs":[],"source":["!ls *zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNoXUTsxryxC"},"outputs":[],"source":["!unzip imdb-dataset-of-50k-movie-reviews.zip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6S-ZShOmsCEx"},"outputs":[],"source":["imdb50k_df = pd.read_csv(\"IMDB Dataset.csv\")\n","imdb50k_df[\"polarity\"] = imdb50k_df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n","imdb50k_df[\"text_raw\"] = imdb50k_df[\"review\"].astype('string')\n","imdb50k_df.drop(columns=['sentiment', 'review'], inplace=True)\n","\n","supervised_db = 'imdb50k'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EBFpI0KusB_h"},"outputs":[],"source":["imdb50k_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"874pXJ8jxDH9"},"outputs":[],"source":["# Remove non-alphanumeric chacters\n","# imdb50k_df['text_lower'] = imdb50k_df['text_raw']\n","\n","pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n","imdb50k_df['text_clean'] = imdb50k_df['text_raw'].str.lower().str.strip().str.findall(pattern).str.join(' ')\n","imdb50k_df.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0hlgbsUzvbg"},"outputs":[],"source":["stopwords_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gk1M_iJgdgCG"},"outputs":[],"source":["nlp = spacy.blank('en')\n","\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","# nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTTwI1MsxC4V"},"outputs":[],"source":["# WARNING: This will execute without error even if SpaCy model not loaded (silent error)\n","#          Must choose between En/Fr and consider PySBD sentence segementation as part of nlp pipeline\n","\n","def lemmatize(text):\n","    \"\"\"Perform lemmatization and stopword removal in the clean text\n","       Returns a list of lemmas\n","    \"\"\"\n","    doc = nlp(text)\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stopwords_en]\n","    return lemma_list\n","\n","# Test\n","print(lemmatize('I was running late and decided to to stop drinking.'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eKCtw6mzCRv"},"outputs":[],"source":["%%time\n","\n","# Method #1: Lemmatize with Pandas apply()\n","\n","# Note: on C.Dickens' Great Expectations\n","\n","imdb50k_df['text_lemma'] = imdb50k_df['text_clean'].apply(lemmatize)\n","imdb50k_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rb1PTFdB377O"},"outputs":[],"source":["# Save checkpoint\n","\n","imdb50k_df.to_csv(f'mlimdb50k_lemma_sents_df.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEWc92Z7CnE3"},"outputs":[],"source":["!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0wRacWECqgY"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2K83u01NC8Ft"},"outputs":[],"source":["files.download('mlimdb50k_lemma_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkc5w9b6DCJJ"},"outputs":[],"source":["files.download('ml_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JefUrhgQDMiQ"},"outputs":[],"source":["!rm mlimdb50k_lemma2_sents_df.csv\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AA5zr9xr0Pmf"},"outputs":[],"source":["# Method #2: Lemmatize with spacy nlp.pipe\n","\n","def lemmatize_pipe(doc):\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stopwords_en] \n","    return lemma_list\n","\n","def preprocess_pipe(texts):\n","    preproc_pipe = []\n","    for doc in nlp.pipe(texts, batch_size=20):\n","        preproc_pipe.append(lemmatize_pipe(doc))\n","    return preproc_pipe\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYZF5fDt0Pdj"},"outputs":[],"source":["%%time\n","\n","# Method #2: Lemmatize with spacy nlp.pipe\n","# Note: 22min\n","\n","imdb50k_df['text_lemma2'] = preprocess_pipe(imdb50k_df['text_clean'])\n","imdb50k_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IvyLp_DEA4q"},"outputs":[],"source":["imdb50k_df.rename(columns={'text_lemma3':'text_lemma'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veKXUZgDD8Ae"},"outputs":[],"source":["imdb50k_df.drop(columns=['text_lemma2'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwiRAVYJD3bM"},"outputs":[],"source":["imdb50k_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVgGKfjIEVC3"},"outputs":[],"source":["imdb50k_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZgnzBM431XH"},"outputs":[],"source":["# Save checkpoint\n","\n","imdb50k_df.to_csv(f'mlimdb50k_lemma2_sents_df.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5t3Nkmo01Gyy"},"outputs":[],"source":["!pip install joblib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEQ2iZOS0vOo"},"outputs":[],"source":["# Method #3: Lemmatize with joblib parallelization\n","\n","from joblib import Parallel, delayed\n","\n","def chunker(iterable, total_length, chunksize):\n","    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n","\n","def flatten(list_of_lists):\n","    \"Flatten a list of lists to a combined list\"\n","    return [item for sublist in list_of_lists for item in sublist]\n","\n","def process_chunk(texts):\n","    preproc_pipe = []\n","    for doc in nlp.pipe(texts, batch_size=20):\n","        preproc_pipe.append(lemmatize_pipe(doc))\n","    return preproc_pipe\n","\n","def preprocess_parallel(texts, chunksize=100):\n","    executor = Parallel(n_jobs=2, backend='multiprocessing', prefer=\"processes\")\n","    do = delayed(process_chunk)\n","    tasks = (do(chunk) for chunk in chunker(texts, len(imdb50k_df), chunksize=chunksize))\n","    result = executor(tasks)\n","    return flatten(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnQ55as-0vEs"},"outputs":[],"source":["%%time\n","\n","# Method #3: Lemmatize with joblib parallelization\n","# NOTE: 17m13s \n","\n","imdb50k_df['text_lemma'] = preprocess_parallel(imdb50k_df['text_clean'], chunksize=1000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2w6i8dv0u_c"},"outputs":[],"source":["# Save checkpoint\n","\n","imdb50k_df.to_csv(f'sa_train_lemma_imdb50k.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcqEhl0UCOaX"},"outputs":[],"source":["imdb50k_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UaZQ8pTtFIvI"},"outputs":[],"source":["print(imdb50k_df.iloc[0]['text_lemma'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFuZBs9KFb4E"},"outputs":[],"source":["def list2str(str_ls):\n","  '''\n","  Given a list of string\n","  Return a all strings concatenated, separated by ' '\n","  '''\n","  joined_str = ' '.join(str_ls)\n","\n","  return joined_str\n","\n","imdb50k_df['text_lemma'] = imdb50k_df['text_lemma'].apply(list2str)\n","# imdb50k_df.drop(columns=['text_lemma'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IynLUxIJMUIJ"},"outputs":[],"source":["imdb50k_df.drop(columns=['text_lemma3'],inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlC9MdSNfBVb"},"outputs":[],"source":["imdb50k_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Y2Qrq1bGf7O"},"outputs":[],"source":["# imdb50k_df.rename(columns={'text_lemma':'text_clean'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifyd8x6OtFo1"},"outputs":[],"source":["# def clean_stemlemma_text(text, stem_fl =False, lemma_fl=True, punct_fl=True, stopword_ls=stopwords_en):\n","# TODO: pandas DataFrame.Series.apply(curried function)\n","\n","# NOTE: SST2: >25m\n","# sst2_sents_df['text_lemma'] = sst2_sents_df['text_raw'].apply(clean_stemlemma_text)\n","\n","# imdb50k_df['text_lower'] = imdb50k_df['text_raw'].str.strip().str.lower()\n","# imdb50k_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDzfd0okE_Yr"},"outputs":[],"source":["# Vectorize IMDB Training dataset with TF-IDF\n","\n","from nltk.corpus import stopwords\n","stopwords_en = stopwords.words('english') # + stopwords.words('french')\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stopwords_en, max_features=1000)\n","vectors = vectorizer.fit_transform(imdb50k_df.text_clean)\n","words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n","words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzqocELYE_Yt"},"outputs":[],"source":["# Separate text from labels\n","\n","X = words_df\n","y = imdb50k_df.polarity\n","X.shape\n","print('\\n')\n","y.shape\n","type(y[0])"]},{"cell_type":"markdown","metadata":{"id":"zVg2Vh1QmYJf"},"source":["**SST-5 (Text)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4l9EPgVmqzU"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"HSa0vYLfmg3i"},"source":["**SST-2 (Text)**\n","\n","* https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html (Accelerate with SpaCy pipelines/joblib)"]},{"cell_type":"markdown","metadata":{"id":"RPoZ3onS8u-1"},"source":["**Retrieve via PyTorch TorchText**\n","\n","* https://github.com/shayneobrien/sentiment-classification/blob/master/notebooks/02-naive-bayes-unigram.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXJyGQ-9Eq7Q"},"outputs":[],"source":["supervised_db = 'sst2'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FheAchd8uxd"},"outputs":[],"source":["import torchtext\n","import numpy as np\n","from sklearn.naive_bayes import MultinomialNB\n","from torchtext.vocab import Vectors\n","from tqdm import tqdm_notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZdWe3yQ9EHn"},"outputs":[],"source":["text = torchtext.data.Field(include_lengths = False)\n","label = torchtext.data.Field(sequential=False)\n","train, val, test = torchtext.datasets.SST.splits(text, label, filter_pred=lambda ex: ex.label != 'neutral')\n","text.build_vocab(train)\n","label.build_vocab(train)\n","train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits((train, val, test), batch_size=10, device=-1, repeat = False)\n","url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n","text.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UhDXd2N9ECt"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"QZrHoKfh2vUC"},"source":["**Retrieve from Kaggle Datasets**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGow41Oz2qpv"},"outputs":[],"source":["!kaggle datasets download -d atulanandjha/stanford-sentiment-treebank-v2-sst2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLs5cDNH2qlM"},"outputs":[],"source":["!unzip stanford-sentiment-treebank-v2-sst2.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhLqfmQh-xLj"},"outputs":[],"source":["!cat SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/README.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcPO1OjQ2qgx"},"outputs":[],"source":["!ls SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nluKanu2qcU"},"outputs":[],"source":["!head -n 5 ./SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/datasetSentences.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEqbDjgN2qXq"},"outputs":[],"source":["!head -n 5 ./SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/sentiment_labels.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbomRqan2qTD"},"outputs":[],"source":["sst2_sents_filename = './SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/dictionary.txt'\n","sst2_sents_df = pd.read_csv(sst2_sents_filename, sep='|', header=None) \n","sst2_sents_df.columns = ['text_raw','phrase_id']\n","sst2_sents_df['text_raw'] = sst2_sents_df['text_raw'].astype('string')\n","sst2_sents_df.head(20)\n","sst2_sents_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EP69q79JdZSb"},"outputs":[],"source":["# def clean_stemlemma_text(text, stem_fl =False, lemma_fl=True, punct_fl=True, stopword_ls=stopwords_en):\n","# TODO: pandas DataFrame.Series.apply(curried function)\n","\n","# NOTE: SST2: >25m\n","# sst2_sents_df['text_lemma'] = sst2_sents_df['text_raw'].apply(clean_stemlemma_text)\n","\n","sst2_sents_df['text_lower'] = sst2_sents_df['text_raw'].str.strip().str.lower()\n","sst2_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtswvhMeaEyV"},"outputs":[],"source":["# sst2_sents_df['text_clean'] = sst2_sents_df['text_raw'].apply(clean_stemlemma_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxEgiVXGeS_z"},"outputs":[],"source":["# sst2_sents_df['text_lower'] = sst2_sents_df['text_raw'].apply(lambda x: lower(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQ8dmvpR2qNz"},"outputs":[],"source":["sst2_labels_filename = './SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/sentiment_labels.txt'\n","sst2_labels_df = pd.read_csv(sst2_labels_filename, sep='|') \n","sst2_labels_df.columns = ['phrase_id','polarity_fl']\n","sst2_labels_df.head(20)\n","sst2_labels_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sZztnmG_xEF"},"outputs":[],"source":["sst2_df = pd.concat([sst2_sents_df.set_index('phrase_id'),sst2_labels_df.set_index('phrase_id')], axis=1, join='inner')\n","sst2_df.head()\n","sst2_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsOcAE-6Cbid"},"outputs":[],"source":["def polarity_float2int(val_fl):\n","  '''\n","  Given a float between 0.0 and 1.0\n","  Return an integer between 0-5 mapped to every 0.2 interval\n","  '''\n","  val_int = 0\n","  if (val_fl < 0.2):\n","    val_int = 0\n","  elif (0.2 <= val_fl < 0.4):\n","    val_int = 1\n","  elif (0.4 <= val_fl < 0.6):\n","    val_int = 2\n","  elif (0.6 <= val_fl < 0.8):\n","    val_int = 3\n","  elif (0.8 <= val_fl <= 1.0):\n","    val_int = 4\n","  else:\n","    print(f'ERROR: polarity value must be [0.0-1.0] but was set to: {val_fl}')\n","    val_int = -99\n","\n","  return val_int\n","\n","# Test\n","polarity_float2int(0.55)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTyFUTZyDWbD"},"outputs":[],"source":["sst2_df['polarity'] = sst2_df['polarity_fl'].apply(lambda x: polarity_float2int(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuBCKBbAEDeB"},"outputs":[],"source":["sst2_df.head(10)\n","sst2_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQa4nAmb6H6X"},"outputs":[],"source":["sst2_df.shape\n","sst2_df[sst2_df.polarity.isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7y_bnL4Dglpk"},"outputs":[],"source":["\"\"\"\n","\n","from sklearn.feature_extraction import text\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","stopwords_custom = text.ENGLISH_STOP_WORDS.union([\"bazinga\"])\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=my_stop_words)\n","\n","X = vectorizer.fit_transform([\"this is an apple.\",\"this is a book.\"])\n","\n","idf_values = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n","\n","# printing the tfidf vectors\n","print(X)\n","\n","# printing the vocabulary\n","print(vectorizer.vocabulary_)\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xw8AN5wghXVT"},"outputs":[],"source":["from sklearn.feature_extraction import text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETCHx_Z5hXI4"},"outputs":[],"source":["stopwords_custom = text.ENGLISH_STOP_WORDS.union([\"bazinga\"])\n","len(stopwords_custom)\n","print('\\n')\n","type(stopwords_custom)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uXkt1Ha5-3d"},"outputs":[],"source":["vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=stopwords_custom, max_features=1000)\n","vectors = vectorizer.fit_transform(sst2_df.text_lower)\n","words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n","words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DML2PyO5-3e"},"outputs":[],"source":["X = words_df\n","y = sst2_df.polarity\n","X.shape\n","print('\\n')\n","y.shape\n","type(y[0])"]},{"cell_type":"markdown","metadata":{"id":"HGzl5dKk2rST"},"source":["**Retrieve from Huggingface Datasets**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSbf4OyPzkgl"},"outputs":[],"source":["# install datasets\n","!pip install datasets\n","\n","# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\n","import pyarrow\n","if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:\n","    import os\n","    os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gH_3O8oEzmB5"},"outputs":[],"source":["# Let's import the library. We typically only need at most four methods:\n","from datasets import list_datasets, list_metrics, load_dataset, load_metric\n","\n","from pprint import pprint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKhOr-XAzl7K"},"outputs":[],"source":["# Currently available datasets and metrics\n","datasets = list_datasets()\n","metrics = list_metrics()\n","\n","print(f\" Currently {len(datasets)} datasets are available on the hub:\")\n","pprint(datasets, compact=True)\n","print(f\" Currently {len(metrics)} metrics are available on the hub:\")\n","pprint(metrics, compact=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WEl8TzImj-P"},"outputs":[],"source":["# You can access various attributes of the datasets before downloading them\n","sst_dataset = list_datasets(with_details=True)[datasets.index('sst')]\n","\n","pprint(sst_dataset.__dict__)  # It's a simple python dataclass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqTkQELdmqgs"},"outputs":[],"source":["# Downloading and loading a dataset\n","dataset = load_dataset('sst', split='validation[:10%]')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjIO3bR_0XBG"},"outputs":[],"source":["# Informations on the dataset (description, citation, size, splits, format...)\n","# are provided in `dataset.info` (a simple python dataclass) and also as direct attributes in the dataset object\n","pprint(dataset.info.__dict__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfG9DIZe0W8r"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"KNCGf1KZEpld"},"source":["## **Summary**\n","\n","Baseline Models\n","\n","* VADER [-1.0 to 1.0] zero peak\n","* TextBlob [-1.0 to 1.0] zero peak\n","* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n","* AFINN [-14 (-8 to 8) 20] discrete\n","* SentimentR 11,710 [-5.4 to 8.8] norm\n","* Syuzhet [-5.4 to 8.8] norm\n","* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n","* Pattern [-1.0 to 1.0] norm\n","* SentiWord [-3.8 to 4.4] norm\n","* SenticNet [-3.8 to 10] norm\n","* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers\n","\n","SentimentR Models\n","\n","* Jockers_Rinker\n","* Jockers\n","* HuLiu\n","* NRC\n","* Loughran-McDonald\n","* SenticNet\n","* SentiWord\n","\n","SyuzhetR Models\n","\n","* Syuzhet\n","* Bing\n","* AFINN\n","* NRC\n","\n","Tranformer Models\n","\n","* NLPTown\n","* RoBERTa Large 15 Datasets\n","* BERT Yelp Dataset\n","* BERT Code Switching Hinglish\n","* IMDB 2-way \n","* Huggingface Default (Distilled BERT)\n","* T5 IMDB 50k Dataset\n","* RoBERTa XML 8 Languages"]},{"cell_type":"markdown","metadata":{"id":"6_j92DSgyhGc"},"source":["# **END OF WORKING**"]}],"metadata":{"colab":{"collapsed_sections":["zeLft8mw7moD","CRb36wyH7moE","BW6YDyHT7moF","-pnjdzeSInEY","HvaOI_64InEY","n2KDazsOInEY","tweDF9wMInEZ","-KeOHItI1a5y","jyi7Pi-u7cVK","4gOBlOeB7fa9"],"name":"sentiment_arcs_part3_lex2ml.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}