{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibHFmIWoU3Vx"
      },
      "source": [
        "# **SentimentArcs (Part 5): Transformer Models**\n",
        "\n",
        "```\n",
        "Jon Chun\n",
        "12 Jun 2021: Started\n",
        "12 Apr 2022: Last Update\n",
        "```\n",
        "\n",
        "TODO: \n",
        "* check for recovery from multiple runs/overwrites\n",
        "* pull save repetitive 3code blocks checkpoint code into single def\n",
        "* Force_Recompute = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG0AAbyyHewF"
      },
      "outputs": [],
      "source": [
        "# Make sure using a GPU\n",
        "\n",
        "# Tesla V100 = (3.0 x) P100 (2 vCPUs)\n",
        "#       P100 = (1.6 x) K80  (2 vCPUs/12GB RAM)\n",
        "#       P100 ~ T4 \n",
        "#       K80  = (     ) NVIDA GeForce RTX 3080 Ti\n",
        "#\n",
        "# High RAM Colab Pro  32GB RAM     $9.99/mo\n",
        "#          Colab Pro+ 52GB        $49.95/mo\n",
        "\n",
        "# Paperspace Free & Paid JupyterLab vs Google Colab\n",
        "# https://blog.paperspace.com/alternative-to-google-colab-pro/#:~:text=Most%20notable%20is%20that%20the,and%2012%20GB%20of%20RAM.&text=Meanwhile%2C%20in%20Paperspace%20Gradient%2C%20GPU,a%20Free%20or%20Paid%20instance. \n",
        "# https://www.kaggle.com/general/198232\n",
        "# https://towardsdatascience.com/deep-learning-on-a-budget-450-egpu-vs-google-colab-494f9a2ff0db (3080 vs Colab)\n",
        "# https://towardsdatascience.com/when-to-use-cpus-vs-gpus-vs-tpus-in-a-kaggle-competition-9af708a8c3eb (Kaggle Comp)\n",
        "# https://towardsdatascience.com/google-colab-pro-is-it-worth-49-99-c542770b8e56 (Colab Pro+)\n",
        "# https://www.quora.com/Which-is-better-Google-Colab-or-Kaggle-Notebook (Kaggle vs Colab GPUs)\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43oGeYK19Pyq"
      },
      "source": [
        "# **[RESTART RUNTIME] May be Required for these Libaries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgvTrI7bevn2"
      },
      "source": [
        "# **[STEP 1] Manual Configuration/Setup**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWXkoLBZFEfi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkcsI681TaDM"
      },
      "source": [
        "## (Popups) Connect Google gDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bfkqjgMiw7T"
      },
      "outputs": [],
      "source": [
        "# [INPUT REQUIRED]: Authorize access to Google gDrive\n",
        "\n",
        "# Connect this Notebook to your permanent Google Drive\n",
        "#   so all generated output is saved to permanent storage there\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n",
        "  drive.mount('/gdrive')\n",
        "else:\n",
        "  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVWagkv16GKQ"
      },
      "source": [
        "## (3 Inputs) Define Directory Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbNX_gP790_M"
      },
      "outputs": [],
      "source": [
        "# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n",
        "#              to math the full path to your gDrive subdirectory which should be the \n",
        "#              root directory cloned from the SentimentArcs github repo.\n",
        "\n",
        "# NOTE: Make sure this subdirectory already exists and there are \n",
        "#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n",
        "\n",
        "# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n",
        "#         letter, number and underscores ('_') characters should appear afterwards.\n",
        "#         Make sure your full path after %cd obeys this constraint or errors may appear.\n",
        "\n",
        "# #@markdown **Instructions**\n",
        "\n",
        "# #@markdown Set Directory and Corpus names:\n",
        "# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n",
        "# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n",
        "# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n",
        "# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n",
        "\n",
        "#@markdown <hr>\n",
        "\n",
        "# Step #1: Get full path to SentimentArcs subdir on gDrive\n",
        "# =======\n",
        "#@markdown **Accept default path on gDrive or Enter new one:**\n",
        "\n",
        "Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n",
        "\n",
        "\n",
        "#@markdown Set this to the project root in your <b>GDrive folder</b>\n",
        "#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n",
        "\n",
        "#@markdown <hr>\n",
        "\n",
        "#@markdown **Which type of texts are you cleaning?** \\\n",
        "\n",
        "Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n",
        "\n",
        "# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n",
        "Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n",
        "\n",
        "\n",
        "Corpus_Number = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "\n",
        "#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n",
        "#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n",
        "#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n",
        "\n",
        "#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n",
        "\n",
        "print('Current Working Directory:')\n",
        "%cd $Path_to_SentimentArcs\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "if Corpus_Type == 'reference':\n",
        "  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_reference'\n",
        "  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'\n",
        "else:\n",
        "  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n",
        "  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n",
        "\n",
        "# PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_TEXT_RAW}'\n",
        "# PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n",
        "PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n",
        "PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n",
        "\n",
        "# TODO: Clean up\n",
        "# SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n",
        "\n",
        "print(f'PATH_SENTIMENT_RAW:\\n  [{PATH_SENTIMENT_RAW}]')\n",
        "print(f'SUBDIR_SENTIMENT_RAW:\\n  [{SUBDIR_SENTIMENT_RAW}]')\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')\n",
        "print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P00BhwLVyL8X"
      },
      "source": [
        "# **[STEP 2] Automatic Configuration/Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeLft8mw7moD"
      },
      "source": [
        "## (each time) Custom Libraries & Define Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPdbnOjw6ycy"
      },
      "outputs": [],
      "source": [
        "# Add PATH for ./utils subdirectory\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "!python --version\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n",
        "PATH_UTILS\n",
        "\n",
        "sys.path.append(PATH_UTILS)\n",
        "\n",
        "print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n",
        "!ls $PATH_UTILS\n",
        "\n",
        "# More Specific than PATH for searching libraries\n",
        "# !echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvMuohQZ6ycz"
      },
      "outputs": [],
      "source": [
        "# Review Global Variables and set the first few\n",
        "\n",
        "import global_vars as global_vars\n",
        "\n",
        "global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n",
        "global_vars.Corpus_Genre = Corpus_Genre\n",
        "global_vars.Corpus_Type = Corpus_Type\n",
        "global_vars.Corpus_Number = Corpus_Number\n",
        "\n",
        "global_vars.SUBDIR_SENTIMENT_RAW = SUBDIR_SENTIMENT_RAW\n",
        "global_vars.PATH_SENTIMENT_RAW = PATH_SENTIMENT_RAW\n",
        "\n",
        "global_vars.SUBDIR_TEXT_CLEAN = SUBDIR_TEXT_CLEAN\n",
        "global_vars.PATH_TEXT_CLEAN = PATH_TEXT_CLEAN\n",
        "\n",
        "from utils import sa_config # (e.g. define TEST_WORDS_LS)\n",
        "\n",
        "sa_config.set_globals()\n",
        "\n",
        "global_vars.TEST_WORDS_LS\n",
        "print('\\n')\n",
        "\n",
        "dir(global_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2IRi-3z7moE"
      },
      "outputs": [],
      "source": [
        "# Initialize and clean for each iteration of notebook\n",
        "\n",
        "# dir(global_vars)\n",
        "\n",
        "global_vars.corpus_texts_dt = {}\n",
        "global_vars.corpus_titles_dt = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IU1IHzA7moE"
      },
      "outputs": [],
      "source": [
        "# Import SentimentArcs Utilities to define Directory Structure\n",
        "#   based the Selected Corpus Genre, Type and Number\n",
        "\n",
        "!pwd \n",
        "print('\\n')\n",
        "\n",
        "# from utils import sa_config # .sentiment_arcs_utils\n",
        "from utils import sa_config\n",
        "\n",
        "print('Objects in sa_config()')\n",
        "print(dir(sa_config))\n",
        "print('\\n')\n",
        "\n",
        "# Directory Structure for the Selected Corpus Genre, Type and Number\n",
        "sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPHLD8KNCPif"
      },
      "outputs": [],
      "source": [
        "# TODO: fix\n",
        "# global_vars.SUBDIR_GRAPHS = './graphs/graphs_novels_new_corpus5/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW6YDyHT7moF"
      },
      "source": [
        "## (each time) Read YAML Configuration for Corpus and Models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUveIcUOzYav"
      },
      "outputs": [],
      "source": [
        "# from utils import sa_config # .sentiment_arcs_utils\n",
        "\n",
        "import yaml\n",
        "\n",
        "from utils import read_yaml\n",
        "\n",
        "print('Objects in read_yaml()')\n",
        "print(dir(read_yaml))\n",
        "print('\\n')\n",
        "\n",
        "# Directory Structure for the Selected Corpus Genre, Type and Number\n",
        "read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n",
        "\n",
        "print('SentimentArcs Model Ensemble ------------------------------\\n')\n",
        "model_titles_ls = global_vars.models_titles_dt.keys()\n",
        "print('\\n'.join(model_titles_ls))\n",
        "\n",
        "\n",
        "print('\\n\\nCorpus Texts ------------------------------\\n')\n",
        "corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n",
        "print('\\n'.join(corpus_titles_ls))\n",
        "\n",
        "\n",
        "print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n",
        "print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n",
        "print('\\n')\n",
        "\n",
        "global_vars.corpus_titles_dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRb36wyH7moE"
      },
      "source": [
        "## Configure Jupyter Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjoIK5U_7moE"
      },
      "outputs": [],
      "source": [
        "# Configure Jupyter\n",
        "\n",
        "# To reload modules under development\n",
        "\n",
        "# Option (a)\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Option (b)\n",
        "# import importlib\n",
        "# importlib.reload(functions.readfunctions)\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5GqEXyRkPjj"
      },
      "source": [
        "## Install Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuWGvo26zAlM"
      },
      "outputs": [],
      "source": [
        "# Intentionally left blank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajD8hCbzkStO"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVNT7dGQsmw3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "import pandas as pd\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "pd.set_option('max_colwidth', 100) # -1)\n",
        "\n",
        "from glob import glob\n",
        "import copy\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoJdRfJh7FSz"
      },
      "outputs": [],
      "source": [
        "# Scikit Utilities, Metrics, Pipelines and Models\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8_qLJBbtoOA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "# Define Globals\n",
        "\n",
        "# Main data structure: Dictionary (key=text_name) of DataFrames (cols: text_raw, text_clean)\n",
        "corpus_texts_dt = {}\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n",
        "\n",
        "%run -i './utils/get_globals.py'\n",
        "\n",
        "SLANG_DT.keys()\n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1yTaY_9Qod"
      },
      "source": [
        "## Setup Matplotlib Style\n",
        "\n",
        "* https://matplotlib.org/stable/tutorials/introductory/customizing.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1s24O-S9JJX"
      },
      "outputs": [],
      "source": [
        "# Configure Matplotlib\n",
        "\n",
        "# View available styles\n",
        "# plt.style.available\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "%run -i './utils/config_matplotlib.py'\n",
        "\n",
        "config_matplotlib()\n",
        "\n",
        "print('Matplotlib Configuration ------------------------------')\n",
        "print('\\n  (Uncomment to view)')\n",
        "# plt.rcParams.keys()\n",
        "print('\\n  Edit ./utils/config_matplotlib.py to change')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dPPrZwyIIze"
      },
      "source": [
        "## Setup Seaborn Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMECX12r_CNo"
      },
      "outputs": [],
      "source": [
        "# Configure Seaborn\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "%run -i './utils/config_seaborn.py'\n",
        "\n",
        "config_seaborn()\n",
        "\n",
        "print('Seaborn Configuration ------------------------------\\n')\n",
        "# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DONrzMXxAmYE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X229IbToHwa2"
      },
      "source": [
        "## Python Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmkC1FbAEpo"
      },
      "source": [
        "### (each time) Generate Convenient Data Lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If9dQpsIAm_h"
      },
      "outputs": [],
      "source": [
        "# Derive List of Texts in Corpus a)keys and b)full author and titles\n",
        "\n",
        "print('Dictionary: corpus_titles_dt')\n",
        "global_vars.corpus_titles_dt\n",
        "print('\\n')\n",
        "\n",
        "corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n",
        "print(f'\\nCorpus Texts:')\n",
        "for akey in corpus_texts_ls:\n",
        "  print(f'  {akey}')\n",
        "print('\\n')\n",
        "\n",
        "print(f'\\nNatural Corpus Titles:')\n",
        "corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n",
        "for akey in corpus_titles_ls:\n",
        "  print(f'  {akey}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQNlQr4_Ckb1"
      },
      "outputs": [],
      "source": [
        "# Get Model Families of Ensemble\n",
        "\n",
        "from utils.get_model_families import get_ensemble_model_famalies\n",
        "\n",
        "global_vars.model_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n",
        "\n",
        "print('\\nTest: Lexicon Family of Models:')\n",
        "global_vars.model_ensemble_dt['lexicon']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3sXwZOq_-0d"
      },
      "source": [
        "### File Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JQBWKKcN2Eo"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "%run -i './utils/file_utils.py'\n",
        "# from utils.file_utils import *\n",
        "\n",
        "# %run -i './utils/file_utils.py'\n",
        "\n",
        "# TODO: Not used? Delete?\n",
        "# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fys3dkJSB656"
      },
      "source": [
        "# **[STEP 3] Read all Preprocessed Novels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O82nw_wvJsz9"
      },
      "source": [
        "## Read Cleaned Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Roq-2Ol8yH5c"
      },
      "outputs": [],
      "source": [
        "# Verify cwd and subdir of Cleaned Corpus Texts\n",
        "\n",
        "print('Current Working Directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'\\nSubdir with all Cleaned Texts of Corpus:\\n  {SUBDIR_TEXT_CLEAN}')\n",
        "\n",
        "print(f'\\n\\nFilenames of Cleaned Texts:\\n')\n",
        "!ls -1 {Path_to_SentimentArcs}{PATH_TEXT_CLEAN}\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(corpus_texts_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ClL4-1Gqe7g"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "# Create a List (preprocessed_ls) of all preprocessed text files\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "try:\n",
        "    preprocessed_ls = glob(f'{PATH_TEXT_CLEAN}/*.csv')\n",
        "    preprocessed_ls = [x.split('/')[-1] for x in preprocessed_ls]\n",
        "    preprocessed_ls = [x.split('.')[0] for x in preprocessed_ls]\n",
        "except IndexError:\n",
        "    raise RuntimeError('No csv file found')\n",
        "\n",
        "print('\\n'.join(preprocessed_ls))\n",
        "print('\\n')\n",
        "print(f'Found {len(preprocessed_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4CqJQY9rNRw"
      },
      "outputs": [],
      "source": [
        "# Read all preprocessed text files into master DataFrame (corpus_dt)\n",
        "\n",
        "global_vars.corpus_texts_dt = {}\n",
        "\n",
        "for i,atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Processing #{i}: {atext}...')\n",
        "  afile_fullpath = f'{PATH_TEXT_CLEAN}/{atext}.csv'\n",
        "  print(f'               {afile_fullpath}')\n",
        "  atext_df = pd.read_csv(afile_fullpath, index_col=[0])\n",
        "  global_vars.corpus_texts_dt[atext] = atext_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHLt5o78tJyR"
      },
      "outputs": [],
      "source": [
        "# Verify the Text read into master Dictionary of DataFrames\n",
        "\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "print('\\n')\n",
        "print(f'There were {len(global_vars.corpus_texts_dt)} preprocessed Text read into the Dict global_vars.corpus_texts_dt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JI2z5wCz8zz"
      },
      "outputs": [],
      "source": [
        "# Check if there are any Null strings in the text_clean columns\n",
        "\n",
        "for i, atext in enumerate(list(global_vars.corpus_texts_dt.keys())):\n",
        "  print(f'\\nNovel #{i}: {atext}')\n",
        "  nan_ct = global_vars.corpus_texts_dt[atext].text_clean.isna().sum()\n",
        "  if nan_ct > 0:\n",
        "    print(f'      {nan_ct} Null strings in the text_clean column')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgLyDNrYzTuF"
      },
      "outputs": [],
      "source": [
        "# Fill in all the Null value of text_clean with placeholder 'empty_string'\n",
        "\n",
        "for i, atext in enumerate(list(global_vars.corpus_texts_dt.keys())):\n",
        "  # print(f'Novel #{i}: {atext}')\n",
        "  # Fill all text_clean == Null with 'empty_string' so sentimentr::sentiment doesn't break\n",
        "  global_vars.corpus_texts_dt[atext].iloc[global_vars.corpus_texts_dt[atext].text_clean.isna()] = 'empty_string'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7gE08b_rNPH"
      },
      "outputs": [],
      "source": [
        "# Verify DataFrame of first Text in Corpus Dictionary\n",
        "\n",
        "global_vars.corpus_texts_dt[next(iter(global_vars.corpus_texts_dt))].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pGdRVhomoV0"
      },
      "outputs": [],
      "source": [
        "# [SKIP]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eefPtETcmpmg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEMOSJ5Ptryn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "print(f'Trying to find EXISTING Raw Sentiment Datafile:\\n  [{FNAME_SENTIMENT_RAW}]\\n  in Raw Sentiment Subdir: {SUBDIR_SENTIMENT_RAW}\\n')\n",
        "\n",
        "corpus_texts_dt = {}\n",
        "subdir_sentiment_raw_ls = os.listdir(SUBDIR_SENTIMENT_RAW)\n",
        "\n",
        "# Verify in SentimentArcs Root Directory and cd into ./utils for R programs\n",
        "os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n",
        "!pwd\n",
        "\n",
        "if FNAME_SENTIMENT_RAW not in subdir_sentiment_raw_ls:\n",
        "  # No Existing Raw Sentiment Data, Just read in Clean Text\n",
        "  print(f'No EXISTING Raw Sentiment Datafile exists, Start from Stratch...\\n')\n",
        "\n",
        "  # Create a List (texts_clean_ls) of all preprocessed text files\n",
        "  try:\n",
        "    texts_clean_ls = glob(f'{SUBDIR_TEXT_CLEAN}*.csv')\n",
        "    texts_clean_ls = [x.split('/')[-1] for x in texts_clean_ls]\n",
        "    texts_clean_ls = [x.split('.')[0] for x in texts_clean_ls]\n",
        "  except IndexError:\n",
        "    raise RuntimeError('No csv file found')\n",
        "\n",
        "  # Read all preprocessed text files into master DataFrame (corpus_dt)\n",
        "  for i,anovel in enumerate(texts_clean_ls):\n",
        "    print(f'Processing #{i}: {anovel}...')\n",
        "    afile_fullpath = f'{SUBDIR_TEXT_CLEAN}{anovel}.csv'\n",
        "    print(f'               {afile_fullpath}')\n",
        "    anovel_df = pd.read_csv(afile_fullpath, index_col=[0])\n",
        "    corpus_texts_dt[anovel] = anovel_df\n",
        "\n",
        "  print('\\n'.join(texts_clean_ls))\n",
        "  print('\\n')\n",
        "  print(f'Found {len(texts_clean_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')\n",
        "else:\n",
        "  # Found Existing Raw Sentiment Data with Clean Text, Read Both\n",
        "  print(f'Found EXISTING  Raw Sentiment Datafile exists, Loading...\\n')\n",
        "  corpus_texts_dt = read_dict_dfs(in_file=FNAME_SENTIMENT_RAW, in_dir=SUBDIR_SENTIMENT_RAW)\n",
        "  print(f'The  Models have Analyzed these Texts in the Corpus:\\n\\n  {corpus_texts_dt.keys()}\\n\\n')    \n",
        "\n",
        "# Verify Corpus DataFrame\n",
        "corpus_titles_ls = list(corpus_texts_dt.keys())\n",
        "# corpus_texts_dt[corpus_titles_ls[0]].head()\n",
        "corpus_texts_dt[corpus_titles_ls[0]].info()\n",
        "\n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UkbPWplNSAf"
      },
      "outputs": [],
      "source": [
        "models_ls = list(set(global_vars.corpus_texts_dt[corpus_texts_ls[0]].columns) - set(['text_clean','text_raw']))\n",
        "print(f'There are [{len(models_ls)} Models] Pre-Existing in the Datafile for this Notebook:\\n')\n",
        "models_ls\n",
        "print('\\n')\n",
        "\n",
        "print(f'There are [{len(corpus_texts_ls)} Texts] in the current Corpus:\\n')\n",
        "corpus_titles_ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCraVO_7ZtnZ"
      },
      "source": [
        "## (del?) If Sentiment Time Series exist, Verify with Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDPkOlSqd42s"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "# r_scaler = RobustScaler() \n",
        "# z_scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5_WhpYp_3Fi"
      },
      "source": [
        "\n",
        "# **[STEP 4] Transformer Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50sZaRg9ILYS"
      },
      "source": [
        "## Common Transformer Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw4gpaOoqQ9B"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead  # T5Base 50k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMQVbJX0fgeh"
      },
      "outputs": [],
      "source": [
        "# Define all Corpus Texts & Ensemble Models as global CONSTANTS\n",
        "\n",
        "model_family = 'transformer'\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "%run -i './utils/get_sentiments.py'\n",
        "\n",
        "#@markdown If Model Values already exist, Recompute anyway and Overwrite?\n",
        "\n",
        "Force_Recompute = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Get list Names for This Section of Python Lexicon Models\n",
        "\n",
        "# models_lexicon_ls = ensemble_models_dt['lexicon']\n",
        "models_pylex_ls = [x for x in global_vars.model_ensemble_dt[model_family]]\n",
        "print(f'Lexicon Models in this Section:\\n')\n",
        "print(models_pylex_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LZXzgoNsuAK"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8gT_HgQKoh4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "# from utils import sa_config (e.g. define TEST_WORDS_LS)\n",
        "\n",
        "from utils import sa_config\n",
        "\n",
        "sa_config.set_globals()\n",
        "\"\"\";\n",
        "\n",
        "global_vars.TEST_WORDS_LS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsc0KpOtCvrk"
      },
      "outputs": [],
      "source": [
        "global_vars.models_titles_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sbi7UFvDVTa"
      },
      "outputs": [],
      "source": [
        "global_vars.model_ensemble_dt.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJbeiU1R_z5y"
      },
      "source": [
        "## **RoBERTa Large 15 Datasets (5 cats)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyZI9cVw_z5z"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 00m11s @12:39 on 20220301 Colab Pro \n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
        "\n",
        "print(sa_model(\"I love this!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkELfJ0ydJAn"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'RoBERTa Large 15DB'\n",
        "model_name = 'roberta15lg'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  word_sentiment_fl = sa_model(aword_str)\n",
        "  print(f'Word: {aword_str}\\n    {word_sentiment_fl}\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  sent_sentiment_fl = sa_model(asent_str)\n",
        "  print(f'Sentence: {asent_str}\\n    {sent_sentiment_fl}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8kKEqOB5hPQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:   ~40m00s @12:39 on 20220301 Colab Pro P100 (1 Novels)\n",
        "#       ~1h31m00s @12:39 on 20220301 Colab Pro P100 (2 Novels: 535k, 536k)\n",
        "#        3h08m00s @03:12 on 20220412 Colab Pro P100 (3 Novels, 628k, 662k, 897k)\n",
        "#         ~00m00s @19:31 on 20220411 Colab Pro T4 (2 Novels: 535k, 536k)\n",
        "#          37m27s @14:40 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#          28m17s @23:35 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    # global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].apply(lambda x: labelscore2fl(classifier(x), sa_model=classifier))\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: labelscore2fl(sa_model(x), sa_model=model_name))\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFeQsfUlH7lB"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbfYpSL8P0fU"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgp3lwdMH7lJ"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xFkzLf2H7lJ"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3utdmNLJuWe"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'roberta15lg'\n",
        "# model_current = 'roberta15lg'\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbll6TM6JuWf"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8opUJxLkunNu"
      },
      "source": [
        "## **Huggingface Distill BERT SST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ajo1C4KFWSl"
      },
      "outputs": [],
      "source": [
        "# BUG: Cannot reuse variable name 'sa_model', causes replicated results of first model declared (e.g. roberta15lg)\n",
        "# sa_model = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = pipeline(\"sentiment-analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqudwECTNlS_"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'Huggingface DistilBERT'\n",
        "model_name = 'huggingface'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Emoji\n",
        "sa_model(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
        "print('\\n\\n')\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  word_sentiment_fl = sa_model(aword_str)\n",
        "  print(f'Word: {aword_str}\\n    {word_sentiment_fl}\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  sent_sentiment_fl = sa_model(asent_str)\n",
        "  print(f'Sentence: {asent_str}\\n    {sent_sentiment_fl}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJyHQbHt19-f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: \n",
        "\n",
        "# NOTE:   10m25s @02:48 on 20220304 Colab Pro T4 (2 Novels: 535k, 536k)\n",
        "#         24m26s @08:58 on 20220412 Colab Pro P100 (3 Novels: 1.2M, 1.3M, 1.7M)\n",
        "#         24m17s @11:20 on 20220412 Colab Pro P100 (3 Novels: 1.2M, 1.3M, 1.7M)\n",
        "#          8m49s @13:52 on 20220412 Colab Pro T4 (2 Texts: 1M, 1.1M)\n",
        "#          3m05s @22:10 on 20220415 Colab Pro K80 (3 Novels, 628k, 662k, 897k)\n",
        "#        ~15m05s @13:45 on 20220418 Colab Pro P100 (2 FinTexts, 3.4M 936k)\n",
        "#          6m26s @15:33 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#          4m15s @00:05 on 20220420 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "# for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "for indx_obj in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  i, atext = indx_obj\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: labelscore2fl(sa_model(x), sa_model='huggingface'))\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwmznmOfAYqC"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "# print(f'win_per: {win_per}')\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "# print(f'text_title_str: {text_title_str}')\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzlcn1QXdvEl"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYRYiibhMN0U"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_texts_ls[0]].select_dtypes(include=[np.float]).columns)\n",
        "\n",
        "global_vars.corpus_texts_dt[corpus_texts_ls[0]][models_all_ls].rolling(600, center=True, min_periods=0).mean().plot()\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQviRCEGDKMf"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuAlC-tj3Tni"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'huggingface'\n",
        "# model_current = 'huggingface'\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQvIsuI73a5_"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3anSxJbAIuM"
      },
      "source": [
        "## **NLPTown Multilingual BERT (5 cats)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eli4ig1yF8ZE"
      },
      "outputs": [],
      "source": [
        "# NLPTown: Multilingual trained BERT Model\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "# Test\n",
        "# print(sa_nlptown(\"I love this!\"))\n",
        "print(sa_model(\"I love this!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_JFo8934hVl"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'NLPTown MultiBERT'\n",
        "model_name = 'nlptown'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Emoji\n",
        "sa_model(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
        "print('\\n\\n')\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  word_sentiment_fl = sa_model(aword_str)\n",
        "  print(f'Word: {aword_str}\\n    {word_sentiment_fl}\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  sent_sentiment_fl = sa_model(asent_str)\n",
        "  print(f'Sentence: {asent_str}\\n    {sent_sentiment_fl}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDm3BdBoFqSX"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:   9m05s @16:40 on 20220301 Colab Pro P100 (1 Novel: cmieville_thecityandthecity)\n",
        "#        ~8m07s @16:49 on 20220201 Colab Pro P100 (1 Novel: scollins_thehungergames1)\n",
        "#        25m14s @08:58 on 20220412 Colab Pro P100 (3 Novels, 1.2M, 1.3M, 1.7M)\n",
        "#        25m14s @11:53 on 20220412 Colab Pro P100 (3 Novels, 1.2M, 1.3M, 1.7M)\n",
        "#       ~18m30s @14:23 on 20220412 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#        12m39s @15:40 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#         7m52s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: labelscore2fl(sa_model(x), sa_model='nlptown'))\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2jA-fQqGVfm"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3TCD7YYQR9s"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBej7tzSByvD"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIVfie2cTUHr"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfhjDlAdJ5II"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'nlptown'\n",
        "# model_current = 'nlptown'\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP0Lr0y7J5IJ"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReDQHLzGrQG8"
      },
      "source": [
        "## **Yelp Finetuned BERT (5 cats)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH-vhk2htCea"
      },
      "outputs": [],
      "source": [
        "# Yelp Trained BERT Sentiment Classifier\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gilf/english-yelp-sentiment\")\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = AutoModelForSequenceClassification.from_pretrained(\"gilf/english-yelp-sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUEyGgzjVDxZ"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "text_str = \"I love sunny days and happy puppy dogs.\"\n",
        "text_enc = tokenizer(text_str, return_tensors='pt')\n",
        "text_sentiment = sa_model(**text_enc)\n",
        "print(f'Yelp SA Model:\\n  INPUT: {text_str}\\n  SENTIMENT: {text_sentiment}')\n",
        "print(f'  TYPE: {type(text_sentiment)}')\n",
        "text_smax_ls_ls = text_sentiment.logits.softmax(dim=-1).tolist()\n",
        "text_smax_ls = text_smax_ls_ls[0]\n",
        "print(type(text_smax_ls[0]))\n",
        "print(f'  sMAX: {text_smax_ls}')\n",
        "max_val = max(text_smax_ls)\n",
        "max_indx = text_smax_ls.index(max_val)\n",
        "print(f'   MAX: {max_val} at indx={max_indx}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfvtrJtUl23T"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "text_str = \"I love sunny days and happy puppy dogs.\"\n",
        "text_enc = tokenizer(text_str, return_tensors='pt')\n",
        "text_sentiment = sa_model(**text_enc)\n",
        "\n",
        "sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(text_sentiment)\n",
        "print(f'\\n\\nSENTENCE: {text_str}')\n",
        "print(f'          Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOhFbpp1Hi5c"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'Yelp BERT'\n",
        "model_name = 'yelp'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Emoji\n",
        "emoji_str = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
        "aemoji_str_enc = tokenizer(emoji_str, return_tensors='pt')\n",
        "logits_tensor = sa_model(**aemoji_str_enc)\n",
        "sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "print(f'Emoji String: {emoji_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "print('\\n\\n')\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  aword_enc = tokenizer(aword_str, return_tensors='pt')\n",
        "  logits_tensor = sa_model(**aword_enc)\n",
        "  sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "  print(f'Word: {aword_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  aword_enc = tokenizer(asent_str, return_tensors='pt')\n",
        "  logits_tensor = sa_model(**aword_enc)\n",
        "  sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "  print(f'Word: {asent_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoscGYshHi5d"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:    10m45s @17:39 on 20220301 Colab Pro P100 (1 Novel: cmieville_thecityandthecity: 10.1k sentences)\n",
        "#          10m19s @17:48 on 20220301 Colab Pro P100 (1 Novel: scollins_thehungergames1: 9.0k sentences)\n",
        "#          49m55s @12:45 on 20220412 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#          49m55s @14:54 on 20220412 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#          30m27s @23:32 on 20220412 Colab Pro P100 (3 Novels, 1.2M, 1.3M, 1.7M)\n",
        "#          13m33s @15:54 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#           8m13s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    # corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: logitstensor2sentiment(sa_yelp(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: logitstensor2sentiment(sa_model(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyDxqK3HIjvF"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPSnWXAwOLhK"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ8JVkmBIjvG"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float,np.int]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7sEbvyNVPDa"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0bypzJaKx5F"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'yelp'\n",
        "# model_current = 'yelp'\n",
        "\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "\n",
        "# Filter out all models but Current One to Save\n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcfwDBbuKx5G"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEOydcZpum2h"
      },
      "source": [
        "## **Hinglish Mixed Code BERT (3 cats)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1XwIQu6wYh7"
      },
      "outputs": [],
      "source": [
        "# Hindi-English Code Switching BERT Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment\")\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = AutoModelForSequenceClassification.from_pretrained(\"rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3An3A2uusdyo"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "text_str = \"I love sunny days and happy puppy dogs.\"\n",
        "text_enc = tokenizer(text_str, return_tensors='pt')\n",
        "text_sentiment = sa_model(**text_enc)\n",
        "print(f'Hinglish SA Model:\\n  INPUT: {text_str}\\n  SENTIMENT: {text_sentiment}')\n",
        "print(f'  TYPE: {type(text_sentiment)}')\n",
        "text_smax_ls_ls = text_sentiment.logits.softmax(dim=-1).tolist()\n",
        "text_smax_ls = text_smax_ls_ls[0]\n",
        "print(type(text_smax_ls[0]))\n",
        "print(f'  sMAX: {text_smax_ls}')\n",
        "max_val = max(text_smax_ls)\n",
        "max_indx = text_smax_ls.index(max_val)\n",
        "print(f'   MAX: {max_val} at indx={max_indx}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4ajfb76u9y0"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "text_str = \"I love sunny days and happy puppy dogs.\"\n",
        "text_enc = tokenizer(text_str, return_tensors='pt')\n",
        "text_sentiment = sa_model(**text_enc)\n",
        "\n",
        "sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(text_sentiment)\n",
        "print(f'\\n\\nSENTENCE: {text_str}')\n",
        "print(f'          Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6QlYZHWe2aK"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'Hinglish Dual Code BERT'\n",
        "model_name = 'hinglish'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Emoji\n",
        "emoji_str = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
        "aemoji_str_enc = tokenizer(emoji_str, return_tensors='pt')\n",
        "logits_tensor = sa_model(**aemoji_str_enc)\n",
        "sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "print(f'Emoji String: {emoji_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "print('\\n\\n')\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  aword_enc = tokenizer(aword_str, return_tensors='pt')\n",
        "  logits_tensor = sa_model(**aword_enc)\n",
        "  sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "  print(f'Word: {aword_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  aword_enc = tokenizer(asent_str, return_tensors='pt')\n",
        "  logits_tensor = sa_model(**aword_enc)\n",
        "  sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "  print(f'Word: {asent_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtlNSW1qe2aL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:    10m45s @17:39 on 20220301 Colab Pro P100 (1 Novel: cmieville_thecityandthecity: 10.1k sentences)\n",
        "#          10m19s @17:48 on 20220301 Colab Pro P100 (1 Novel: scollins_thehungergames1: 9.0k sentences)\n",
        "#          49m55s @12:45 on 20220412 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#          49m55s @14:54 on 20220412 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#          30m45s @00:06 on 20220413 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#          11m37s on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#           8m26s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    # corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: logitstensor2sentiment(sa_yelp(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: logitstensor2sentiment(sa_model(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAuFFlmee2aL"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLqyvjMe2aL"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJZqRbwRe2aM"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float,np.int]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95m-Uj86u9y3"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5iMQdFxLUky"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'hinglish'\n",
        "# model_current = 'hinglish'\n",
        "\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO9SW3NyLUky"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEC93OI0um9e"
      },
      "source": [
        "## **BERT IMDB Sentiment (2 cats)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63dihgd3um9e"
      },
      "outputs": [],
      "source": [
        "# BERT Sentiment Classifier fine-tuned on IMDB\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"abhishek/autonlp-imdb_sentiment_classification-31154\")\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = AutoModelForSequenceClassification.from_pretrained(\"abhishek/autonlp-imdb_sentiment_classification-31154\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QYJ_BSBwnBA"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "text_str = \"I love sunny days and happy puppy dogs.\"\n",
        "text_enc = tokenizer(text_str, return_tensors='pt')\n",
        "text_sentiment = sa_model(**text_enc)\n",
        "print(f'IMDB SA Model:\\n  INPUT: {text_str}\\n  SENTIMENT: {text_sentiment}')\n",
        "print(f'  TYPE: {type(text_sentiment)}')\n",
        "text_smax_ls_ls = text_sentiment.logits.softmax(dim=-1).tolist()\n",
        "text_smax_ls = text_smax_ls_ls[0]\n",
        "print(type(text_smax_ls[0]))\n",
        "print(f'  sMAX: {text_smax_ls}')\n",
        "max_val = max(text_smax_ls)\n",
        "max_indx = text_smax_ls.index(max_val)\n",
        "print(f'   MAX: {max_val} at indx={max_indx}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxQk_q_dwnBB"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "text_str = \"I love sunny days and happy puppy dogs.\"\n",
        "text_enc = tokenizer(text_str, return_tensors='pt')\n",
        "text_sentiment = sa_model(**text_enc)\n",
        "\n",
        "sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(text_sentiment)\n",
        "print(f'\\n\\nSENTENCE: {text_str}')\n",
        "print(f'          Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQyqCXHzwnBB"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'IMDB BERT'\n",
        "model_name = 'imdb2way'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Emoji\n",
        "emoji_str = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
        "aemoji_str_enc = tokenizer(emoji_str, return_tensors='pt')\n",
        "logits_tensor = sa_model(**aemoji_str_enc)\n",
        "sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "print(f'Emoji String: {emoji_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "print('\\n\\n')\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  aword_enc = tokenizer(aword_str, return_tensors='pt')\n",
        "  logits_tensor = sa_model(**aword_enc)\n",
        "  sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "  print(f'Word: {aword_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  aword_enc = tokenizer(asent_str, return_tensors='pt')\n",
        "  logits_tensor = sa_model(**aword_enc)\n",
        "  sentiment_val, sentiment_scale, sentiment_prob = logitstensor2sentiment(logits_tensor)\n",
        "  print(f'Word: {asent_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjWj0z1RwnBC"
      },
      "outputs": [],
      "source": [
        "# Verify saving results to correct model name\n",
        "\n",
        "print(f'Saving results to Model: {model_name}')\n",
        "\n",
        "sent_test_str = \"I hate Mondays and bad doctors.\"\n",
        "print(f\"Sentence: {sent_test_str}\\n {logitstensor2sentiment(sa_model(**tokenizer(sent_test_str, return_tensors='pt')))[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttrZs_o-f9Yj"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:     4m32s @18:10 on 20220301 Colab Pro P100 (1 Novel: cmieville_thecityandthecity: 10.1k sentences)\n",
        "#           4m08s @18:15 on 20220301 Colab Pro P100 (1 Novel: scollins_thehungergames1: 9.0k sentences)\n",
        "#          00m00s @12:45 on 20220412 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#          00m00s @14:54 on 20220412 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#          17m03s @17:37 on 20220412 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#          15m07s @00:40 on 20220413 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#           5m41s @16:21 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#           4m13s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    # corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: logitstensor2sentiment(sa_yelp(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: logitstensor2sentiment(sa_model(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRuMnJ1QiGMy"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTB4s7ZciGMy"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k09eg91iGMz"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float,np.int]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-HXu3jQwnBD"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPHN62VeNg8X"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'imdb2way'\n",
        "# model_current = 'imdb2way'\n",
        "\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQlIenyMNg8Y"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPtgYODJ4EIc"
      },
      "source": [
        "## **T5Base 50k Finetuned IMDB Sentiment Extraction (2 cats)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps6t3aY0LgnT"
      },
      "outputs": [],
      "source": [
        "# T5 Base Model Fine-tuned on 50k IMDB \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-imdb-sentiment\")\n",
        "\n",
        "amodel = None\n",
        "del amodel\n",
        "amodel = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-small-finetuned-imdb-sentiment\")\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "\n",
        "def sa_model(text):\n",
        "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
        "\n",
        "  output = amodel.generate(input_ids=input_ids,\n",
        "               max_length=2)\n",
        "  \n",
        "  dec = [tokenizer.decode(ids) for ids in output]\n",
        "  label = dec[0]\n",
        "  return label\n",
        "\n",
        "sa_model(\"I dislike a lot that film\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kaCiM-7x4l-"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "text_str = \"I love sunny days and happy puppy dogs.\"\n",
        "res = sa_model(text_str)\n",
        "# type(res)\n",
        "print(f'res: [{res}]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTcuYxpbxHxl"
      },
      "outputs": [],
      "source": [
        "def t5str2sentiment(t5_str):\n",
        "  '''\n",
        "  Given a T5 text string with sentiment and tags\n",
        "  Return a -1,0,1 for negative,neutral/OOV,positive sentiment\n",
        "  '''\n",
        "\n",
        "  t5_sentiment1_str = t5_str.replace('<pad>','')\n",
        "  t5_sentiment2_str = t5_sentiment1_str.replace('</s>','')\n",
        "  t5_core_str = t5_sentiment2_str.strip().lower()\n",
        "  # print(f't5_sentiment_raw_str: {t5_sentiment_raw_str}')\n",
        "  # print(f't5_sentiment_str: {t5_sentiment_str}')\n",
        "  # print(f't5_core_str: {t5_core_str}')\n",
        "  if t5_core_str == 'positive':\n",
        "    t5_sentiment_fl = 1\n",
        "  elif t5_core_str == 'negative':\n",
        "    t5_sentiment_fl = -1\n",
        "  elif len(t5_core_str) > 0:\n",
        "    # Out of Vocab (OOV) Word\n",
        "    t5_sentiment_fl = 0\n",
        "  elif len(t5_core_str) == 0:\n",
        "    # Empty String\n",
        "    t5_sentiment_fl = 0\n",
        "  else:\n",
        "    print(f'ERROR: Illegal value ts_sentiment_str: {t5_sentiment_str}')\n",
        "\n",
        "  return int(t5_sentiment_fl)\n",
        "\n",
        "# Test\n",
        "asent_str = \"I love the good and kind-hearted lovely lady who give me tasty cookies.\"\n",
        "t5_response_str = sa_model(asent_str)\n",
        "asentiment_int = t5str2sentiment(t5_response_str)\n",
        "print(f'STRING: {asent_str}\\nSENTIMENT: {asentiment_int}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwgt9DrMx4l_"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'T5 Base IMDB50K'\n",
        "model_name = 't5imdb50k'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  t5_sentiment_raw_str = t5str2sentiment(sa_model(aword_str))\n",
        "  print(f'Word: {aword_str}\\n    Sentiment: {t5_sentiment_raw_str}\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  t5_sentiment_raw_str = t5str2sentiment(sa_model(asent_str))\n",
        "  print(f'Word: {asent_str}\\n    Sentiment: {t5_sentiment_raw_str}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBcnEFbNx4mA"
      },
      "outputs": [],
      "source": [
        "# Verify saving results to correct model name\n",
        "\n",
        "print(f'Saving results to Model: {model_name}\\n')\n",
        "\n",
        "sent_test_str = \"I hate Mondays and bad doctors.\"\n",
        "# sent_test_str = \"You are a disgusting pig - I hate you.\"\n",
        "print(f\"SENTENCE: {sent_test_str}\\nSENTIMENT: {t5str2sentiment(sa_model(sent_test_str))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u4uAVmu64vZ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:    10m45s @17:39 on 20220301 Colab Pro (1 Novel: cmieville_thecityandthecity: 10.1k sentences)\n",
        "#          10m00s @17:48 on 20220301 Colab Pro (1 Novel: scollins_thehungergames1: 9.0k sentences)\n",
        "#          15m21s @18:06 on 20220412 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#          15m57s @00:57 on 20220413 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#           6m06s @16:28 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#           4m22s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: t5str2sentiment(sa_model(x)))\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JF33ncWihFa"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82td_0l0ihFb"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESxoOzHXihFb"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float,np.int]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnOzPpdUx4mB"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlswc6CgRAMh"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 't5imdb50k'\n",
        "# model_current = 't5imdb50k'\n",
        "\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q540OYGORAMi"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilhcBc9JAOqv"
      },
      "source": [
        "## **RoBERTa XLM Twitter 8 Multilingual (3 cats)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68KhdhcQNKKe"
      },
      "outputs": [],
      "source": [
        "# RoBERTa XLM Fine-tuend on 8 Multilingual Tweets\n",
        "\n",
        "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n",
        "\n",
        "# Test\n",
        "sa_model(\"T'estimo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNVyB7fXOHny"
      },
      "outputs": [],
      "source": [
        "# from transformers.utils.dummy_pt_objects import SQUEEZEBERT_PRETRAINED_MODEL_ARCHIVE_LIST\n",
        "\n",
        "# Define Model and Test\n",
        "\n",
        "model_title = 'RoBERTa XLM Twitter 8Lang'\n",
        "model_name = 'robertaxml8lang'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  # word_sentiment_fl = SQUEEZEBERT_PRETRAINED_MODEL_ARCHIVE_LIST(aword_str)\n",
        "  word_sentiment_fl = sa_model(aword_str)\n",
        "  print(f'Word: {aword_str}\\n    {word_sentiment_fl}\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  # sent_sentiment_fl = labelscore2fl(sa_model(asent_str), sa_model='robertaxml8lang')\n",
        "  sent_sentiment_fl = labelscore2fl(sa_model(asent_str), sa_model='robertaxml8lang')\n",
        "  print(f'Sentence: {asent_str}\\n    {sent_sentiment_fl:.3f}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HYdDDOI7BmK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:     9m05s @16:40 on 20220301 Colab Pro P100 (1 Novel: cmieville_thecityandthecity)\n",
        "#          ~8m07s @16:49 on 20220201 Colab Pro P100 (1 Novel: scollins_thehungergames1)\n",
        "#          30m36s @18:33 on 20220412 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#          28m29s @01:16 on 20220413 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#          11m34s @16:35 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#           7m58s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: labelscore2fl(sa_model(x), sa_model='robertaxml8lang'))\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM6k_qYGCHig"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx] \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIa5VjocCHig"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6EtSFFKCHih"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float,np.int]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwnX94mmOHn2"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chISx9JURcD8"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'robertaxml8lang'\n",
        "# model_current = 'robertaxml8lang'\n",
        "\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpvyQfbtRcD8"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zonShooc-vk"
      },
      "source": [
        "## **FinBERT Tone (3 cats)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOKxL84Vducx"
      },
      "outputs": [],
      "source": [
        "# FinBERT Tone\n",
        "\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "sa_model = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wskHuy7Kd745"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "fin_sentences_ls = [\"there is a shortage of capital, and we need extra financing\",  \n",
        "             \"growth is strong and we have plenty of liquidity\", \n",
        "             \"there are doubts about our finances\", \n",
        "             \"profits are flat\"]\n",
        "results_ls = sa_model(fin_sentences_ls)\n",
        "print(results_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEytDnd0c-vk"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "fin_sentences_ls = [\"I love sunny days and happy puppy dogs.\",\n",
        "                    \"The market crashed and all indicators were down\",\n",
        "                    \"Strong institutional buying resulted in rising share prices and a bull market\"]\n",
        "                  \n",
        "results_ls = sa_model(fin_sentences_ls)\n",
        "\n",
        "def dtlabelscore2fl(adict):\n",
        "  \"\"\"\n",
        "  Given a dictionary of 2 keys ['label']=[positive|negative|neutral] and ['score']=[0.0 to +1.0]\n",
        "  Return an adjusted sentiment score [-1.0 to +1.0]\n",
        "  \"\"\"\n",
        "  adj_sentiment = 0.\n",
        "  apolarity = adict['label']\n",
        "  asentiment = adict['score']\n",
        "  if apolarity == 'negative':\n",
        "    adj_sentiment = -1. * float(asentiment)\n",
        "  elif apolarity == 'positive':\n",
        "    adj_sentiment = float(asentiment)\n",
        "  else:\n",
        "    adj_sentiment = 0.\n",
        "\n",
        "  return adj_sentiment\n",
        "\n",
        "for i, asent_dt in enumerate(results_ls):\n",
        "  asentence = fin_sentences_ls[i]\n",
        "  raw_results = results_ls[i]\n",
        "  adj_senti = dtlabelscore2fl(asent_dt)\n",
        "  # print(f'SENTENCE:\\n    {asentence}\\n    apolarity: {apolarity}\\n    asentiment: {asentiment}\\n    adj_sentiment: {adj_sentiment}') \n",
        "  print(f'SENTENCE:\\n    {asentence}\\n    raw_results: {raw_results}\\n    adj_sentiment: {adj_senti}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMUvRe7Yc-vk"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'FinBERT Tone'\n",
        "model_name = 'finberttone'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Emoji\n",
        "emoji_str = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
        "raw_sentiment = sa_model(emoji_str)[0]\n",
        "adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "print(f'Emoji String: {emoji_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "print('\\n\\n')\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  raw_sentiment = sa_model(aword_str)[0]\n",
        "  adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "  print(f'Word String: {aword_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "  # print('\\n\\n')\n",
        "  # print(f'Word: {aword_str}\\n    Raw Sentiment: {adj_sentiment} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  raw_sentiment = sa_model(asent_str)[0]\n",
        "  adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "  print(f'Sentence String: {asent_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "  # print(f'Sentence: {asent_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA5GuE8Yc-vl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:    16m22s @21:56 on 20220301 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#          28m01s @01:48 on 20220413 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#          11m25s @16:57 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#           7m58s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    # corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: logitstensor2sentiment(sa_yelp(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: dtlabelscore2fl(sa_model(x)[0]))\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyJVUU8Mc-vl"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmsE3zE5c-vl"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmaPnFc9c-vl"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float,np.int]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StOXVRBvc-vl"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr2kkRc5RrF0"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'finberttone'\n",
        "# model_current = 'finberttone'\n",
        "\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPl14sVsRrF0"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc0QfXZ7FYnG"
      },
      "source": [
        "## **DistilRoBERTa Financial News (3 cats)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIyG-gxqu4Yn"
      },
      "outputs": [],
      "source": [
        "# distilbertfinnews\n",
        "\n",
        "distilrobertfinnews = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = pipeline(\"sentiment-analysis\", model=distilrobertfinnews, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc1D7YQzutfh"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "fin_sentences_ls = [\"there is a shortage of capital, and we need extra financing\",  \n",
        "             \"growth is strong and we have plenty of liquidity\", \n",
        "             \"there are doubts about our finances\", \n",
        "             \"profits are flat\"]\n",
        "results_ls = sa_model(fin_sentences_ls)\n",
        "print(results_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PgBL1bwutfh"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "fin_sentences_ls = [\"I love sunny days and happy puppy dogs.\",\n",
        "                    \"The market crashed and all indicators were down\",\n",
        "                    \"Strong institutional buying resulted in rising share prices and a bull market\"]\n",
        "                  \n",
        "results_ls = sa_model(fin_sentences_ls)\n",
        "\n",
        "def dtlabelscore2fl(adict):\n",
        "  \"\"\"\n",
        "  Given a dictionary of 2 keys ['label']=[positive|negative|neutral] and ['score']=[0.0 to +1.0]\n",
        "  Return an adjusted sentiment score [-1.0 to +1.0]\n",
        "  \"\"\"\n",
        "  adj_sentiment = 0.\n",
        "  apolarity = adict['label']\n",
        "  asentiment = adict['score']\n",
        "  if apolarity == 'negative':\n",
        "    adj_sentiment = -1. * float(asentiment)\n",
        "  elif apolarity == 'positive':\n",
        "    adj_sentiment = float(asentiment)\n",
        "  else:\n",
        "    adj_sentiment = 0.\n",
        "\n",
        "  return adj_sentiment\n",
        "\n",
        "for i, asent_dt in enumerate(results_ls):\n",
        "  asentence = fin_sentences_ls[i]\n",
        "  raw_results = results_ls[i]\n",
        "  adj_senti = dtlabelscore2fl(asent_dt)\n",
        "  # print(f'SENTENCE:\\n    {asentence}\\n    apolarity: {apolarity}\\n    asentiment: {asentiment}\\n    adj_sentiment: {adj_sentiment}') \n",
        "  print(f'SENTENCE:\\n    {asentence}\\n    raw_results: {raw_results}\\n    adj_sentiment: {adj_senti}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI2UdLplutfh"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'DistilBERT FinNews'\n",
        "model_name = 'distilbertfinnews'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Emoji\n",
        "emoji_str = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
        "raw_sentiment = sa_model(emoji_str)[0]\n",
        "adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "print(f'Emoji String: {emoji_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "print('\\n\\n')\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  raw_sentiment = sa_model(aword_str)[0]\n",
        "  adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "  print(f'Word String: {aword_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "  # print('\\n\\n')\n",
        "  # print(f'Word: {aword_str}\\n    Raw Sentiment: {adj_sentiment} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  raw_sentiment = sa_model(asent_str)[0]\n",
        "  adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "  print(f'Sentence String: {asent_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "  # print(f'Sentence: {asent_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEmI0St_utfi"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:    9m16s @21:56 on 20220301 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#         28m01s @02:17 on 20220413 Colab Pro P100 (3 FinTexts, 1.2M, 1.3M, 1.7M)\n",
        "#          6m07s @16:59 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#         00m00s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    # corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: logitstensor2sentiment(sa_yelp(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: dtlabelscore2fl(sa_model(x)[0]))\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp2bEilZutfi"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAldFYTgutfi"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ned025qVutfi"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float,np.int]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hixlXtS1mDis"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxnXtNvFRx0C"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'distilbertfinnews'\n",
        "# model_current = 'distilbertfinnews'\n",
        "\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kqdk0OHRx0D"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmBWvNZU3cgu"
      },
      "source": [
        "## **FinBERT (3 cats)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpabT0eL3cgv"
      },
      "outputs": [],
      "source": [
        "# ProsusAI FinBERT\n",
        "\n",
        "finbert = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "sa_model = None\n",
        "del sa_model\n",
        "sa_model = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PabxpGmKw0Q4"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "fin_sentences_ls = [\"there is a shortage of capital, and we need extra financing\",  \n",
        "             \"growth is strong and we have plenty of liquidity\", \n",
        "             \"there are doubts about our finances\", \n",
        "             \"profits are flat\"]\n",
        "results_ls = sa_model(fin_sentences_ls)\n",
        "print(results_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhPWnSQww0Q5"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "fin_sentences_ls = [\"I love sunny days and happy puppy dogs.\",\n",
        "                    \"The market crashed and all indicators were down\",\n",
        "                    \"Strong institutional buying resulted in rising share prices and a bull market\"]\n",
        "                  \n",
        "results_ls = sa_model(fin_sentences_ls)\n",
        "\n",
        "def dtlabelscore2fl(adict):\n",
        "  \"\"\"\n",
        "  Given a dictionary of 2 keys ['label']=[positive|negative|neutral] and ['score']=[0.0 to +1.0]\n",
        "  Return an adjusted sentiment score [-1.0 to +1.0]\n",
        "  \"\"\"\n",
        "  adj_sentiment = 0.\n",
        "  apolarity = adict['label']\n",
        "  asentiment = adict['score']\n",
        "  if apolarity == 'negative':\n",
        "    adj_sentiment = -1. * float(asentiment)\n",
        "  elif apolarity == 'positive':\n",
        "    adj_sentiment = float(asentiment)\n",
        "  else:\n",
        "    adj_sentiment = 0.\n",
        "\n",
        "  return adj_sentiment\n",
        "\n",
        "for i, asent_dt in enumerate(results_ls):\n",
        "  asentence = fin_sentences_ls[i]\n",
        "  raw_results = results_ls[i]\n",
        "  adj_senti = dtlabelscore2fl(asent_dt)\n",
        "  # print(f'SENTENCE:\\n    {asentence}\\n    apolarity: {apolarity}\\n    asentiment: {asentiment}\\n    adj_sentiment: {adj_sentiment}') \n",
        "  print(f'SENTENCE:\\n    {asentence}\\n    raw_results: {raw_results}\\n    adj_sentiment: {adj_senti}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq3fR3ZAw0Q5"
      },
      "outputs": [],
      "source": [
        "# Define Model and Test\n",
        "\n",
        "model_title = 'FinBERT'\n",
        "model_name = 'finbert'\n",
        "model_type = 'Transformer'\n",
        "\n",
        "# Test Emoji\n",
        "emoji_str = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
        "raw_sentiment = sa_model(emoji_str)[0]\n",
        "adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "print(f'Emoji String: {emoji_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "print('\\n\\n')\n",
        "\n",
        "# Test Words\n",
        "print(f'Testing WORD Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, aword_str in enumerate(global_vars.TEST_WORDS_LS):\n",
        "\n",
        "  raw_sentiment = sa_model(aword_str)[0]\n",
        "  adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "  print(f'Word String: {aword_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "  # print('\\n\\n')\n",
        "  # print(f'Word: {aword_str}\\n    Raw Sentiment: {adj_sentiment} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n",
        "\n",
        "# Test Sentences\n",
        "print(f'\\nTesting SENTENCE Sentiment')\n",
        "print('--------------------------------------------------')\n",
        "for i, asent_str in enumerate(global_vars.TEST_SENTENCES_LS):\n",
        "\n",
        "  raw_sentiment = sa_model(asent_str)[0]\n",
        "  adj_sentiment = dtlabelscore2fl(raw_sentiment)\n",
        "  print(f'Sentence String: {asent_str}\\n    Raw Sentiment: {raw_sentiment}\\n    Adj Sentiment: {adj_sentiment}\\n')\n",
        "  # print(f'Sentence: {asent_str}\\n    Sentiment: {sentiment_val} out of {sentiment_scale} (prob={sentiment_prob:.3f})\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W85MAFuEw0Q5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:    16m13s @22:39 on 20220301 Colab Pro T4 (2 FinTexts: 1M, 1.1M)\n",
        "#          11m35s @17:07 on 20220419 Colab Pro T4 (1 Novel: 502k)\n",
        "#          00m00s @00:00 on 20220419 Colab Pro P100 (1 Novel: 343k)\n",
        "\n",
        "for i, atext in enumerate(global_vars.corpus_texts_dt.keys()):\n",
        "  models_ls = global_vars.corpus_texts_dt[atext].columns\n",
        "  print(f\"Processing #{i}: {atext}\")\n",
        "  if (Force_Recompute == True) or (model_name not in models_ls):\n",
        "    # corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: logitstensor2sentiment(sa_yelp(**tokenizer(x, return_tensors='pt')))[0])\n",
        "    global_vars.corpus_texts_dt[atext][model_name] = global_vars.corpus_texts_dt[atext]['text_clean'].progress_apply(lambda x: dtlabelscore2fl(sa_model(x)[0]))\n",
        "    print(f'  [{model_name}] Sentiment Recomputed and Value Updated')\n",
        "  else:\n",
        "    print(f'  [{model_name}] Already in DataFrame and no Forced Recompute, so no update made')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R_Ll4wmw0Q5"
      },
      "outputs": [],
      "source": [
        "# Verify Plausiblity of new Sentiment Values by Plotting\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "# amodel_str = 'huggingface' \n",
        "win_aper = 10\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][model_name].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTJRRq7Mw0Q6"
      },
      "outputs": [],
      "source": [
        "global_vars.corpus_texts_dt[corpus_text_str].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csyWbD9sw0Q6"
      },
      "outputs": [],
      "source": [
        "# Plot all models\n",
        "\n",
        "novel_indx = 0\n",
        "corpus_text_str = corpus_texts_ls[novel_indx]\n",
        "win_aper = 10\n",
        "\n",
        "models_all_ls = list(global_vars.corpus_texts_dt[corpus_text_str].select_dtypes(include=[np.float,np.int]).columns)\n",
        "\n",
        "win_per = int((win_aper/100.)*global_vars.corpus_texts_dt[corpus_text_str][model_name].shape[0])\n",
        "text_title_str = f\"{global_vars.corpus_titles_dt[corpus_text_str][0]}\\nSentiment Analysis: Smoothed w/SMA ({win_aper}%)\\n{model_type} Model: {model_title}\"\n",
        "global_vars.corpus_texts_dt[corpus_text_str][models_all_ls].rolling(win_per, center=True, min_periods=0).mean().plot(title=text_title_str)\n",
        "plt.grid(True)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iA1BrJ9w0Q6"
      },
      "source": [
        "### **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYuu31fzR37u"
      },
      "outputs": [],
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "\n",
        "# defined above\n",
        "# model_name = 'finbert'\n",
        "# model_current = 'finbert'\n",
        "\n",
        "\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "print('Currently in SentimentArcs root directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# Verify Subdir to save Cleaned Texts and Texts into..\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "global_vars.corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW\n",
        "\n",
        "\n",
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "# os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_transformer_{model_name}.json'\n",
        "else:\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_transformer_{model_name}.json'\n",
        "\n",
        "# Filter out all models but Current One \n",
        "current_model_dt = {}\n",
        "cols_current_model_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'Saving {model_name} Model for all Texts')\n",
        "  current_model_dt[atext] = pd.DataFrame(global_vars.corpus_texts_dt[atext][model_name])\n",
        "\n",
        "write_dict_dfs(current_model_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VONEqhgYR37v"
      },
      "outputs": [],
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_j92DSgyhGc"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CRb36wyH7moE",
        "o5GqEXyRkPjj",
        "7dPPrZwyIIze",
        "O82nw_wvJsz9",
        "50sZaRg9ILYS",
        "Sc0QfXZ7FYnG"
      ],
      "name": "sentiment_arcs_part5_transformers.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
